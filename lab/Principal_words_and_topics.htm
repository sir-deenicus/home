<meta http-equiv='Content-Type' content='Type=text/html; charset=utf-8'>
                             <script type='text/javascript' async src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML'></script>
                             <title>Principal words and topics</title>
                             <link rel='stylesheet' type='text/css' href='style.css'>
<p>Principal words and topics</p>

<p>Once the vocabulary is large enough the model will compute principal words. Then future documents can be computed in terms of this matrix with the top N. Words can be multiplied against matrix then averaged to get similarity to it. Then these vectors can be compared to each other.</p>

<hr />

<p><em>What path through paragraphs best matches wrights manifold, evolution,learning entropy and what is the neighborhood of concepts around these?</em></p>

<p>getting top phrases</p>

<p>getting top phraes but restricting to some set</p>

<p>Getting overview of results and smarter search results</p>

<p>when searching first get top paragraphs or docs then top sentences</p>

<p>for rri , either use weights as in dualbandits or take log of matrix before comparisons</p>

<p>Do a back tracking search with intersections from A -> B -> C, if no intersect do 1 deg out otherwise fail.</p>

<p>Or multiply against our topic with our words, then the complement set of words.</p>

<p>two words that share the most verb, nouns and adverbs and adjective neighborhoods arbitrary combinators allow</p>

<p>what questions are noun heavy
how questions are verb heavy
whys are reasons, therefore,hence, because, due to dual system</p>

<p>Alternative searches: what is X, how does Y</p>

<p>summarization, multi document, webpages as visualization of graph</p>

<p>Answering Why</p>

<p>Train an XPercptron on: "because", "cause", "reason", "consider", "since", "due"</p>

<hr />

<p>Surprises and topic words</p>

<p>Surprise is computed on the model level by Multiyping a before and after matrix. Extracting the diagnal with the lowest number being most surprising.</p>

<p>Uncommon words: in addition to top words, with low vector score an xtron trained on them and their neighbors could generate candidates as well as eignencentrals of gram matrix from rri para nouns</p>

<hr />

<p>Highlighting top sentences
this is done by using comparing against the topic matrix computed from the trained random vectors. Sentence Matrix * Topic Matrix. => Average that. Then if score avg > 0.5 and scores highly in document  highlihgt (keep for 0.8)</p>

<hr />

<p>Predicting What you might like</p>

<p>This uses % values of each topic to predict. Use xperceptron and naive bayes as well as tree aug on topic words themselves
use averaged perceptron on all 3, class name as [01][10]. xperceptron is just a number</p>

<hr />

<p>Intersect many,</p>

<p>A quick way to see which variables are related is to simplfoldy  over the set. If a set is empty we check the neigbors of its neighbors and intersect in a second more lenient set (two sets of sets are being kept) then we move on to the next using the previous sets respectively if sets are empty.</p>

<p>Seperate with space. Words in quotes their neigborhood unions are taken together.</p>

<p>you break the sentence and paras into chunks or just pairs (meaningful?) then take intersection. Multiply against some topic vector to get it to vector form.  Then convolve using a non-commutative operator Until at final vector form. Those can then be compared</p>

<hr />

<p>agent search</p>

<p>agent search proceeds by  downloading 
the task is what to pick next? All positive weights go into rri. find eigen centrals and sample from top n. Then also, one can compute per topic.  Weight each then sample pairs and generate a path that will then be used as a search query. another option is sample a word then pick its neighbor from each topic agent</p>

<p>initial page (search [a,b,c])</p>

<p>read each page (download -> analyze [a,b,c])</p>

<p>Select next step (new search queries) 
repeat step 2 N times</p>

<p>wiki has links, in this case we rank them according to ?agents 
]what of types?</p>

<hr />

<p>summaries</p>

<p>first filter to 0.55 of important. then do xperceptron trained on topic words based ranking. then group according to topic matrix</p>

<p>multi summary</p>

<p>do comparison and sort by most connected vertices. allow for matrix fac as well.</p>

<hr />

<p>search resorting</p>

<p>use xperceptron to highlight and rerank</p>

<hr />

<p>segmentations</p>

<p>using topic matrix for each sentence stack words into matrix them multiply. Average into vectors and if next sentence is close keep else new paragraph.</p>

<hr />

<p>Deductions</p>

<p>all phrases make a matrix
for input word find neighbors, use those instead.
sample? or generate all at once.
proceed deduction as normal.
except option for comparison is checking neighborhood
Weights? Treat as probability normalized(exp) then propogate with mult</p>

<p>=========</p>

<h1>TEMP20213</h1>

<p>Filter and graph intersections and deduction as important.</p>

<p>breaking sentences up into chunks and then putting it together based on topic phrases as search through a tree. use consistuent words sum as ORs of index vectors as seeds? Clamp to some max number of nun zeroes, idea is context vectors get added in a similar way.</p>

<p>recursive spanning tree internal node graph -> spanning tree implementation</p>

<p>will RNNs be useful as a syntantic comparison construciton?</p>

<hr />

<p>Across all texts we do a minimum spanning tree. Then we make one for the cluster and so on up to N levels. Phrase by text decomposition. The text of higher levels will be phrases of lower level.</p>

<p>When a new text is seen we allow it to pass through all of the trees above some threshold and then associate with the texts at the leaf nodes.</p>

<hr />

<p>3 ways to compare things</p>

<ul>
<li>RRI
Matrices</li>
</ul>

<p>a = W <em> T -> AVG(AVGROWS (V))
b = Neighborhood(W) </em> N(T)  |>AVGROWS |> AVG</p>

<p>score = w * a + (1 - w) * b</p>

<ul>
<li><p>Graphs
Intersections of words and texts</p></li>
<li><p>RNNs</p></li>
</ul>

<p>weighting by nouns and verbs
paragraphs reduce to noun and verb phrases?</p>

<p>Minimum spanning trees and sub graphs with small RNNs?</p>

<p>AI cannot have a perfect internal model of itself</p>

<p>Seth Lloyd</p>

<hr />

<h1>RESEARCH</h1>

<p>In asking questions such as "cerebellum does or consists of"</p>

<p>can do Intersect(cerebellum, does), Intersect(cerebellum, near does restrict noun)</p>

<p>RNN trained from shortest to longest on concept vectors
start with seed and then sample</p>

<hr />

<p>Abstract Summary</p>

<p>Get top N nouns. Get top noun heavy sentence &amp; top verb heavy for each. Get top sentence overall.</p>

<p>Compression: Eliminate all but nouns,adjective and verbs.
Learn an LSTM trained on POS tags to predict comma insertion</p>

<p>Single document summarization: Important words weigh heavy or are unique or are similar to personal topics</p>

<h2>Deductions?</h2>

<p>Comparisons</p>

<p>Common words, Common neighborhood, shared phrases, phrases neighborhood | filter by Parts of speech, or Entities</p>

<p>Contrasting</p>

<p>Form set of all words. Create target set and its complement. Calculate set differences of the above.</p>

<p>Structural Analogies</p>

<pre><code>         E
      /  |  \
    A - B - C 
           |   /
           D
</code></pre>

<p>e,b,c
d,b,c</p>

<p>e ~= d</p>

<p>a,b,e
c,b,e</p>

<p>a ~= c</p>

<p>Across parts of speech</p>

<p>///////////////////</p>

<p>Document comparison</p>

<p>Getting a quick overview of results maybe clustering
Do split view</p>

<p>/////////////</p>

<p>Agents can be Unsupervised or Semisupervised</p>

<p>Unsupervised are for source -> target searches over partially observed graphs. A direct comparison with target makes it easier.
Searching for a specific sentence or with a where what or why booster. Boosting has top and rest resutls.</p>

<p>Semisupervised uses agents. With graph based methods it can create a matrix and then choose lower values of diag(M * M')   to sample new searches. For step 1 lowest positive weights might be the places to look. phrases can also be extracted and scored as alternate candidates. Perhaps with respect to highest candidate 1 generation. A person, place etc booster can be used/</p>

<p>For non-bigraph perhaps lowest scoring positive weights for next directions. Similarly, lowest scoring positive phrases.</p>

<p>////////////////////\</p>

<p>Would LinAlg make NLP stuff faster?</p>

<p>//////</p>

<p>for reflected random indexing, a words count and the neighbors count influence how large the update will be.</p>

<p>and one way to get structured analogies is to</p>

<p>2 Level Hierarchical Sentence comparison</p>

<p>Level 1 is Sentence just average of their word vectors.
Then in neighborhood:
Then slide window across doing convolutions either on bigrams or chunks\\</p>

<p>the boy kicked the ball and then ran over to his mom
the boy , kicked the , ball and, then ran, over to, his mom</p>

<p>the king of france ruled with an iron fist for decades
the king, of france, ruled with, an iron, fist for, decades</p>

<p>can either use splits 2 at a time as chunks to bind. then weighted sum those.</p>

<p>////////\</p>

<p>with who questions, can take the noun, find the verb then find person ranking by 0.6 noun and 0.4 verb</p>

<p>//////////</p>

<p>Surprise Based facts</p>

<p>This is simply sentence definitions comparisons where polarity (whether sentiment) or negation is detected.</p>

<p>//////////////</p>

<p>Topic Elicitation</p>

<p>restricting neighbors to just words on a page or words not on page might be interesting,.\ or highest edge connected nodes of words on page for more general topic</p>

<p>the most central words or high scoring, knowledge base low  text high words.</p>
