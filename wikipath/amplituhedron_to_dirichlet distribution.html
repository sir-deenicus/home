<meta http-equiv='Content-Type' content='Type=text/html; charset=utf-8'>                  <link rel="stylesheet" type="text/css" href="dagre-d3-simple.css">                     <script src="d3.v3.min.js" charset="utf-8"></script>                    <script type="text/javascript" src="dagre.js"></script>                    <script src="dagre-d3-simple.js"></script>                    <svg id="svg-canvas" ></svg>                     <script>                      renderDotToD3('digraph G {0 [label="Amplituhedron"];1 [label="Projective space"];2 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because In the approach, the ((On shell and off shell|on-shell)) scattering process tree is described by a positive ((Grassmannian)), a structure in ((algebraic geometry)) analogous to a ((convex polytope)), that generalizes the idea of a ((simplex)) in ((projective space)). (0.3)</div>"];3 [label="On shell and off shell"];4 [label="Polytope"];5 [label="Geometry"];6 [label="Convex polytope"];7 [label="Simplex"];8 [label="Algebraic geometry"];9 [label="Principle of locality"];10 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because Amplituhedron theory challenges the notion that space-time ((Principle of locality|locality)) and ((unitarity (physics)|unitarity)) are necessary components of a model of particle interactions. (0.29)</div>"];11 [label="Unitarity (physics)"];12 [label="Subatomic particles"];13 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because When the volume of the amplituhedron is calculated in the ((1/N expansion|planar limit)) of  ((N=4_super_Yang-Mills |N&nbsp;=&nbsp;4 D&nbsp;=&nbsp;4 supersymmetric Yang–Mills theory)), it describes the ((scattering amplitude))s of ((subatomic particles)). (0.29)</div>"];14 [label="1/N expansion"];15 [label="Toy theory"];16 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because Since the planar limit of the N&nbsp;=&nbsp;4 supersymmetric Yang–Mills theory is a ((toy theory)) that does not describe the real world, the relevance of this technique for more realistic quantum field theories is currently unknown, but it provides promising directions for research into theories about the real world. (0.27)</div>"];17 [label="Categorical distribution"];18 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because In probability theory, the points of the standard n-simplex in <math>(n+1)</math>-space are the space of possible parameters (probabilities) of the ((categorical distribution)) on n+1 possible outcomes. (0.37)</div>"];19 [label="Hasse diagram"];20 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because The ((Hasse diagram)) of the face lattice of an n-simplex is isomorphic to the digraph of the (n+1)-((hypercube))s edges, with the hypercubes vertices mapping to each of the n-simplexs elements, including the entire simplex and the null polytope as the extreme points of the lattice (mapped to two opposite vertices on the hypercube). (0.3)</div>"];21 [label="Convex hull"];22 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because Specifically, a k-simplex is a k-dimensional ((polytope)) which is the ((convex hull)) of its k&nbsp;+&nbsp;1 ((Vertex (geometry)|vertices)). (0.3)</div>"];23 [label="Vertex (geometry)"];24 [label="Vector (geometry)"];25 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because where each column of the n&nbsp;×&nbsp;n ((determinant)) is the difference between the ((vector (geometry)|vectors)) representing two vertices. (0.3)</div>"];26 [label="Determinant"];27 [label="Pythagorean theorem"];28 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because  It can be calculated from the first property using the ((Pythagorean theorem)) (choose any of the two square roots), and so the second vector can be completed: (0.3)</div>"];29 [label="Probability theory"];30 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because Especially in numerical applications of ((probability theory)) a ((Graphical projection|projection)) onto the standard simplex is of interest. (0.29)</div>"];31 [label="Graphical projection"];32 [label="Generalized barycentric coordinates"];33 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because These are known as ((generalized barycentric coordinates)), and express every polytope as the image of a simplex: <math>\Delta^{n-1} \twoheadrightarrow P. (0.29)</div>"];34 [label="Dot product"];35 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because  The second property means the ((dot product)) between any pair of the vectors is <math>-1/n</math>. (0.28)</div>"];36 [label="Statistical inference"];37 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because This distribution plays an important role in ((hierarchical Bayesian model))s, because when doing ((statistical inference|inference)) over such models using methods such as ((Gibbs sampling)) or ((variational Bayes)), Dirichlet prior distributions are often marginalized out. (0.44)</div>"];38 [label="Variational Bayes"];39 [label="Prior distribution"];40 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because   This means that in a model consisting of a data point having a categorical distribution with unknown parameter vector p, and (in standard Bayesian style) we choose to treat this parameter as a ((random variable)) and give it a ((prior distribution)) defined using a ((Dirichlet distribution)), then the ((posterior distribution)) of the parameter, after incorporating the knowledge gained from the observed data, is also a Dirichlet. (0.44)</div>"];41 [label="Mixture model"];42 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because  ((mixture model))s and models including mixture components), the Dirichlet distributions are often collapsed out (((marginal distribution|marginalized out))) of the network, which introduces dependencies among the various categorical nodes dependent on a given prior (specifically, their ((joint distribution)) is a ((Dirichlet-multinomial distribution))). (0.41)</div>"];43 [label="Uniform distribution (continuous)"];44 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because   This reflects the fact that a Dirichlet distribution with <math>\boldsymbol\alpha = (1,1,\ldots)</math> has a completely flat shape — essentially, a ((uniform distribution (continuous)|uniform distribution)) over the ((simplex)) of possible values of p. (0.41)</div>"];45 [label="Binomial distribution"];46 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because   The ((joint distribution)) of the same variables with the same Dirichlet-multinomial distribution has two different forms depending on whether it is characterized as a distribution whose domain is over individual categorical nodes or over multinomial-style counts of nodes in each particular category (similar to the distinction between a set of ((Bernoulli distribution|Bernoulli-distributed)) nodes and a single ((binomial distribution|binomial-distributed)) node). (0.41)</div>"];47 [label="Joint distribution"];48 [label="Multinomial distribution"];49 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because </ref> This imprecise usage stems from the fact that it is sometimes convenient to express the outcome of a categorical distribution as a 1-of-K vector (a vector with one element containing a 1 and all other elements containing a 0) rather than as an integer in the range 1 to K; in this form, a categorical distribution is equivalent to a multinomial distribution for a single observation (see below). (0.4)</div>"];50 [label="Posterior predictive distribution"];51 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because The ((posterior predictive distribution)) of a new observation in the above model is the distribution that a new observation <math>\tilde{x}</math> would take given the set <math>\mathbb{X}</math> of N categorical observations. (0.4)</div>"];52 [label="Gibbs sampling"];53 [label="<div style = \'max-width: 300px; padding:5px; max-height: 200px\'>I think this important because   For example, in a ((Dirichlet-multinomial distribution)), which arises commonly in natural language processing models (although not usually with this name) as a result of ((collapsed Gibbs sampling)) where ((Dirichlet distribution))s are collapsed out of a ((Hierarchical Bayesian model)), it is very important to distinguish categorical from multinomial. (0.4)</div>"];54 [label="Dirichlet distribution"];55 [label="Collapsed Gibbs sampling"];56 [label="stopping here =)"];0 -> 1 [];0 -> 3 [];0 -> 4 [];0 -> 5 [];0 -> 6 [];0 -> 7 [];0 -> 8 [];0 -> 9 [];0 -> 11 [];0 -> 12 [];0 -> 14 [];0 -> 15 [];1 -> 2 [];3 -> 2 [];4 -> 2 [];5 -> 2 [];6 -> 2 [];7 -> 2 [];8 -> 2 [];7 -> 4 [];4 -> 22 [];16 -> 7 [];7 -> 17 [];7 -> 19 [];7 -> 21 [];7 -> 23 [];7 -> 24 [];7 -> 26 [];7 -> 27 [];7 -> 29 [];7 -> 31 [];7 -> 32 [];7 -> 34 [];9 -> 10 [];11 -> 10 [];12 -> 13 [];14 -> 13 [];15 -> 16 [];17 -> 18 [];35 -> 17 [];17 -> 36 [];17 -> 38 [];17 -> 39 [];17 -> 41 [];17 -> 43 [];17 -> 45 [];17 -> 47 [];17 -> 48 [];17 -> 50 [];17 -> 52 [];17 -> 54 [];17 -> 55 [];19 -> 20 [];21 -> 22 [];23 -> 22 [];24 -> 25 [];26 -> 25 [];27 -> 28 [];29 -> 30 [];31 -> 30 [];32 -> 33 [];34 -> 35 [];36 -> 37 [];38 -> 37 [];39 -> 40 [];41 -> 42 [];43 -> 44 [];45 -> 46 [];47 -> 46 [];48 -> 49 [];50 -> 51 [];52 -> 53 [];54 -> 53 [];55 -> 53 [];54 -> 56 [];}', "svg");                    </script>