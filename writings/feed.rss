<rss version="2.0">
  <title>Thoughts Feed</title>
  <link>http://www.metarecursive.com/writings</link>
  <description>Trying to get my head around some things</description>
  <language>en-us</language>
  <channel>
    <item>
      <title>Sketching A Take On Creative Intelligence</title>
      <link>http://www.metarecursive.com/writings/Sketching_A_Take_On_Creative_Intelligence.html</link>
      <guid>http://www.metarecursive.com/writings/Sketching_A_Take_On_Creative_Intelligence.html</guid>
      <pubDate>Fri, 06 Jan 2006 00:00:00 GMT</pubDate>
      <description>04/05/2018 - I&amp;#39;m surprised by how long the nature of intelligence has interested me! When I originally wrote this, my only relevant background was a semester long high school psychology course, taken a couple years prior and which I got a C in. Reading through this again, I see that I&amp;#39;d worked out the essence of or got quite close to: the symbol binding problem, advantages of being embodied, the utility of a generative model for counter-factual reasoning, the role of compression, reward learning and most surprising to me of all, as I do not think it obvious: that consciousness might not be important! I remember going back and forth on that point. Although my goal had been to work out the meaning of math, I more worked out what kind of agent can appreciate math? Which compacts to: what does it take to be a true intelligence? The nature of math is related to the world being predictable. 

I see also that I&amp;#39;d vastly underestimated the enormity of the task I had set myself (invent new math? hah!). Inventing new math is working backwards from some structure you want and coming up with a consistent set of rules which connects smoothly with an existing background of rules. I very much doubt I could invent new math—it requires extensive knowledge and skill—but I&amp;#39;ve since tried to connect certain ideas. In particular, information geometry from one direction and adjoint functors from another (what are we doing when we are learning?) but nothing coherent even as yet.



Friday, 09 December 2005&amp;amp;nbsp;- It can be shown that the reality which we experience is one of many possible realities. However, since it is also the case that each reality is in fact but a facet of one true reality then it becomes evident that the term reality is grossly unsuitable for the terming of our perceptual experiences.



We say that it is an amazing fact that the mathematics models reality at all and in fact in not only doing so, but with such remarkable facility as well. However, I will show that this is not at all a surprising result since it is obvious that such a phenomenon should exist. Do not our eyes present to us an image or representation of the world?



Sunday, 11 December 2005&amp;amp;nbsp;- It should be evident that the reality which we experience, that which we see, is but an abstraction, a slimming down of what actually is. &amp;amp;nbsp;A simple example is one from vision, the image of the world, the physical image that is actually projected of the world from the external reality is actually a highly distorted, inverted, curved and 2 dimensional projection of what actually is. The images below attempt to capture the general idea of the retinal image although they showcase depth and colour constancy and are likely not curved in the same manner as the image found on the retina. Note, the shape of the retina is required to know this for sure.&amp;amp;nbsp;

&amp;amp;nbsp;





It is only because of the visual processing abilities of the visual cortex that we can perceive such things as depth, sharp corners and non-inverted images in life. &amp;amp;nbsp;In fact, there was an experiment carried out in which subjects were told to wear vision inverting glasses, a period of a few weeks after which they adjusted and were actually able to reinvert the upside down images and actually see right side up again! A testament to the subjectivity of the sensory experience, if ever there was one. &amp;amp;nbsp;Even sounds, touches and smells are due to stimulations of sensory organs which are then reinterpreted by the special processing units of the brain into the totality of our perceptual experience. It is impossible to show that what we experience is what actually is. &amp;amp;nbsp;For some information of the minds susceptibility to illusions see the site http://psylux.psych.tu-dresden.de/i1/kaw/diverses%20Material/www.illusionworks.com/html/ames_room.html&amp;amp;nbsp;and the references contained within. 



Consider touch then, in the scale of the atomic the distance between the object and the hand is great, separated [proportionally] by many miles. The exchange of forces of electromagnetic origin and the interaction of fields (whose true nature we can only speculate on and perceive mathematically) is felt as tension in the muscles and skin. The tactile receptors of the fingers translate various motions guided by the nature (energies, momentums, field strengths etc.) of the interactions into feelings of texture and temperature. These abstract concepts that are very divided from our everyday experiences are translated by the brain into something that is obviously a cup. The essence here is that the reality which we perceive is necessarily apart from that which is and thus is an abstraction. Reality as seen by our senses is an abstraction.



So also in a similar way can mathematics be called an abstracting of reality. &amp;amp;nbsp;Just as is the case of our perceptual experience abstracting out, stripping down, reducing and converting the external stimuli and experiences into graspable, manageable and manipulable precepts; so also can the same be said to be done in the world of mathematics. &amp;amp;nbsp;Save that here, the manipulated entities are in fact the manipulations of the precepts themselves. &amp;amp;nbsp;In fact, it can be shown that mathematics is simply another way of seeing the world that is passed to us by our senses but because the processing unit is more complex &amp;amp;ndash; the cerebral cortex that is &amp;amp;ndash; it is necessary that the perceptual processing and experience as controlled by the cerebral cortex be richer and more complex and also less assuming (although arguably still fairly presumptive in its operation) than those brought to by the simpler units. The sensory input can be considered to be further processed beyond that done by the simpler units and even arguably, in parallel. But I shall leave that to those more able to discuss the finer details of. &amp;amp;nbsp;



Thus mathematics is another way of seeing the world so to speak. Therefore, it should come as no surprise that it represents the world so well. &amp;amp;nbsp;It does an even better job of translating our sensory experiences than the visual cortex and other similar. It is no surprise that it should describe the world &amp;amp;ldquo;as well as it does&amp;amp;rdquo;, there is no magic or difficult concept involved here.



Friday, 23 December 2005&amp;amp;nbsp;&amp;amp;ndash; It has been some time since I have written an update, I have been a bit lazy in putting to magnetized oriented ions my thoughts. &amp;amp;nbsp;The next logical order in the progression of the cortex-&amp;amp;ldquo;ial&amp;amp;rdquo; line of development is the cerebral. &amp;amp;nbsp;In order that actual &amp;amp;ldquo;meaning&amp;amp;rdquo; and sense be attributable to the world, the external entities must further be processed in a manner such that the natural outgrowth be another layer or level of processing that is able to freely manipulate and make constructs and or symbolisms of all the perceptual data that is received by the sensory organs. We see the world and represent it within (and make it as without) with a special structure that is our take or twist on reality. Our limited senses and brain reconstruct internalised representations of reality based on what minutely small subset of reality that is experienced. As advanced creatures we can further give meaning to these visual and other constructs to make them cerebral by building further upon the sensory data by giving them symbolic attributes. 



Monday, 26 December 2005&amp;amp;nbsp;- Just as the visual system can add depth to a flat 2 dimensional picture, so also is a similar thing done in giving meaning to that which is not apparently meaningful. Thus we see that the plethora of stimuli and perceptions experienced must be given a symbolic attribution in order that they obtain meaning to us. &amp;amp;nbsp;The brain must maintain a semi flexible rigid structure in its attribution process. The significance of this is at the moment unexplorable by me. I shall return to this fact at some later date.&amp;amp;nbsp; I also must learn on schemas and grouping mechanisms of Gestalt theory to properly elucidate certain ideas I have to do with the brains ability and manner of making sense of reality by grouping and ordering.



Nonetheless, I can speak on some general features required to make sense of reality. &amp;amp;nbsp;We know that the visual cortex uses different grouping methods and adds attributes such as depth and colour constancy to our perceptions, things which were not a part of the original reality, to make sense of images. &amp;amp;nbsp;It is thus not so far fetched to say that the cerebral cortex adds attributes of a symbolic nature to attach meaning that is not necessarily so. &amp;amp;nbsp;Although we must assume that the meanings attached are not completely arbitrary and make some sort of sense. &amp;amp;nbsp;In order that it be possible that meaning be applicable to perception, such properties as separation, enumerability, grouping/ordering and countability must exist. It is worth noting that under grouping we find that we can more or less relegate most properties as colour, size and shape etc. &amp;amp;nbsp;Here, a conjecture is made that if a manner of symbolism exists (which is a natural outgrowth for a sense to be made of reality) then it is necessary that a method for their manipulation exists. &amp;amp;nbsp;This is called thought and the externalised expression of which is language. Subset of language and due also to the existence of symbolisms and their manipulations is mathematics.





As recap, we state that just as the visual experience is one whose construction is learned as one gains in experience so also is one&amp;amp;rsquo;s meaningful experience; a set of interlinked symbolisms are attached to a set of diverse sensory inputs and signals to generate our subjective reality. Grouping mechanisms based on the similarities and apparent uniformity of the inputs are used to order the world into colours, furniture, people etc. I argue that large quantities of energetic motion (with the point of comparison being of the molecular scale) is required for anything to be experienced by humans in this world. To identify a set of energies and interactions one sees that they are in motion relative to some background. We see those energies, which move and seem to have constraints in their separation and call them masses. We notice tables, which might seem very unenergetic on the surface but are in fact reacting at a very large level with photons and phonons and other such. Strong fields (that is anything which creates any form of resistance for the human) and not the exclusion principle, is principal to our tactile experience of reality. 



Since symbolic meaning is due to a set of interlinked constructs whose generation is due to experience and is relative, it is necessary that not only is our experience of reality subjective and limited, it is limited also to things which can be compared and stated relative to one another. The last is a limit that it is hoped can be consciously overcame once things are cast in a different perspective. &amp;amp;nbsp;Indeed the general idea is that the manipulation of perception, jokingly called takes (as in my Take on that) is the next level of abstraction, which allows one to experience things at a higher level than before. Allowing perhaps, that more and deeper questions be answered by shedding a new and brighter light unto things that are already there but putting to light things that were once in shade. Or at least, that is the hope. &amp;amp;nbsp; &amp;amp;nbsp;





For meaning to exist there must be a concept of memory, construction, significance and the ability to divorce, separate, become more independent. The nature of memory is well studied and I shall not go into it here beyond stating that having a set of linked, grouped and ordered set of various sets of experienced stimuli for later manipulation is pivotal for an entity&amp;amp;rsquo;s intellectual ability. Of significance one might suppose that a method for attachment to or ability of having (dis)preferences for a set of stimuli for which an ordering could possibly be a representation of tree or Jack exists. Concurrently, we might assume that the concept of hurt might lead to one of satisfaction to happiness and its opposite and then to full fledged emotional ability developing. Why, one might even make the tentative suggestion that in order to do mathematics most proficiently, one must be an emotionally complex entity. &amp;amp;nbsp;One imagines that the ability to create without external stimuli, to divorce entirely from the external sensory inputs to create a representational what if world would prove most advantageous to a creature&amp;amp;rsquo;s survival. It could be that a set of these representations, created from a set of interlinked stored stimuli and various past experiences for which there existed a strong positive emotion. Such a creature could use sets with different configurations and choose one which resonates strongest with some hidden significance attribution process. This would allow one to &amp;amp;rdquo;do&amp;amp;rdquo; without doing, a most useful ability. The ability to dream shows that the brain is capable of independent construction or manipulations of stimuli regardless of external influences to construct a sensory experience independent of, but strongly based on the external world. A construction within the construction. One necessarily more flawed than the first, indeed one might wonder if the disparity between imaginings and reality might shed a glimpse into that between reconstructed reality and actual reality. Perhaps they are proportional? A look into the constraints&amp;amp;hellip;&amp;amp;nbsp;but I digress and get ahead of myself.



The key is in the ability to divorce into higher and higher levels of independence. For example, one might not require spacial reconstructions but simply un-solidified concepts of where. The gained flexibility allows one to deal with and reapply solutions into strange situations most effectively. Indeed, if the manipulations focused less on stimuli but rather on the totality of the stored experiences &amp;amp;ndash; the manipulations of which create the strongest resonances or feelings of satisfaction &amp;amp;ndash; one might argue that meaning emerges within a hop, a skip and a step. &amp;amp;nbsp;But then, where does consciousness enter into this and is it required to do mathematics? This I cannot answer but there is a line of reason which may be explored that may shed some light into this. Among the other mentioned processes, memory is key to one&amp;amp;rsquo;s ability in mental operations. One might also assume that with manipulations of mental processes - especially combinations of those which result in meaning - that awareness might occur. Indeed, one might presume a line of development which encompasses such phenomena as self preservation and avoidance behaviours to conceptions of satisfaction, contentment and preference, to delight and happiness in one&amp;amp;rsquo;s accomplishments. Properly acquiring a trickily located piece of food being an example perhaps. 



With such complicated processes and their manipulation, it is not unlikely that consciousness might occur. Nonetheless, the placing or development of consciousness is not required for an accurate theory of the underlying nature of the mind&amp;amp;rsquo;s ability with mathematics. Awareness and an ability to appreciate, simply, dictate the ability to do mathematics that is personally meaningful. The ability to divorce or abstract and assign significance to resonant ideas (implying ability for preferences and internal resonance with developed conceptual structures) as well as to construct and manipulate conceptual sets, which are operated on beyond the bounds of the existent concept sets which are most directly related to reality are all that are requisite for creative mathematics to be done. Where or how and if consciousness plays a part or is even required at all is unknown to me. There is however the supposition that if all the afore mentioned requirements for meaningful and creative mathematics are met, then it is highly probable that a conscious mind exists.





Friday, 06 January 2006 &amp;amp;ndash; As stated, the ability to disconnect such that a language exists must imply that the entity is able to manipulate its experiences and present them in a generic form (condensable to mediums such as text, music etc.) and mapped unto any capable device or mind. Ability for such abstraction allows that such a mind can do mathematics. &amp;amp;nbsp;There exist for man a preference for resonant sounds and strong beats; one can only conjecture that such is an indication of the underlying organizational structure of the human mind. &amp;amp;nbsp;The creation of music involves the manipulations of our perceptions of sound and our experience of it, specifically the combinations most preferable or resonant to us. This fact is something worth exploring, music points to some internal mental logical structure and hence a look into an ability to do math among others. The same statements can as well be said about art. Art is a direct projection of our internal visual experience, indeed, the so called abstract pieces (despite my dislike for them) are projections of one level of abstraction beyond the visual. They are the presentation of the manipulation of emotional and preferential precepts presented in a form and structure which itself is an indication into the structure of our mental working. In fact I say here that the interpreting of abstract art is silly since it is a personalized process. The work of the artist is mapped unto the map of the individual, where the projection (of the artists internal experience) is the art, since the minds are not isomorphic and the projection is many magnitudes of dimensions less than the artists experience, the reconstruction in the brain is a reflection of the self and differences in one&amp;amp;rsquo;s mental preferences. I note also that the artist need not have a well developed meaning, the work may simply be a direct projection and one not done necessarily with special meaning beyond expression. &amp;amp;nbsp;One level of abstraction beyond art (although not necessarily abstract art and highly complex works of music) is that of written works. &amp;amp;nbsp;Although seeming simpler, due to our constant use of it, language constructs involve the manipulations of the very symbolisms which have been attached to give meaning and thus the entire sensual experience. With poetry one finds the focus is placed on audio resonance, with creative works, the invocation of complex visuals and finally non-fiction the ordering and reordering of our schemas or ordering process itself. 



With that, we return to the example of kinematics. In addition to an internalised visual representation of the world, there exists a physical one, one whose laws are noted and ordered to work with one&amp;amp;rsquo;s perceptual experience. The shooting of a basketball involves intense calculations, precision and control of various things as trajectories, acceleration, forces and impulses &amp;amp;ndash; all done after multiple transformations of spaces. Thus a brain&amp;amp;rsquo;s innate ability to perform advanced calculations innately is displayed. &amp;amp;nbsp;If there exists a language and a reasoning process with, implying meaning exists in the world and if this language has been developed to an even higher level of abstraction where the language has been given a symbolism that allows a projection unto external media then we find that the development of a mathematics is almost inevitable. Err, writing exists.

&amp;amp;nbsp;

With a culture capable of creating artistic works which are harmonious to one&amp;amp;rsquo;s conception of beauty and an ability to disconnect and relate to such a level that writing exists then it is likely that a curiosity for the environment might develop. With consciousness or awareness an inquisition into why events occur to the self might be undertaken and with the ability to communicate meaningfully an interest into what else might be capable of such. Nonetheless the point of focus is in an interest in the environment, perhaps motivated by all this and also a need to manipulate to better solve problems. &amp;amp;nbsp;An arithmetic notion is inevitably the first work of a culture capable of writing. The concept of language is extended to the symbolisms which pertain to one&amp;amp;rsquo;s ordering of the environment.



If music, art etc. can be described as the manipulation and ordering of one&amp;amp;rsquo;s experience of elements of the set of perceived sounds (or visuals, with periodicity and harmonics being a frequent theme) then mathematics would be the manipulation of one&amp;amp;rsquo;s representation of the physical behaviour of reality and the very underlying schema that allow for understanding itself. The physical experience of reality is made bare for the mind, this physical understanding underpins our very experience of reality. And here finally it becomes possible to state the thesis of this work. &amp;amp;nbsp;The totality of experience is a synthesis of the sum &amp;amp;nbsp;of our perceptual experiences mapped unto a internalised &amp;amp;nbsp;physical representation of which the relating structure (from physical to visual to audible etc.) is our conceptual [logical] orderings. &amp;amp;nbsp;To do mathematics beyond geometry one must also be able to manipulate the abstract logical ordering. This relational structure is more directly connected to the external reality as it is the manipulation of said precepts and thus speaks more on reality than our visual experience of it.&amp;amp;nbsp; Indeed after an arithmetic, a geometry is almost always developed, I argue, and that although a geometry is often strongly biased towards a culture (with the strong emphasis being the visual aspect of the physical) an algebraic method is often most general. Thus a geometry is biased but an algebra or algebraic geometry very much less so, the same bias again for a topology but not an algebraic topology etc. Indeed the task will be to develop a method to describe these concepts of mind and reducing all mathematical concepts to these. The first task will be done with crisp sets and then again, more accurately with the concept of subdefinite or vague sets. &amp;amp;nbsp;Mathematics is a highly abstract conceptual Take or Hue (similar to a space) that places for manipulation one&amp;amp;rsquo;s experience of reality. &amp;amp;nbsp;It is the manipulation of the constituents of our perceptual experience and also the underlying structural organizations of a relating structure whose patterns must be some harmonic of an external reality. That which we feel directly is a subset of such a concept. Thus mathematics is an ultimate manner of perception &amp;amp;ndash;one capable of making statements on itself. Mathematics is not invented or discovered, it is experienced, it is the human experience revealed in its totality.



And so I have developed these ideas to a state that which I am satisfied with. I can now return to these idea and give a far more coherent and mathematical treatment to lay out a new mathematics I think. Worth exploring is the breaking down of different mathematical concepts to the constructs of this theory. As a first year university math student I am struggling to the droll nature of exercises, I see this as a fun synthesis that should make interesting many things. I will start with limits. :D </description>
    </item>
    <item>
      <title>On the Requirements of Artificial Intelligence</title>
      <link>http://www.metarecursive.com/writings/On_the_Requirements_of_Artificial_Intelligence.htm</link>
      <guid>http://www.metarecursive.com/writings/On_the_Requirements_of_Artificial_Intelligence.htm</guid>
      <pubDate>Tue, 19 Jul 2011 00:00:00 GMT</pubDate>
      <description>19 July 2011

Before AI can exist a number of things must first occur. These things which must occur will also make programming easier and more efficient. In fact the two are strongly related and make AI very likely for reasons other than exponentially increasing transistor density. In fact I am skeptical that the obsessions in this direction will bear any fruit at all. But I do think the following line of thinking will bear fruit.

Computers will continue to increase in complexity and power
The demands and complexities of the tasks on computers will continue to improve
Our ability to program them must improve. Scaling with man power will not prove sufficient
Part of this improvements will mean smarter compilers, programs and manner of tackling problems

It then follows that anything which makes programming easier, more efficient and require less hand holding of the computer must be doing more on its own and doing it in a way that is increasingly less stupid. The more general these systems become, the closer to AI we get. Hence those working on programming languages and computer design software and tools are in fact working on a very simple nascent version of AI.

So we can define AI as the end result of the evolutionary process of making a computing device that continually does more with less hardcoded instructions - i.e. the process of making a computer more and more useful, less and less annoying. We are in our own way following in the path of nature, going from hard coded inflexible instincts to ever more flexible behavourial patterns. I will also clarify that when I say AI, I mean basic mammal or corvid like flexibility and not human level general intelligence. Human level and perhaps beyond I feel are likely but the dimensions of that space are so immense that I can offer no predictions - it could happen in a decade or a century,. toTo my 3D perception it may be many miles away but there might be a shortcut in 6D. Something else worth noting is that although the AI might not be so intelligent in an absolute sense, if its niche is for example in complex reasoning on genetic sequences its basic mammalian like flexibility may far surpass a human in certain aspects of reasoning. Beaten only by creativity and holistic reasoning of the human. The AIs may be considered autistic in a sense.

Before AI can exist computer programming must become easier and not take so much effort. Otherwise, any device that can contain the specification of an AI will collapse into a black hole. We need higher level languages, we need to make it easier for programs to modify themselves, we need to remove the separation between machine learning and machine instructing (programming) and more importantly; we must also better understand the theory of programming as it is one of specification and intimately tied with the concept of what is knowable.

Specifically, Kolmogorov Complexity is in essence what is finitely describable - it is about what can be compressed in the form of a generating process specified in a computer language. And if we do not have a better understanding of how to specify such things and what it means to write them efficiently with certain guarantees then we have no hope in creating an AI. With that out of the way, the below is a list of things that will make programming easier and hence is a list of preconditions for AI.

The last is starred because it is the only thing on the list that is not a requirement but instead will develop hand in hand with the field. Which highlights the fact that no where did I mention neurobiology or psychology. I do not believe that we need to understand how the brain works to get to AI. Indeed I think such an approach would be counter productive by not be leveraging the fact that computers are a fundamentally different platform (high frequency computation, low latency, perfect recall) and hence must operate on a rather different set of basic principles even if the outputs are measurably within the same plane of output behaviours. Nonetheless it would be foolish not to cross pollinate and get guidance of what an Intelligence cannot be composed of by studying humans and other animals.

Mathematics is the most effective way to lossfully compress knowledge contained by a set of concepts. For simpler systems we can get a full description of the process with no loss, with the models growing in size with the complexity involved and accuracy required. This is all mathematics is and is not something limited to numbers or calculus. Numbers and calculus are an example of this compression, reduction or abstraction process. Because maths need not be about numbers and while proofs are necessary as a measure of consistency, these are not the first thing that should come to mind when I suggest that computer programming needs a mathematical description. Instead I mean a tool that makes it easier to reason about complex things by compressing them without losing fidelity so that we can more readily come to meaningful conclusions, limits and constraints. There have been several related attempts using Set Theory, Type Theory, Model Theory, Domain Theory and Category Theory - with the best in my opinion being Category theory, being able to encompass all of the prior.

However they all remain too difficult and locked up in academia with no practical use or even explanation to the engineer. And in particular only really help with the smaller details of algorithms and data structures and not the bigger picture and actual difficulty of implementation and architecture. Category theory arguably does this but in a way that ties your hands to its axioms. Attempts using diagrams and specifications are more common in industry but are too complex and have gotten lost in their own details - resulting in specifications that take even more work than what is being specified itself and quadrupling the work load. Hence we need a simple math that can be employed to get and communicate meaningful ideas and test limits before work starts. This will make the work less blind. By being more aware of programming we reap the benefits that geometry gave to construction and bootstrap ever more complex processes. What makes this even more interesting is that as stated, a theory or mathematics of programming is also a mathematics of what is knowable. It is not unlikely that invariants from this theory will be useful in AI and a branch specifically dealing with that will form.

AI may not necessarily be superior to human intelligence in all aspects. This is because the concept of insights and creativity, the requirement of abstraction are all due to human limits of speed and recall. A computer has no such limits with respect to us so the question becomes does this mean computers cannot be creative?</description>
    </item>
    <item>
      <title>The Future of Computers</title>
      <link>http://www.metarecursive.com/writings/The_Future_of_Computers.htm</link>
      <guid>http://www.metarecursive.com/writings/The_Future_of_Computers.htm</guid>
      <pubDate>Thu, 16 Aug 2012 00:00:00 GMT</pubDate>
      <description>Thursday, August 16, 2012 - 10:50 PM

So right now we have a lot of computing power and are able to generate a bunch of data and are starting to get better at learning from data. But there is an inefficiency in the way current learning algorithms are implemented. The inefficiency is that each class of learning algorithms is not being matched to the most appropriate set of hardware. More than just more parallelism, we also need more heterogeneous processor architectures. When each class of algorithm is running on the most appropriate platform, it will be far quicker and hence a lot more energy efficient. Here is my breakdown:

Bayesian and other Probabilistic or Sampling Based Methods:  Probabilistic Chips (should also go well with mobile)

Methods Based on Linear Algebra and Optimization: GPU/Stream Processing like Architectures

Neural Network and Pattern Based Classifiers: memristors

And Current computers are best for exactly the techniques they are least used for: symbolic/logic and program tree based methods. And also heuristic search + non gradient based optimization

Of course, these categories are not hard and fast. You can have aspects of program tree based methods on GPU or probabilistic chips and aspects of variational Bayesian methods and all of Neural Nets do well on GPU. There&amp;#39;s also the fact that memristors don&amp;#39;t exist yet so we don&amp;#39;t really know much on their limits or potential. But the bottom line is that there is a lot to be gained simply by matching algorithms to the best possible substrate.</description>
    </item>
    <item>
      <title>Compressibility is a Key Concept</title>
      <link>http://www.metarecursive.com/writings/Compressibility_is_a_Key_Concept.htm</link>
      <guid>http://www.metarecursive.com/writings/Compressibility_is_a_Key_Concept.htm</guid>
      <pubDate>Sun, 09 Sep 2012 00:00:00 GMT</pubDate>
      <description>Saturday, June 30, 2012 - 4:02 PM edited Sunday, September 09, 2012 - 2:00 PM

In the future, I think we will look back at how incomplete our model of probability was because it did not fully incorporate notions of compressibility.

In http://www.johndcook.com/blog/2012/09/07/limits-of-statistics/ Cook states:

The central dogma of statistics is that data should be viewed as realizations of random variables. This has been a very fruitful idea, but it has its limits. It’s a reification of the world. And like all reifications, it eventually becomes invisible to those who rely on it.

This jives with the qualms I have with that aspect of the subject, which I will now elucidate.

Notions of Compressibility and Probability should really go hand in hand. For example, if someone says something is unpredictable (which is clearly not true for anything interesting) what do they mean? They mean that they think the phenomenon is so complex it would be hard to build a predictive model that does better than random without building the entire thing itself. Did you notice something implicit in that definition?

Consider the sequence &amp;quot;010101010..&amp;quot;, what is the best way to characterize this sequence?

Leave that thought hanging a moment and consider the two infinite sequences; A:&amp;quot;001001001001...&amp;quot; and B:&amp;quot;111000010001...&amp;quot;.

The probability of randomly picking a &amp;quot;1&amp;quot; in both of these sequences is 1/3 yet there is something different about one of the two. One of them is less regular, more random. Can you guess which? Although instinctually people tend to generate sequences like A when trying to be random, when compared like this it is clear that B is more random. We can arbitrarily approach 100% confidence in knowing the next digit in sequence A but can never do better than random for sequence B.

Probability theory is not equipped with the vocabulary to tell the two apart. And while you can certainly build a Markov Chain conditioned on prior history that can properly represent this sequence, an umbrella term for what this concept falls under is not easily expressed in the language or probability. There are valiant attempts in model picking, Minimum Description Lengths and various information criterion to do so and while they see practical use, proper use is as yet an art form.

The word which accurately and cleanly describes all these concepts is compressibility. Sequence A can easily be compressed to a Markov chain or computer program. Sequence B cannot. The shortest computer program amongst arbitrarily many which can model A is the most preferable for no other reason than it minimized complexity. That makes that explanation most robust. Unfortunately the Halting Problem makes this metric incomputable. As said attempts such as regularization, early stopping, adding noise, dropping features are all ad hoc methods to get around this. MDL is the cleanest but still remains messy and while computability theory permanently force this as an art form perhaps there is some principled way we can account for Compressibility and Complexity...

When someone says something is unpredictable, what they really mean is that this thing is incompressible, it is already in a state of maximal entropy. Incompressible things cannot be learned. It is an interesting parallel that in physical systems, no work can be squeezed from states of maximal entropy.</description>
    </item>
    <item>
      <title>On the Unreasonable Effectiveness of Mathematics</title>
      <link>http://www.metarecursive.com/writings/On_the_Unreasonable_Effectiveness_of_Mathematics.htm</link>
      <guid>http://www.metarecursive.com/writings/On_the_Unreasonable_Effectiveness_of_Mathematics.htm</guid>
      <pubDate>Thu, 27 Sep 2012 00:00:00 GMT</pubDate>
      <description>27 Sep 2012

Some people like to talk about the unreasonable effectiveness of mathematics in describing nature. But I&amp;#39;ve never been impressed with how well math describes the universe. I&amp;#39;d be more surprised if it didn&amp;#39;t. I find it very hard to express this concept and have only recently gained the sophistication to even attempt this but here it goes:

Math that predicts reality is inevitable in a universe where learning occurs.

The basic idea is that it is tautological that mathematics works. A universe you can learn about and understand is one whose description is compressible. It turns out that math is one of those ways to describe it. Programs are another (constructive only). If the universe was not compressible we could not, by definition learn anything about it and math would not work.

You can&amp;#39;t make predictions about an uncompressible system. If you can learn about the universe then there exists a system that captures a compressed description. A compressed description is by definition one that is able to predict beyond its size (Bastardizing Kolmogorov Complexity a bit here). Math is one language with which you can create a compressed description of nature. Math that predicts reality is inevitable in a universe where learning occurs.﻿

The other issue is partially anthropic; an inconsistent universe would be less likely to have us here being awed that a methodology based on the consistent applications of rules works at describing it. If one posits that a certain type of idea of symmetry well captures some properties of particles then I don&amp;#39;t think it is surprising when a consistent theory of symmetry explored and unfolded has the ability to predict the properties of any system consistent with its basic operations and axioms. Whether the system lives on paper or out there in the universe.

Any consistent system whose basic behaviour is explainable with a theory or algebra of symmetry should be predictable when the theory is further expanded in a deductive manner. Just a more involved example of the not at all surprising phenomenon that a pizza divided amongst 4 people yields 2 pieces each.

What I am trying to say is if this mathematical model has succesfully captured/compressed the phenemenom then its internal rules must be consistent with the phenemom&amp;#39;s rules. So if you explore the mathematical system, so long as you are consistent in your expansion, then it follows that your discoveries may also be observed one day in the universe - which when viewed this way, the universe is a subset of mathematics. One of many possible expressions and choices of space, variables and algebras. Hence the only difference between the space of all possible mathematics and specific realities is the placing of constraints and removal of &amp;#39;simplifying&amp;#39; assumptions.

Science then is searching this mathematical space for a description that sufficiently compresses an observed phenemenom.

Taking the analogy of a graph/tree, I think the difference between Lie Algebras and dividing pizza is one of depth not structure.  The same procedures of constructing the tree is used for both its just the one is of greater depth.

Note that incompressible or actually, of high Kolmogorov complexity need not be inconsistent. So we can talk about universes whose structure are of varying KComplexity as related to how difficult it would be to learn. Life would be possible but intelligent life less likely as Evolution would have a more complex search space. Intelligent life if it existed would be less likely to develop Occam&amp;#39;s razor as a guiding principle.

Most things of interest to us are actually of fairly low KC and so it comes as no surprise that &amp;quot;inferring&amp;quot; the rules of the automaton are possible at all. I know my terminology is very biased to computer science but I hope I&amp;#39;ve managed to make clearer my key thoughts on this.

Here are my assumptions I hold as true: The Strong Church turing thesis. Everything in the universe is computable and by a [quantum] turing machine. I do not think the universe is a computer or a simulation but I  think all its phenomena are computable.

Kolmogorov complexity is very important because it is actually about predictability (its also closely related to entropy). For example consider an entity running an experiment. They gather a lot of data. Now they want to find a set of equations that properly captures the data. The higher the KC of the generating function of the data, the harder it is to learn. That the universe has exploitable structure means one can get decent results from combinatorial search. That the functions are low KC means they can be learned  in a decent amount of time. If the laws of nature were of high KC then evolution could not have developed so much in this time.

What I am saying is that it is not that impressive that math works because all it is is a language to encode compressed descriptions any place where learning occurs most have such things. What is more interesting is how much structure there is and how &amp;quot;easy&amp;quot; most things are. But then, thinking further, if one imagines a probability distribution described by a cellular automaton generating all possible physics and all possible universes optimally (not saying thats how it is), it is sensible to assume simpler universes would be more likely to be computed. As such it should come as no surprise that we are in one such.

It is true that in principle the Laplace could have deduced much of quantum mechanics but in practice such a construction is (not so different than constructing a tree) is too difficult. I don&amp;#39;t agree that physics is so different either. I think for other subjects, biology say, the issue is that our tools  were not sufficiently developed to allow us to treat  them in the same manner as physics. And they are more complex so harder to do without querying the environment and less amenable to pure deduction. Consider that it takes more bits to specify a particular human than say the entire universe and just its rules unfolded. Just as while the equation for the Mandlebrot set is low complexity,to specify a small part of a specific location takes more bits and more complexity.

...nature is predictable which means that the idea that it has rules which it follows is valid. Capturing these rules is learning a model - a compressed description of the phenomenon and the pattern of its rules. If we imagine these rules as a large tree to be inferred or a program or cellular automaton or whatever, then early math based on reality could be one of the top nodes or a simple rule from the early states.  Mathematics might be argued to be a way of learning how to construct this tree - learned by sampling lots of experiences with reality. That is math is learning a compressed desc of the rules of the rules of nature and executing this &amp;quot;program&amp;quot;.

As such following mathematics builds up possible programs that are consistent with the &amp;quot;Nature Tree&amp;quot; even if not based on any phenomenon. So later on when someone finds a phenomenon that matches a theory, because Math follows the same structure of applying rules that is in built with nature the theory will be a valid way to describe nature. And the difference in Reality and Math is one of energy constraints on what are valid physical trees.

In this view Mathematics is meta-learning of the patterns in nature. It is also my argument that it is fairly simple (we can learn it after all and our brains are only decently powerful). I mean it doesn&amp;#39;t even use all of First Order Logic IIRC. One wonders a world with multiple possible construction methods - one with magic perhaps? Hah.

...

It follows that if you can construct rules then any system that can execute those rules would be able to simulate how nature would behave. It turns out that we have a universe which for some reason has these universal executors.

I believe the Church turing thesis is correct and as such believe the brain is one such executor and that only intuitionistic concepts of constructable mathematics are valid in the universe. As well, in a world not complex enough to have UTMs consciousness could not exist based on my thinking. One key difference between our brain and a computer is that our brains are much better at manipulating symbolic infinity and so we are better at doing mathematics (amongst many other reasons to do with exploiting structure).</description>
    </item>
    <item>
      <title>Compression for Clustering</title>
      <link>http://www.metarecursive.com/writings/Compression for Clustering.html</link>
      <guid>http://www.metarecursive.com/writings/Compression for Clustering.html</guid>
      <pubDate>Sun, 24 Feb 2013 00:00:00 GMT</pubDate>
      <description>A refrain that I repeat over and over is that many things can be viewed as a prediction game and that better prediction is just better compression.  And when someone says something is unpredictable they actually mean this data is highly incompressible - maybe the data generating function is of high kolmogorov complexity or comes from a treacherous search space. 

So for example I can tell you that in their core essence, there is not much difference between a scientist and gzip or winzip (actually the best comparison would be 7zip as I explain later). A scientist is trying to make sense of data by creating a simpler description that makes predictions - i.e. compression.

This can be taken  literally by doing actual science using something like 7-zip. Using a very simple technique called normalized compression distance - a measure of the similarity between two bit strings by approximating their normalized information distance, itself incomputable since it is based on the Kolmogorov complexity function. For example, one can compare and cluster genomes into phylogenetic trees using 7zip. 

To do so I first used the PPMD algo from the 7zip library to compare pairs of files (something like LZW/gzip will not do due to having too small a context though using a larger dictionary cancels this). I then wrote an implementation of the nearest-neighbor chain algorithm for clustering (in F#, no errors, it just worked on first go! Also, in retrospect I see this is easily adaptable to huffman coding). 

Of course I don&amp;#39;t have the time or bandwidth to compare whole genomes so settled for a couple chromosomes across a a handful of monkeys/apes (gorilla, chimp, baboon, rhesus monkey, macaque and human). You can see the resulting graph below even though looking at a single chromosome or two is not very representative. It ran through the O(N^2) algorithm on the ~ 1GB data set in a few minutes on my machine.

I did the same with some 30 or so text documents (much smaller so faster) and got a result much more easily judged. 

Marcus Hutter has an excellent argument for why being able to compress English text at its expected entropy rate ~ 1 bit per character would necessarily signify an AI. In fact top entries in his competition make use of Neural Networks in their compression algorithm and take nearly 10 hours to compress 1GB, blurring the illusory boundaries between Machine Learning and compression. Alas, while impressive they are still mostly limited to the word level. To get better concepts will need to be incorporated.

You can look at it this way. Near perfect compression implies a couple of things. It implies the ability to understand ambiguous contexts, non-specific references and vague language. Such an algorithm would require sophisticated knowledge of not just grammar but knowledge and culture, hence it could read and write English as well as any human.

Consider a simple example of compressing a series of digits such as 14159265... This is at first incompressible but if you recognized it as the decimal portion of pi you could simply represent it symbolically. Of course utilizing all of human knowledge as the model makes a mere look up table and is counter to Occam&amp;#39;s Razor. It would be intelligence through brute force stupidity which is not even computationally tractable - hence the need for powerful but smaller models. The more intelligent the model the more it minimizes the data&amp;#39;s minimum description length: Length(Model) - log2(probability of data Given Model). We prefer the smaller model not because it is necessarily better but because all things being equal it&amp;#39;s much less likely to break way into the future.

I hope after reading this you now have an appreciation for the intelligence behind 7zip (the best open source compression library btw) and how learning, compression, coding and prediction are all related.


The below are the clusters created from the NCDs.








Below is the code. It was not written with optimality in mind although it is still fairly performant (I like that word so nya).


         lit.er.al.ly   
adv.




Usage Problem


Usage Note: For more than a hundred years, critics have remarked on the incoherency of using literally in a way that suggests the exact opposite of its primary sense of &amp;quot;in a manner that accords with the literal sense of the words.&amp;quot; In 1926, for example, H.W. Fowler cited the example &amp;quot;The 300,000 Unionists ... will be literally thrown to the wolves.&amp;quot; The practice does not stem from a change in the meaning of literally itselfif it did, the word would long since have come to mean &amp;quot;virtually&amp;quot; or &amp;quot;figuratively&amp;quot;but from a natural tendency to use the word as a general intensive, as in They had literally no help from the government on the project, where no contrast with the figurative sense of the words is intended. 
   </description>
    </item>
    <item>
      <title>Models and Prediction</title>
      <link>http://www.metarecursive.com/writings/Models_and_Prediction.htm</link>
      <guid>http://www.metarecursive.com/writings/Models_and_Prediction.htm</guid>
      <pubDate>Mon, 27 May 2013 00:00:00 GMT</pubDate>
      <description>In which I talk about Zones of Thought and offer a much clearer =D alternative framework to Popper on how scientific theories are established

27 May 2013

There are two ways for a model to be bad. It can be full of wrong assumptions or it can overfit.  The typical example of overfitting is predicting the orbits of planets. The original theory of epicycles - where the Earth was the center of the universe - worked well enough until new data caused a revision that ever increased the complexity. The theory exhibited too much variance. And yet, it was still predictive and so cannot truly be labeled as wrong.

Indeed no model is ever fully wrong - or right - but a good modeler tries to be closer to right than wrong most of the time (i.e. good predictive accuracy). One example of a technique to achieve this (not the only) is a Bayesian model over possible theories. One uses priors as a regularizer - up bias, reduce variance to reduce overfitting - but also incorporates data to help discount bad theories.

But if a model is predictive why should one care if it overfits? Occam&amp;#39;s Razor is often mistakenly used to defend simpler theories in a way that suggests that they are inherently better. But this is not true. Occam&amp;#39;s Razor is only really saying prefer the model that 1.) saves energy and space 2.) is simpler.  Preferring (1) is a matter of economy, why waste resources when there is no need? (2) is to avoid overfitting. If I have 5 points I can hit them all with a fifth degree polynomial - seemingly better than a simple linear fit. But this commits you to a particular model in the space of polynomials, more likely to go wrong than not. A simple linear fit makes far fewer commitments on the shape of the data and will do well enough in the near.

Focusing more on (1), what does it mean for a theory to save energy and space? We can pull on two types of complexity. Kolmogorov Complexity is a kind of descriptive complexity. How much space would it take to write down everything there is to know about this thing? Simpler more compressible things take less space. One can also take less space by ignoring unimportant details - a kind of lossy compression - i.e. general relativity vs Newtonian Gravity.

The energy aspect is in terms of computational complexity, which basically asks, how much energy would it take to compute an answer to this question? General relativity is not much more complex than Newton&amp;#39;s Theory in terms of descriptive complexity - it&amp;#39;s just a couple lines of partial differential equations. But it is much more computationally/energy intensive to calculate. Most of the time one does not need that level of nuance. Quantum Mechanics is a simpler theory in the sense of requiring both less computation^[1] and not much descriptive complexity.

Interestingly, this notion of better theories not being more correct but instead a trade off between being economical and predictive cleanly addresses the question of whether science allows us to paint an ever more accurate picture of the universe. The answer is no - although the question itself is ill defined. But what is true is that with science, we are continually improving our ability to model and predict things about the world. That is essentially what one understanding of Quantum Mechanics called QBayes says.

It states that Quantum Mechanics is the correct probability theory, the most accurate way so far to incorporate information about the universe and make predictions. That is, the wave function is not some many branched World-Tree of Fates nor a distribution over some underlying reality (actually a recent result shows this as not possible so anyone who believes this must believe Quantum mechanics is fundamentally mistaken - i.e. Bell Inequality inappropriate) but instead it is better to think of the world as if it was made up of tiny little things that exhibited their behavior in a manner commensurate with sampling from a complex-number probability space. Doing this will allow you to best predict what things in the universe will do. It does not say that the actual nature of the universe is probabilistic, only that you can hardly do better than that for some reason. It is very important to know that Quantum Mechanics does not demolish determinism. Indeed the World Tree Interpretation (aka many worlds) is deterministic - it can actually be a useful tool in thinking about some type of problems, even if the universe tree is not constantly growing new branches (sloppy, talking about the universe as if it is something is meaningless).

What about wrongness of assumptions? The easiest theories to spot as being wrong headed are often simple while failing to be a generalization of the phenomenon. Often the domain of so called cranks. Wrongly complex theories are much harder to do. In fact I believe with the exception of Mathematics (is the Riemann Hypothesis Proven? The answer is not known), humans are not intelligent enough to do that kind of thing. This makes a kind of sense, a wrongly complex thing has to be obviously wrong to some being, that you used complex suggests that you&amp;#39;re just not smart enough to see how trivial and woefully inadequate the assumptions are. What about consistency?

Well it turns out consistency is not really a useful metric. All physics theories have a number of contradictions or are resistant to axiomatization. That is fine - no big deal. We are adept at maneuvering around the issues. Where as one can have a consistent but cumbersome theory of epicycles that is clearly not the best tool. The key issue is in contradictory assumptions/postulates more than mere inconsistencies within the theory.

So in summary, you want economical yet predictive theories. No one actually tries to falsify theories, instead they keep adjusting and fiddling until a clearly superior (low Kolmogorov and computational complexity, high predictability) alternative arrives .e.g Aether vs Special Relativity.

: 1  - On what  grounds do I justify that General Relativity is more computationally intensive than quantum mechanics? Well, to mere polynomial machines, everything can be done but with exponential slow down. This muddies intuition. Another approach is to invert the question and ask instead: how much computation could a universe, where particular aspects of theory are physical and harnessable in principle, do? If you have read Vernor Vinge&amp;#39;s Fire Upon Deep then you will have read about Zones of Thought. Wikipedia describes it as:

The novel posits that space around the Milky Way is divided into concentric layers called Zones, each being constrained by different laws of physics and each allowing for different degrees of biological and technological advancement. The innermost, the &amp;quot;Unthinking Depths&amp;quot;, surrounds the galactic core and is incapable of supporting advanced life forms at all. The next layer, the &amp;quot;Slow Zone&amp;quot;, is roughly equivalent to the real world in behavior and potential. Further out, the zone named the &amp;quot;Beyond&amp;quot; can support futuristic technologies such as AI and FTL travel. The outermost zone, the &amp;quot;Transcend&amp;quot;, contains most of the galactic halo and is populated by incomprehensibly vast and powerful posthuman entities.

We are on outside edge of Slow zone, but how could we imagine the universe differently with each theory? General Relativity is an exotic theory. It allows for time travel, FTL travel, and Closed Time like curves (CTCs). A universe with the full power of General Relativity would be an incredible one with god like entities. In terms of computation, depending on how we look at it, CTCs allow anything from solving  PSPACE (already amazing) to All computational problems (well excepting decidability, it would not be a mystical hypercomputer, it can&amp;#39;t in principle compute more than a Turing Machine).  A universe of General Relativity would be like the &amp;quot;Transcend&amp;quot;. Not only is AI possible, they would be god like intellects able to easily solve NP problems that we believe would take exponential time now.

On the other hand Quantum Computing would take us to the low to middle beyond - except no FTL (no FTL communication either contrary to common belief). We learn a lot about molecular dynamics, biology, materials etc. We end up being able to simulate and accelerate a great deal of science. But huge swathes of algorithms, including in AI, are left virtually untouched.

This is another thing we need a theory of quantum gravity for. Would it give us computers more powerful than general relativity or somewhere between QC and GR (my bet)? What are the most powerful computers we could possibly build? What zone of thought do we inhabit?</description>
    </item>
    <item>
      <title>Machine Learning is not Magic Some useful Intuitions</title>
      <link>http://www.metarecursive.com/writings/Machine_Learning_is_not_Magic_Some_useful_Intuitions.htm</link>
      <guid>http://www.metarecursive.com/writings/Machine_Learning_is_not_Magic_Some_useful_Intuitions.htm</guid>
      <pubDate>Sun, 21 Dec 2014 00:00:00 GMT</pubDate>
      <description>Deen Abiola - 21 Dec 2014

One thing I find amusing is when people talk about Machine Learning as if it&amp;#39;s some kind of magic pixie dust you sprinkle over your program thus giving it special intelligence powers. When really, Machine Learned models are, as typically used, scripts in a simple language. The previous sentence needs some unraveling: what I mean by magic and what I mean by scripting.

People often talk as if you can throw machine learning at any problem and have it magically figure things out. This is very much like the zoom-enhance trope.

Actually, it is exactly like zoom-enhance since scaling up is itself a kind of inference. Just as you can&amp;#39;t fill in details that aren&amp;#39;t there, you can&amp;#39;t learn something that is either unapproachably complex (incompressible) or whose dynamics aren&amp;#39;t stationary. For example, you can&amp;#39;t throw machine learning at market data and think it&amp;#39;ll just work. Sure it&amp;#39;ll learn something but that something is almost certainly a quirky coincidence of that sample. Even if it tests well out of band, it would only mean the dynamics are as yet unchanged. Another example: you can&amp;#39;t throw data at an algorithm and have it figure out how a viral outbreak is going to progress. Similarly, complexity wise, you can&amp;#39;t throw a genome at an ML algorithm and have it try to predict physical attributes. In this case, the data just isn&amp;#39;t there, or more accurately, each current stage acts as data to feed the next state -- you&amp;#39;d have to literally compute the full organism to get it right. This is the kind of thing Wolfram calls Computational irreducability.

On the other hand, there are lots of useful problems that are stationary or close enough to (speech, image, translation) and lots that even if they aren&amp;#39;t stationary, we should in principle be able to build algorithms that can adapt in time (edge of current feasibility). Then there are complex seeming problems that might not be as impossible as they seem. Take protein folding, what trick has evolution figured out? Protein folding is NP-complete, even a quantum computer shouldn&amp;#39;t be able to help there. So what&amp;#39;s going on, how can biological systems make such short work of it? Would a suitably advanced algorithm -- something beyond deep learning; able reify its abstractions, perform deductions as well as induction -- be able to figure out the hidden pattern, the hidden shortcut? I think so. But once again it&amp;#39;s important to remember that AI isn&amp;#39;t magic, these are the same sort of computations that happen when you query a database (search) and save a jpeg or mp3 (compression).

The most important thing to keep in mind is that the amazing &amp;#39;neural network&amp;#39; or what have you is running on a computer. That is, it is bounded to be no more powerful than a Turing Machine and in particular, is almost always less powerful as a computing substrate than most programming languages. In principle, that Support Vector Machine or Random Forest could have been hand-coded. There is nothing special going on there and in fact, many learning algorithms operate in essentially a propositional calculus, having no quantifiers. The models, being fixed, function exactly as scripts would.

Turing Completeness is an attribute (for machines at least) where if you&amp;#39;ve attained it, then nothing can &amp;#39;think&amp;#39; things beyond you. Quoting Wiki: &amp;quot;In any Turing complete language, it is possible to write any computer program, so in a very rigorous sense nearly all programming languages are equally capable. Turing tarpits show that theoretical ability is not the same as usefulness in practice&amp;quot;.

One can think of various Machine Learning algorithms in an analogical manner. For example, a Neural Network with one hidden layer is universal as an approximation of continuous functions from one finite space to another. But a shallow network dwells in the depths of the computational learning equivalent of  Turing&amp;#39;s Tarpit. The big deal about Deep learning is more layers; which lead to large increases in expressivity, much like the difference between Brainfuck and BASIC. Further on that, recent papers have found that shallow networks can in fact represent, with good fidelity, the same functions as deeper ones. This tells us that, if there&amp;#39;s anything to be said about deep learning, it&amp;#39;s that it is a way to imbue structure to a problem in such a way as to simplify search. Nascent abstractions which improve search by biasing toward more promising paths.

Okay, so the important takeaways are :

And so with 3) we hit the utility of Machine Learning. They are a particular form of Auto-programming.  Learned models are functions which compute maps from one space to another in such a way that distances and structures are as close to preserved as possible. What separates a good learner from a bad learner is how complex the sort of regularities it can identify are and how liable it is to get stuck at local optima. Generalization is done by having these functions exploit structure in the problem so that future instances are correctly mapped.

You can imagine a map from pixel intensity values or waveforms to vectors representing words (just numbers!). Maps from sequences to sequences where the elements just happen to capture word senses and contexts: implicit but not deep meaning, though still enough to provide a great deal of utility. Since they do not require a full table be memorized, they can be viewed as computing a particular kind of compression. The compression represents understanding of the patterns in use without caring for a deeper why. This friction actually underlies what people mean when they say AI has no true understanding. It does but its concerns are very narrow.

The incredible philosophical consequences of learning as exactly a form of programming to follow.
‎


comic 1: http://tvtropes.org/pmwiki/pmwiki.php/Main/EnhanceButton source: http://www.phdcomics.com/comics.php?f=1156

comic 2: http://www.robcottingham.ca/cartoon/archive/freeze-zoom-in-now-enhance-and-fart-rainbows-and-turn-lead-into-gold</description>
    </item>
    <item>
      <title>Learning and Computation</title>
      <link>http://www.metarecursive.com/writings/Learning_and_Computation.htm</link>
      <guid>http://www.metarecursive.com/writings/Learning_and_Computation.htm</guid>
      <pubDate>Wed, 24 Dec 2014 00:00:00 GMT</pubDate>
      <description>Deen Abiola - 24 Dec 2014 


Not only are we constantly modeling the world mathematically, animal brains are also constantly proving things about it. The proofs are consistent even if not necessarily sound nor complete. I&amp;#39;ve not seen it spelled out anywhere before but it&amp;#39;s an incredible consequence of the fact that in essence, machine learning is a method to search for programs. For some people this might all read out as obvious and this is a good thing I think. For me, this is a good case study for my philosophy of not keeping things compartmentalized but instead trying to think in terms of bridges from many things to many other things.

In a previous post (Machine Learning is not Magic), I tried to lay the ground for this essay by building some intuitions on learning algorithms; emphasizing them as functions or maps and computations, talking about compression and generalization and identifying causality vs being satisfied with correlations (not in so many words, deserving of its own essay). This essay will serve to more closely link learning with search, logic and computation. It&amp;#39;s also worthwhile to spend some time thinking about why machine learning is so desirable: namely because most of the useful things we do are not conscious.

In fact, one of the most interesting things to come out of attempts to build AI is what&amp;#39;s known as Moravec&amp;#39;s Paradox. Moravec&amp;#39;s Paradox is where all the simple stuff we take for granted: vision, speech, walking etc. were expected to be easy while stuff like chess, math, puzzles were thought of as the harder things to implement. Instead it turned out to be the opposite, not just because concepts like common sense are less available and so harder, but because they take a great deal more computational resources while being of higher algorithmic complexity. It&amp;#39;s easy to forget how amazingly impressive what virtually every human baby is capable of in a relatively short amount of time; learning the incredibly complex sequences and patterns behind language as well as states of mind they communicate, learning vision, sounds, walking, building a physics model of the world, modeling other humans -- all with minimal guidance (and without being conscious too... mostly...gets in the way?†)!

The bulk of our intelligence is not in our rudimentary puzzle solving abilities; trying to build an AI has taught us that actually, most of the things we thought required minimal intelligence are actually some of the most complex things we do in an objective sense. Moravec also suggested that evolution never got round to optimizing (or there was little benefit for) the deeper logical reasoning computers find easy. This makes sense but I also think there&amp;#39;s another more important issue. Reasoning with exact numbers and the extremely large error free scratch spaces computers have requires a great deal of energy while evolution&amp;#39;s priority was one of minimizing energy usage above all else. While computers are excellent at forward reasoning/search, humans are (for now) better at pattern matching and recognition since the latter does not require maintaining a massive state space.

There&amp;#39;s another consequence of the paradox. It&amp;#39;s common to think  that in so far that automation is a problem for jobs, more education will fix it. But Moravec&amp;#39;s paradox suggests to us that the trade jobs and the jobs requiring the kind of higher order pattern matching that so sets us apart will be safest. It&amp;#39;s the management jobs, the rote jobs and entry level jobs across large swathes of industry that will fall. In law, in science, in computing, and yes in manufacture; as long as your job doesn&amp;#39;t require constant novel problems difficult to automate, you&amp;#39;re a target. This means that it&amp;#39;s the graduates, often young, who will be the most affected ones. Fixing the problem will require something more significant than pushing everyone through university.

Suppose I wanted to write a program that could recognize cats or faces or whatever; to write this program would effectively be impossible because a lot of human intelligence is not surfaced. Much of what we do is not made available to us -- it&amp;#39;s unconscious -- hence for example, we don&amp;#39;t know how it is we tell one person&amp;#39;s voice from another&amp;#39;s, even with only a very short and distorted listening time. This means that even if there were a simple program behind it, it would not be possible for us to write it.

So what machine learning really is, is auto-programming. Indeed much of how machine learning is currently used -- batch training -- is exactly like scripting in the sense that you run something external to your code that&amp;#39;s fixed at development time and hopefully increases flexibility and appearance of intelligence at runtime. In other words, the end products should be considered as little and limited programs and not something magical. But limited in what sense you might ask? Well, the biggest one is that the programs most models learn, even the infamous feed-forward deep learning ones, are not Turing Complete. This is a good thing mostly, it makes search and reasoning about them much easier.

But how do we find these programs automatically? The first thing is to recognize is that the space of programs is mind bogglingly huge; you need some way to quickly guide you to the specific program you&amp;#39;re looking for. And the way this is typically done is, paired with some algorithm that uses errors to decide on direction, you feed in lots of data. Data which help divide/categorize the space of examples while also constraining the space of programs to a particular locale. The model which gets output, together with the ML algorithm, specify a program that is able to hopefully accomplish the task we set it to. Barring exceptions like genetic programming -- and also decision trees/forests, I&amp;#39;d argue -- the model itself is not the program. You can&amp;#39;t run the model, they&amp;#39;re parameters for the algorithm. The algorithm is a function which when run on input data gives us the desired output most of the time. End result is, thanks to machine learning, programs get written tackling tasks which have insufficient conscious availability to have allowed us to have written them ourselves.

It&amp;#39;s evident then, but not often remarked upon, that the output of a learning algorithm is (or at least parameters for) a program. This means that learning is a particular kind of search, a search whose end point should be a program that is also a compression of the visited samples, aka generalization. Some people make a distinction between search and optimization but I view this as artificial, serving only to bury opportunities to make connections under trivial details. Optimization might typically only happen on differentiable manifolds -- guided by gradients -- but all that is, is a very specific kind of search, more principled but also more limited (relatively, still huge, basically all state of the art is done in that kind of space) and likely not how the brain implements its specifics.

For something like a Neural Network or Support Vector Machine it&amp;#39;s not immediately obvious how training is like searching for a program. Consider a Neural Network, you can think for our purposes, of it as tables of numbers which represent the connections between nodes of a network. Learning involves tweaking those numbers so that when an input vector, a column of data, is fed in, gates (functions like: f (x) = log (1 + exp x)) at each node turn on or not in such a way that input/output pairs similar to those seen in training time are generated. It&amp;#39;s a function, a functional program. But how is training like search? What are the parameters?

The output parameters are not the program, instead the parameters tune the model&amp;#39;s evaluation algorithm such that it becomes a function specialized to the training data. What I mean by specialization is hard to find a metaphor for and the best I can point to is from computer science itself. The notion of how a general learning algorithm is specialized to a particular function by parameters can be loosely compared to something like the relationship between a regular expression engine and a state machine. The input data specialize and tune the parameters to a program in a similar way that a Levenstein automaton or a depth first automaton might specialize regular expression matching. That&amp;#39;s a rough analogy and if it makes no sense then an example based on decision trees might prove clearer.

Decision trees are more obviously programs. They&amp;#39;re programs like if x = 3 Or y &amp;amp;lt; 2 then if x = 0 then Car else... etc; they serve to partition the problem space and the search is guided by trying to maximize information gain of each if else split in the data. A bunch of nested if then else statements are more obviously seen as a program and so the search for the best tree is a search, guided by concepts from information theory, for a program which best partitions and explains the observed data. Decision Trees I&amp;#39;d argue, especially because of their interpretability, are closer to how brains represent knowledge (on a conceptual level) than Neural Networks. (Genetic programming is without any ambiguity, a search for programs)

Training a model is exactly a search for a program. One of the most profound concepts in Computer Science is the Curry-Howard Isomorphism. The Curry-Howard Isomorphism links the type systems of computer programs to proofs in logic. The link is not a trivial one, it&amp;#39;s rich and deep and has lead to powerful theorem provers but here it suffices to draw the correspondence from types to theorems and programs to proofs (evaluation to implication expansion). What&amp;#39;s interesting here is that a machine learning algorithm is an algorithm which searches for programs and programs are proofs hence the act of learning is equivalent to searching for a proof in some non-trivial sense. But proofs of what? It&amp;#39;s not clear exactly, since nothing like types are specified. But that doesn&amp;#39;t matter since types can be inferred and often, the systems in use are simply typed and decidable. The types are not interesting, being usually very general [e.g. List of implies Member of Set{a,b,c}], it&amp;#39;s the proofs that are of interest. And since the types are so general, the proofs will not necessarily cover the phenomena even if the theorem is satisfied. But embodied in the programs will be partial proofs on the set of observations being visited.

And as plenty of often cruel experiments have taught us, even things we take for granted†††, like vision and hearing are actually mostly learned. So there&amp;#39;s this beautiful observation that in learning to see, every child is learning/searching for a program/building a proof about the behavior of light and objects out in the world. The same is true for hearing, language, walking etc. Not to mention higher order learning. And having learned things, animals end up carrying lots of little programs, evaluating the world and thus carrying out proofs on every observation (e.g. modus ponens) while also, (for the ones that can learn) constantly searching for proofs.

There is a way in which this gets really interesting when applied to Evolution -- not just the program aspect but especially the learning angle. I&amp;#39;ll assume for now that the Church Turing Thesis is valid -- not that the universe is a computer but that everything is computing itself. So a rock could be replaced with a Turing Machine that computes a rock as a byproduct or output or whatever without any loss.

Now consider that evolution is a learning algorithm. This is well known and no longer controversial, you can look at it like this: Imagine traits for some population; number of legs, color of fur, height, scale or fur etc. Each organism is placed in an environment. Over time the ones that die will shift the distribution of traits and how this happens is very much like an algorithm computing probability distributions over traits in a way similar to what happens in machine learning (you can link it to Bayesian learning or for sexual evolution, to game playing with weighted majority the manner in which the distribution is balanced/evolved). Mutation is not the main story, it&amp;#39;s random and serves mainly to introduce the raw material for evolution. The traits are far less specific of course, corresponding more to genetic units/alleles but the key idea remains.

So we have evolution as search/learning. But what is being learned? I don&amp;#39;t know but what is interesting to look at is the driving goal of evolution at all scales: replication. From simple RNA viruses all the way up to humans, replicating close to the current arrangement of atoms is a (the?) major driver.

Then, with evolution as learning and assuming everything can be swapped out without loss with an equivalent Turing machine (or less), then effectively, each organism is the result of a search for a proof related to what it takes to maintain and replicate states in this universe!

But is assuming a Turing machine too strong? Okay, suppose the Church Turing Thesis is wrong. Next, consider that a Turing machine can efficiently simulate, with arbitrary precision, all classical systems. Allow extensions (e.g. quantum Turing machine) and taking into account that everything a Turing machine can do, the universe can as well but the opposite is not true, then: Turing machines are a subset of the universe. So this argument should still hold with the only weakness being if the essence of evolution and cognition require not just incomputable things, but physically unharnessable, no incomputable things only harnessable by brains and the building blocks of living things. This is a very strong assumption and piles on unnecessary complexity, violating Occam&amp;#39;s razor.

The common argument, that we used to analogize via clocks etc., is incredibly flawed by taking into account that Clockwork is not universal in the same way a Universal Turing Machine is. Here is what G&amp;#246;del had to say:

&amp;quot;It may also be shown that a function which is computable [&amp;#39;reckonable&amp;#39;] in one of the systems Si, or even in a system of transfinite type, is already computable [reckonable] in S1. Thus the concept &amp;#39;computable&amp;#39; [&amp;#39;reckonable&amp;#39;] is in a certain definite sense &amp;#39;absolute&amp;#39;, while practically all other familiar metamathematical concepts (e.g. provable, definable, etc.) depend quite essentially on the system to which they are defined&amp;quot;

† Consciousness

I&amp;#39;m not sure why people put self-consciousness as some kind of pinnacle. I suppose as a seemingly unique human trait, it&amp;#39;s placed on a pedestal and worshipped as something that separates us from all other kinds of intelligences. Yet if you look closer, it&amp;#39;s not difficult to see that consciousness is very over-rated. 1) As I pointed out above, some of the most incredible feats of intelligence are performed by barely or not even yet conscious human babies. 2) A lot of wisdom, Zen quotes and quotes on mastery are all about shutting down your conscious mind. A novice dancer or martial artist is conscious of all their movements. They&amp;#39;re jerky and awkward, where as graceful and fluid movements are only doable once the knowledge has been transferred to the unconscious. This is not just true for automatic movements but for the highest levels of creativity too. 3) The state of flow, where conscious awareness is dimmed and the boundary between self and task is lessened results in the highest levels of performance. 4) People speak often of sleeping on a problem, not thinking about it consciously and having the solution (the brilliant Poincare wrote extensively of this as his method) come to them seeming spontaneously. It seems that all the effortless, graceful and masterful acts are done by the non-conscious part of the brain.

Consciousness is also limited. A large distributed entity would forgo it due to latency and an entity wanting to maintain parallel levels of awareness and threads of cognition will likely think the serial aspect of consciousness too limiting. An entity does not need to be conscious to allow the goals of others to affect which actions it selects nor to model itself against the background of some environment. Consciousness is a tool only; a bookkeeping, blame assigning, goal maintaining tool that&amp;#39;s some how morphed into this pointy headed boss that seeks to claim credit for everything that happens in the brain. In fact, I&amp;#39;m having trouble imagining a selfish, jealous, spiteful and petty non-conscious entity.

††  Unreasonable  Effectiveness of  Mathematics

Assuming the Church Turing Thesis allows all sorts of beautiful links between evolution, learning, logic, proofs and physics to fall out. With compression as generalization we also get links between entropy,  Kolmogorov complexity and learning. The physics comes from the link from programs to cartesian closed or monoidal categories. As time goes on, I&amp;#39;m starting to find the Effectiveness of  Mathematics not just very reasonable but also, almost...tautological.

†††  You can see the experiments on the poor kittens here: Development of the Brain depends on the Visual Environment, https://computervisionblog.wordpress.com/2013/06/01/cats-and-vision-is-vision-acquired-or-innate/.

The wikipedia article on the critical period hypothesis is also worth a look: http://en.wikipedia.org/wiki/Critical_period_hypothesis#Deaf_and_feral_children

But my favorite example is that the M&amp;#252;ller-Lyer illusion is sensitive to whether someone grew up in a city or a desert.

It has been shown that perception of the M&amp;#252;ller-Lyer illusion varies across cultures and age groups.
Segall, Campbell and Herskovitz[4] compared susceptibility to four different visual illusions in three population samples of Caucasians, twelve of Africans, and one from the Philippines. For the M&amp;#252;ller-Lyer illusion, the mean fractional misperception of the length of the line segments varied from 1.4% to 20.3%. The three European-derived samples were the three most susceptible samples, while the San foragers of the Kalahari desert were the least susceptible.

</description>
    </item>
    <item>
      <title>The Current Best Hypothesis is that the Brain is Computable</title>
      <link>http://www.metarecursive.com/writings/The_Current_Best_Hypothesis_is_that_the_Brain_is_Computable.htm</link>
      <guid>http://www.metarecursive.com/writings/The_Current_Best_Hypothesis_is_that_the_Brain_is_Computable.htm</guid>
      <pubDate>Fri, 14 Aug 2015 00:00:00 GMT</pubDate>
      <description>

There are many (most?) people who dispute the idea that the brain is computable—there is something different and special about the human brain, they say. It is not possible to dispute this for now, but my own stance is a basic one: You may be right that the brain is somehow magical but my position is simpler and, all things being equal, more likely to end up as the correct one.

The argument that the brain is not a machine broadly rests on three ideas: those who lean to science and say: something, something quantum or another (quantum gravity if you want to be really fancy), or those who think it something magical, such as possessing a soul. The final group simply argue that the brain is not computable.

It is not uncommon to see the argument put forward that the brain is not computable, that what computers do is mere mechanistic cranking of mathematical algorithms. This is true, but who&amp;#39;s to say the brain is also not doing this?

Occam&amp;#39;s razor, Bayesian smoothing and regularization are all tools to keep one from over-fitting the evidence and failing to generalize. They are not laws, but tools to help you minimize your regret—make the fewest learning mistakes—over time. They do not say your idea must be simple, only that it does not say more than is possible given the data. The idea that the brain is computable fits within this regime as the hypothesis that is the simplest fit to the data. Why?

I often hear the point made that since people once compared the brain to clockwork and steam engines—comparisons we now know to be false—what makes you think an equivalence (and not just analogy) with computers won&amp;#39;t show the same failing in time? Small aside: steam engines and the brain, thanks to the link between thermodynamics and information, is actually more interesting than what one might at first think.

Turing Machines are, unlike a clock, universal. They can emulate any machine or procedure that is &amp;quot;effectively calculable&amp;quot;. Our physical theories might use crutches such as real numbers or infinities but are, at the end of the day, only testable using computable procedures and numbers. This is what sets Turing Machines apart: any testable quantitative theory about the universe we can expect to devise will be simulatable (given enough time) on a Turing Machine (note: this is not the same thing as The Church Turing Thesis, instead of placing the restriction on the universe as CT does, it places it on any testable theory that compresses data. That is, more than a map from observation to expected outcome).

Even for the case that some physical things like the brain cannot be computed, it is simpler to believe that whatever non-computability the brain exploits is not unique to the exact biochemical make up of brains.

Interestingly, Occam&amp;#39;s Razor applies here too, and my argument is short. Even if Souls are a property of the universe unexplainable by science, it is still simpler to believe that the pattern and arrangement of matter that ends up with things acquiring souls is not unique to a gelatin soup of fats and proteins. Something that thinks and acts as if it is conscious, is (in essence, I drop the extra requirement that the object must also be an organic human brain like thing). That, in a nutshell, is also Turing&amp;#39;s argument.

But what is fascinating is that computer science has made the idea of a soul a scientific and testable hypothesis. If we do build intelligence (and maybe some of them will be more intelligent than humans in every way measureable) and yet they never wake up or attain consciousness or anything resembling (that is, nothing ever passes for consistently conscious but humans), then this is very suggestive of something unique and special about human beings. Until then, that hypothesis is unnecessarily complex.

Quantum mechanics is the go to argument for people who want to appear scientific even while talking nonsense. However, it is possible that the brain does something that our current machines cannot.

It is overwhelmingly unlikely that the brain is a Quantum Computer. What we know about quantum mechanics makes this highly unlikely considering how wet, noisy and hot the brain is. It is implausible that coherent and entangled states could remain in such a situation. Additionally, humans do poorly at things we expect Quantum Computers will be good at (things such as factoring, perceiving quantum interactions intuitively—simulating quantum evolution). In fact, regular Turing Machines already outpace us in many areas; we don&amp;#39;t focus as much on the fact that we&amp;#39;re terrible at deductive reasoning, arithmetic or enumerating the possibilities of a large search space; for those things, it did not take long for computers to surpass human ability.

But, suppose the brain was not quantum mechanical but still leveraged quantum mechanical artifacts for its functioning—artifacts unavailable to our machines—then it is possible that current efforts will not lead to AGI.

In a certain trivial sense everything is quantum mechanical in that an agent adhering to predictions based on the theory will be able to explain the world with the highest accuracy. Of course, with such a broad definition then even the computer you are currently reading this on is a Quantum one. Not at all a helpful distinction.

Yet there is also a non-trivial sense in which quantum effects can be leveraged. We see this with our current processors; part of the difficulty with getting higher speeds and lower power is that (amongst other reasons) quantum tunneling effects are getting in the way. Biological homing mechanisms and photosynthesis have also been implicated with taking advantage of quantum effects.

Evolution is extremely powerful at coming up with unexpected uses to subtle phenomenon. Consider the following, from a fascinating article:

A program is a sequence of logic instructions that the computer applies to the 1s and 0s as they pass through its circuitry.  So the evolution that is driven by genetic algorithms happens only in the virtual world of a programming language. What would happen, Thompson asked, if it were possible to strip away the digital constraints and apply evolution directly to the hardware?  Would evolution be able to exploit all the electronic properties of silicon components in the same way that it has exploited the biochemical structures of the organic world?

In order to ensure that his circuit came up with a unique result, Thompson deliberately left a clock out of the primordial soup of components from which the circuit evolved.  Of course, a clock could have evolved. The simplest would probably be a &amp;quot;ring oscillator&amp;quot;-—a circle of cells that change their output every time a signal passes through.

But Thompson reckoned that a ring oscillator was unlikely to evolve because only 100 cells were available.  So how did evolution do it—and without a clock? When he looked at the final circuit, Thompson found the input signal routed through a complex assortment of feedback loops.  He believes that these probably create modified and time-delayed versions of the signal that interfere with the original signal in a way that enables the circuit to discriminate between the two tones. &amp;quot;But really, I don&amp;#39;t have the faintest idea how it works,&amp;quot; he says.  One thing is certain: the FPGA is working in an analogue manner.

Up until the final version, the circuits were producing analogue waveforms, not the neat digital outputs of 0 volts and 5 volts.  Thompson says the feedback loops in the final circuit are unlikely to sustain the 0 and 1 logic levels of a digital circuit. &amp;quot;Evolution has been free to explore the full repertoire of behaviours available from the silicon resources,&amp;quot; says Thompson.

Although the configuration program specified tasks for all 100 cells, it transpired that only 32 were essential to the circuit&amp;#39;s operation.  Thompson could bypass the other cells without affecting it. A further five cells appeared to serve no logical purpose at all—there was no route of connections by which they could influence the output.  And yet if he disconnected them, the circuit stopped working. It appears that evolution made use of some physical property of these cells—possibly a capacitive effect or electromagnetic inductance—to influence a signal passing nearby.  Somehow, it seized on this subtle effect and incorporated it into the solution.

But how well would that design travel?  To test this, Thompson downloaded the fittest configuration program onto another 10 by 10 array on the FPGA. The resulting circuit was unreliable. Another challenge is to make the circuit work over a wide temperature range. On this score, the human digital scheme proves its worth.  Conventional microprocessors typically work between -20 0C and 80 0C. Thompson&amp;#39;s evolved circuit only works over a 10 0C range—the temperature range in the laboratory during the experiment.  This is probably because the temperature changes the capacitance, resistance or some other property of the circuit&amp;#39;s components.

Although this is the result of a genetic algorithm, a similarity with its natural counterpart is found: the exploitation of subtle effects and specificity to the environment it was evolved within. The article shows us two things: how evolution is not bounded by man&amp;#39;s windowed creativity but also, that, even if our current designs do not leverage some subtle effect while brains do, there&amp;#39;s no reason why we could not build a process that searches over hardware to leverage similar powerful processes. The search could be more guided; instead of random mutations, we have something else that is learning via reinforcement what actions to take for a given state of components and connections (we could have another suggesting components to inject freshness) then we select the best performing programs from the pool as the basis of the next round and appropriately reward the proposal generators.

Returning to the quantum, what, if there were something subtle about ion-channels or neuron vesicles, that allowed more powerful computation than one might expect. Perhaps something akin to a very noisy quantum annealing process is available to all animal brain&amp;#39;s optimization and problem solving processes? The advantage need not even be quantum it might even be that perhaps subtle electromagnetic effects or whatever are leveraged in a way that allows more efficient computation per unit time. This argument is one I&amp;#39;ve never seen made—yet, still, it consists of much extra speculation. Plausible though it is, I will only shift the weight of my hypotheses in that direction if we hit some insurmountable wall in our attempts to build thinking machines. For now, after seeing how very inherently mathematical the operations we perform with our language are (some may dispute that this is cherry picking but that is irrelevant because the point is the fact that this is possible at all is highly suggestive and strongly favors moving away from skepticism and), it is premature to hold such (and other) needlessly complex hypotheses on the uniqueness of the human brain.

I have not argued against the soul or that the brain is incomputable or somehow special, instead I&amp;#39;ve argued that such hypotheses are unnecessary given what we know today. And even indirectly, when we look at history, we see one where assumptions of specialness have tended not to hold. The Earth is not the center of the universe, the speed of light is finite, simultaneity is undefined, what can be formally proven in any given theory is limited, a universal optimal learner is impossible, most things are computationally intractable, entropy is impossible to escape, most things are incomputable, most things are unlearnable (and not interesting), there is only a finite amount of information that can be stored within a particular volume (which is dependent on surface area and not volume), the universe is expanding, baryonic matter makes up only a fraction of the universe, earth like planets are common, some animals are capable of some mental feats that humans are not, the universe is fundamentally limited to being knowable by probabilistic means (this is not the same thing as the universe is non-deterministic)!

While one cannot directly draw any conclusions on the brain from these, when constructing our prior (beliefs) it perhaps behooves us to take these as evidence suggesting a weighting away from hypotheses reliant on exception and special clauses.</description>
    </item>
    <item>
      <title>Levels of Understanding</title>
      <link>http://www.metarecursive.com/writings/Levels_of_Understanding.htm</link>
      <guid>http://www.metarecursive.com/writings/Levels_of_Understanding.htm</guid>
      <pubDate>Fri, 05 May 2017 01:36:40 GMT</pubDate>
      <description>
Freepik CC 3.0 BY

I believe one can usefully represent and capture levels of understanding with 3 levels. The 3 levels being: memorization, topic extraction and compression. The first level is the domain of computers—searching, indexing, clipping etc. Humanwise, I believe savants dominate memorization. And temporarily, though far less reliably, also the level most students achieve on tested subjects. Recently, this is also how most people operate online. &amp;#39;Cyborgs&amp;#39; stitching information snippets together without any deep comprehension. There are a lot of them on the internet.

Level 2 is topic extraction. This is the form of a lot of human knowledge. Where, you don&amp;#39;t remember quite what you read this but: &amp;#39;you know, it had to do with this or maybe it was that and...are you sure you didn&amp;#39;t read it too? Oh well, anyways it was interesting.&amp;#39;

Thanks to search tools like Google, you can often use these clues to quickly reconstruct the original information—with varying success rates (but much greater than before). In this way, we combine computers—which are good at memorization—with humans, who can extract topics, to augment recall. The combined system allows better and deeper recall than either system alone. At this point I&amp;#39;d like to emphasize that this storing of information outside the brain is nothing new. It is actually a core aspect of how the brain works.

For example, it is very common to forget something but then trivially remember it when at the same location you thought it up. Or leaving something by the door lest you forget—these triggers and associative recall are how the brain efficiently manages memory and shuffles priorities into and away from conscious action by outsourcing them into the environment. Search is but a continuation of a long tradition. It is not by any means making us &amp;#39;dumber&amp;#39; (what&amp;#39;s making us dumber is the increase in noise, from scientific research to blog posts, the majority are confusion). It is in fact helping the brain do what it has always done better than ever before.

There is still a lot of friction however; in that search, clipping etc. are still expensive and active processes. In addition, extracting relevant information from results is still a non-trivial expense. There&amp;#39;s a cost such that search is not always undertaken—how else can you explain people saying &amp;#39;IIRC, this is blah blah&amp;#39; when that information is readily available online? [footnote: to those of you who remember libraries, this is not about being spoilt but to emphasize that the shorter the time between thought and feedback, the greater the boon to cognition and that relationship is highly non-linear].

The most sophisticated level of understanding is compression. I conjecture understanding to be a direct search for first what combination of existing bases best represent some concept (in other words, metaphor and analogy are key to reasoning, another corollary is that we never start from scratch there are some in built structures) which are then used to represent this new knowledge. Mastery is the ability to arbitrarily form linear combinations of the basis concepts in this new space. Importantly and in addition, the dictionary/basis set is not necessarily fixed—allowing additional and often superfluous (not linearly independent) vectors as necessary.  Hypergraphs or relations and projections,  I conjecture, best represents the link between different spaces. The spaces are likely fluid, contracting and expanding as needed, mapping between themselves and building structures (combination of new concepts) based on functions that worked in other spaces.

A corollary is, while learning new things expands the mind, understanding is a contraction of bases. It reduces dimensionality. Understanding can identify that two previously thought independent concepts are actually codependent and thus can be expressed in terms of some other more fundamental concepts/basis concepts. On the other hand, learning can require additional bases (optimization, search) to represent truly foreign concepts. This suggests that the bases are not truly orthogonal and understanding is a search for this.

Other than being a useful metaphor, there is real work in this direction. When you create a jpeg, it is in a sense, a very local &amp;#39;understanding&amp;#39; of the image—throwing away useless aspects to allow reconstruction. More complex neural net based image generators or compressors learn something of not just colors but also correlations that map to what we might call textures or sections of objects. The algorithms could be turned to learning what aspect of color is relevant for human vision, given an appropriate loss (note: correlations not abstractions, let&amp;#39;s leave abstractions as correlations converted to symbols used for unrelated reasoning by an intelligence).

Vision in the brain also works similarly, except the bases are overcomplete (using far larger dimensionalities than that of the signal), hence the vectors are sparse. There the learned bases are akin to Dictionaries and visual signals are sparsely coded with respect to this dictionary. While the mind is certainly more than a vector space, the fact that vision is so well modeled, and with how evolution tends to conserve and reuse, causes me to think of this model of understanding as key.

A common objection to AI is that it has not displayed true understanding, it&amp;#39;s just performing such and such mathematical function. In my view, the correct to response to this is: &amp;quot;why would it be any other way, for anything else&amp;quot;? The idea that happiness and anger might in fact have a mathematical description seems unacceptable to many. On the other hand, it does mean that emotions are algorithms that satisficed some optimality criterion and are not merely dismissible as irrationality or brokenness.

There is a sense however, in which the expressed disagreements about whether AI is yet creative can be seen as not wrong. Imagine an object trying to regulate itself. A dynamical system trying to maintain homeostasis against some non-static background must have (even a simple) model of that background.

In order for a prediction machine to operate effectively it needs to capture statistical properties of the world around it. Vision correlates with the outside world but should not be mistaken for anything other than an uncertain representation thereof. It is in this sense the statistical can be said to correlate with the true signal. This property can be very general, as can be found in pigeons able to learn to discriminate between words and non-words: (see: Orthographic processing in pigeons (Columba livia).

Consider a robot arm connected to a computer. The robot was trained on drawings and can now generate drawings it has never before seen. The robot arm was not programmed, it generates things that it was never trained on and is not random. The robot is like nothing we have ever seen before and while many are happy to call it creative, this does not sit well with most. I too was once happy to call such programs creative but recently stopped, upon arriving at a distinction. Ada Lovelace once eloquently stated:

The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths.

Our robot arm (buildable today), can do things we did not order it to perform and it can be said to originate. But it still has no power of anticipating analytical relations or truths because the arm cannot exit the manifold its learning algorithms had sought. Any movement too far leads to collapse and the generation of noise, because as the Lady Ada presaged, analytical relations cannot yet be anticipated.

A picture can be taken as a snapshot or projection of reality. The robot arm could similarly be considered a projection, echo or ghost of the drawing ability of the countless poorly paid souls which provided its training data. That robot is animate but not in the manner that a human is. Yet, as a projection of the thoughts of a thinking thing, it is something more than a mere picture or a recording.

When learning, humans seem somehow to be able to develop higher order conceptions of the nature of a search space. We learn not just an embedding or lower dimensional submanifold, as algorithms of today but also can detect non-trivial symmetries, invariances and even algebras. We likely operate on statistical manifolds, moving from model to model and somehow able to constrain movement in the space of models such that a surprising percentage of moves are fruitful. This might be why we are good at poker, chess and Go using far less resources at learning and at play.  Flexibility in adapting to underlying structure displays a level of understanding far beyond what any algorithm to date has achieved.

For example, we can notice that the weather and brains are both describable in terms of dynamical systems. Learning about physics of weather and unshared properties with the brain can serve to constrain hypotheses about the brain. Or consider constructing a general object and deducing many things about it. After which, you find some new thing, prove it has some property that makes it like the object you deduced many things about and instantly get knowledge about your new thing. Whether deduction or some sophisticated way of conditioning on a space, these kinds of long range connections are things we just barely have inklings of how to achieve.

Some promising directions today are gaussian processes and generative models that well consider complexity across a model class.

Another difference can be found when things mean something to us. Something that might feed into that is our ability to work not just off correlations and conditional distributions but also, in that we bind signals to percepts and then reason with them. We evolved in a world with actors where many things have causes and there is a need to model internal states of other actors. The object binding, together with a ratcheting of complexity in modelling and recursivity likely feeds into our notion of meaningful. Beyond compression, we are able to better appreciate the general structure of a search space as well as bind signals to more abstract/aggregated/usefully coarse grained and recursively modeled symbols.

It&amp;#39;s worth noting that humans are not perfect and suffer large limitations. Humans function best in rich spaces with lots of structure such as when signals have a hierarchical structure that is well decomposable. When the space is rich enough, this yields great results but human reasoning can really fall apart in more general domains. Nonetheless, it is in the above mentioned two very important senses ( 1. we can deduce things much beyond what we experience 2. when we create we can anticipate other agents in a rich way in terms of symbols and use that to guide our choices) that robots do not understand nor create.

The architecture of computers means they are better suited to certain kinds of computations and humans to another. We assume that because we are intelligent and conscious, that is the ideal form. But such is not necessarily true. Spiders and ants are very successful, without also being in danger of self-termination. A thing that was hyeprspecialized at say nuclear physics or genetics without being causally oriented or conscious would probably do more good than a conscious thing whose eventual greed we might have cause to be wary of.

The shared ancestor of humans and chimps had something very unique about its genetics. Chimps diverged and lost it but some developed it further down the hominid line; competed, merged and resulted in a type of consciousness known as human. There is no indication that throughout the long history of life on the planet, anything else like it ever developed. It might be that our type of self-modelling consciousness is a rare and difficult to arrive at modality.

Some might imagine there is some kind of hierarchy, that other AIs will be human like—capable of goals and self direction but it might just as well be that the space of intelligences is mostly one of highly specialized decidable compression algorithms that are really good at solving one particular class of problem.</description>
    </item>
    <item>
      <title>On the Breakdown of the Bicameral Mind and the Importance of Diversity </title>
      <link>http://www.metarecursive.com/writings/On_the_Breakdown_of_the_Bicameral_Mind_and_the_Importance_of_Diversity_.htm</link>
      <guid>http://www.metarecursive.com/writings/On_the_Breakdown_of_the_Bicameral_Mind_and_the_Importance_of_Diversity_.htm</guid>
      <pubDate>Sat, 12 Aug 2017 00:00:00 GMT</pubDate>
      <description>Have you heard of the bicameral mind? If you haven&amp;#39;t, it&amp;#39;s worth checking out. While I don&amp;#39;t think it correct, I think it takes only a slight modification to get us something highly probable. In this view, like in the Bicameral setting, we don&amp;#39;t all experience consciousness the same way. The core mediator is the differences in sophistication of theory of mind. The brain is plastic and different percepts affect both attentional priors and how language is used for self-modelling. The more complex stories we told, the better the listeners had to be at modeling those scenarios and the better the brain had to be at representing states at a meta level.

The break-down occurred through diffusion and cultural interchange leading to richer stories which in turn called for more complex modelling. The better you are at modelling others&amp;#39; mental states, the better you will be at modelling your own.

Note: The next paragraph is conditional on the occurrence of a breakdown like event but the core message is independent of it. The compositionality of culture and knowledge means the gains to interaction are multiplicative which in turn affect the creativity of the citizens by the widening of available mental tools and percepts. The similarity between my exposition and the bicameral mind is that like in the bicameral mind, there was a phase change resulting in an enriching of the manner in which consciousness was instantiated in humans. The difference is that this was not due to evolution or an inferiority of previous brains but rather, a side effect of learning to learn and sharing between sufficiently diverse cultures combined with/multiplied by language and yielding a more powerful theory of mind ability.

The breakdown of the bicameral mind was not one of evolution but due to the brain&amp;#39;s plasticity. Increasing intellectual demands from the compositionality of culture, combined with a general learning engine of the brain, resulted in more flexible approaches to thought. We know language is linked to consciousness and higher order reasoning (see [1] on how metaphor shapes reasoning or [4] for how language augments numeracy as examples) and also that it&amp;#39;s one area where the brain must learn how to learn. While the individuals were not less conscious nor less intelligent, the lack of richness of available percepts severely circumscribed their ability for ideation. I doubt there&amp;#39;s anything falsifiable that could be said of an ancient left/right brain breakdown, but it&amp;#39;s certainly true that  available default abstractions would strongly impact a person&amp;#39;s manner of cognizing about the world, theorizing of events, arranging causation and most importantly, reasoning about themselves. This could plausibly affect how consciousness was realized in the individual (emphasizing that there is no objective hierarchy or ordering). Although clearly a good thing, this sort of multi-cultural richness is not without its own set of hurdles.

Well, differences in ability to predict mental states yields difficulty in collaboration. If two group&amp;#39;s mental organization is so different as to make communication very exhaustive then coordination and attending to the same signals, required for positive welfare outcomes will be difficult to achieve.

The answer however, is not less diversity, it&amp;#39;s more cooperation. Genetic, ecosystem and hypothesis diversity (Principle of Epicurus) are unequivocally good. Neural nets and even humans [2] suffer from mode collapse, a form of lack of diversity leading to reasoning errors, which can lead to suboptimal exploration and poor decisions. If the human mind is computable then a group of humans is not other than a highly bottlenecked parallel computer.

Again, considering [2], humans which share too much in common will be akin to starting a sampler near similar locations. If we seek to converge on the part of the landscape with peak probability, it pays to sample in parallel (via multiple people) from many start locations. If the brain is a sampling based explorer, with active inference to mitigate autocorrelation, then it&amp;#39;s clear that lived experiences leads to richer exploration than any other source of diversity (and as long as it&amp;#39;s computable, there&amp;#39;s very good reason [5] to believe that it is a sampler in the large and possibly variational locally). Ideological diversity must be retained however, since it does provide different attentional weightings—it&amp;#39;s just that the gains are not as large as from a diversity of parallel active samplers.

The only sense in which diversity can be considered harmful is when they make coordination more difficult (while I do not agree with their approach, I think China understands this on some level, that and economic protectionism explains many of their policies). Ideological diversity is probably especially prone to this. Ideological diversity, I should point out, is not just what politics or economic policies you adhere to but also, what kind of music and hobbies you like! Focusing on other forms of diversity can only increase ideological diversity.

However, short range correlations (siloing into isolated cultures) does not lead to interesting structures. Recall that one plausible way for the phase change in theory of mind to have occurred is from the richness in stories from cultural interaction and sharing. Resulting then, in a gain in sophistication on conscious reasoning about agents. Wider experiences leads to richer representations—though each human possesses only a degraded average over encounters—it is yet a comparatively richer set of examplar states to draw samples from. In essence, a certain group gets the &amp;quot;problem&amp;quot; of diversity exactly backwards. Diversity issues stem not from physical differences but from experiential and ideological heterogeneity, leading to different priorities and attentional weightings. Furthermore, experience is a stronger mediator of ideology than whatever labels (e.g. centrist) is currently fashionable to apply to one&amp;#39;s self. All the kinds of diversity which concerns this certain crowd are in fact good but the kind which they view as important (as do I), provides the most trouble.

So what then? I think the answer is to raise human children emphasizing shared commonalities, more sharing of culture (yes, I&amp;#39;m pro cultural appropriation) and how to empathize (that is, not dehumanizing the other) with other beings. Additionally, a lot of problems are caused by fortifying identity by dredging for differences. If we can create a society such that there are sufficient avenues and means for collaboration and sharing—whether on art, stories or music, any kind of creation—the need to find meaning by belonging to something, should lessen by a large amount.

Meaning from sharp boundaries group identity reduces a human&amp;#39;s ability to reason, replacing it with pattern matched stock responses and confabulations. This is an example of when correlations can go wrong, trapping members in equilibria even if the members could have long since drifted past the original goals and utilities.

Prediction: If children all over the world grow up watching each other&amp;#39;s culture&amp;#39;s cartoons, this will have a significant ameliorative effect on any ill coordinative effects of cultural diversity. Of course, some might question the extent to which this is a prediction given the broad arch of history.

It is unlikely that humanity cannot learn to cooperate broadly and arbitrarily; this is rather, a problem of learning how to escape pernicious attractor states and achieving positive social utility through mechanism design.

In this essay, I point out that a computable brain performing sampling based inference as suggested in [2], and a group of humans yields a parallel sampler. Connecting this with the observation in [3] that autocorrelation is minimized by active inference tells us that to get better exploration and representative posterior probability distributions with respect to reality, we want a wider range of physical experiences. As a result, diversity is important. I point out however, that since ideological diversity is cognitive and attentional, it is broader than politics—increasing general diversity must increase ideology. Secondly, it can be negative, due to a reduced shared ability to attend to the same signals and efficiently communicate (coming off such different attentional weights).

I argue however, that diversity is worth it by pointing out how a scenario like the break-down of the bicameral mind might have been precipitated by trade and cultural diffusion leading to richer stories and hence better and more practicing of theory of mind (I suggested replacing origin of consciousness with enriching of consciousness, based on the compositionality of culture together with plasticity from our ability to learn how to learn). I also suggest cultural appropriation and lesser emphasis on identity/belonging and more on collaboration to counter ideological clashes of diversity.

I bring up the issue of ideologies because careful manipulation of attended to signals can lead to feedback loops which sort matched humans into clusters of sharper and wider boundaries, divorced from reality—a general human failing which gets in the way of coordination—and therefore handicaping our ability to achieve positive welfare outcomes globally. The issue is tribal (geographically correlated attentional priors and utilities) in nature and happens in every country with multiple ethnicities. Which is almost every country. I&amp;#39;ll note that the EU was a response to one such.

[1] http://www.sciencedirect.com/science/article/pii/S1364661317301535

[2] http://www.sciencedirect.com/science/article/pii/S1364661316301565

[3] http://www.biorxiv.org/content/biorxiv/early/2017/02/17/109355.full.pdf

[4] http://langcog.stanford.edu/papers/FEFG-cognition.pdf

[5] http://www.sciencedirect.com/science/article/pii/S0278262615300038</description>
    </item>
    <item>
      <title>A Sketching Look at AI Ethics</title>
      <link>http://www.metarecursive.com/writings/A_Sketching_Look_at_AI_Ethics.htm</link>
      <guid>http://www.metarecursive.com/writings/A_Sketching_Look_at_AI_Ethics.htm</guid>
      <pubDate>Sat, 12 Aug 2017 00:00:00 GMT</pubDate>
      <description>It&amp;#39;s common to mock economics as useless and completely detached from reality. Yet, the fact that algorithms as simple as regret matching or multiplicative weights update, converge on various important equilibrium concepts—algorithms we believe frequently occur in some form in nature—tells us that the core ideas of game theory are in fact very useful. The mistake that has been made is confusing the simplifying assumptions selected so human economist could hand wave a defense for flawed policy advice—unrealistic utility functions, simple two player games, zero sum, intractable equilibrium concepts—for proof of a lack of utility of the subject. In actuality, the game theory is extremely natural, merely poorly applied. We should move from boring discussions on 0-sum games, nash equilibria, prisoner&amp;#39;s dilemma, tit for tat (which does not scale), to discussions on coordination, cooperation, welfare and fairness. And as it turns out, there has been a lot of recent work done in that area—the only problem is the focus on ad markets and auctions—there is no reason it can&amp;#39;t be adapted to improve human lives. Some aspects of modern economics are indistinguishable from online machine learning or computational game theory. It&amp;#39;s a large subject deserving of more attention. If we choose this path, the question then arises: is it ethical to turn over important decision making to machines?

The debate on AI ethics is currently dominated by 3 archetypes. I&amp;#39;ll summarize them before arguing for another area that&amp;#39;s at least as and possibly more important.

In this line of work, researchers point out how blind following of algorithms can lead to bad outcomes by amplifying bias and strengthening inequality. Although a few look at how to ensure that AI doesn&amp;#39;t end up as yet another tool to suppress humans (by working on energy efficient or decentralized learning algorithms), most of the work is in how algorithms amplify bias.

Text is highly structured, in particular, it crystalizes projections on thought and embeds the way humans use language and thus simple algorithms can display high competence with little understanding. As an example, consider the output of a generative text model (I recently came up with—not a markov chain, not an rnn, similar to both—post coming soon), such that when prompting it with the phrase &amp;quot;the prefrontal cortex is&amp;quot; yields the following (not cherry picked):

the prefrontal cortex is the attention to prediction of the world

the prefrontal cortex is the control in the mind

the prefrontal cortex is the control of the general factor of intelligence

the prefrontal cortex is the control attention

the prefrontal cortex is the control in the human brain

None of these appear anywhere in my corpus of training documents but it&amp;#39;s eerie in how good of a summary it is of the relevant literature. Training it on a voat corpus and then prompting it on blacks or Jews compared to a wikipedia corpus would yield very different results. This is not reflective of reality, it&amp;#39;s reflective of how humans of a culture use language and what they write about. What humans think with their = 10 Watts of conscious cognition (assuming uniform process/energy use, 20 Watts, most processing not conscious) is very limited and can be divorced from reality.

My main criticism of the bias work is with the idea that there can be algorithmic correction. There&amp;#39;s a great deal of danger from a false sense of complacency, thinking a job well done with biases &amp;quot;corrected&amp;quot;. Worse, who is to say those corrections are even appropriate when applied inflexibly at scale? Indeed, beyond an inability to effectively enumerate the space of biases,the larger, more pressing problem is in the combinatorial possibilities of seemingly innocuous individual actions. No system, that trains till average losses are minimized, therefore assuming some stationary distribution, will be able to match the need for nuance and context in reality.

So you&amp;#39;ve handled race and gender issues. Have you corrected the sentiment bias of short bus? What about the countless other combinations you&amp;#39;re unaware of? Or, consider correcting for bias in health outcomes when an insurer is focused on above normal profits (and going beyond the bare minimum require to stay running). That would be putting the cart before the horse. Rather than play a continuous game of whack a mole, it would be better to create a culture of not blindly deferring to AI judgement in any area that could materially affect a human life. The policy aspect will make a lot more difference than most anything that can be done with algorithms (until we get AI&amp;#39;s that aren&amp;#39;t blindly performant).

The general idea here is to slow down work on AI to ensure no one develops AIs which view humans as inconsequential. MIRI leads algorithmic work on the topic and places like the Future of Life Institute lead the policy aspect. However, I do not believe anyone knows how to mitigate these claimed risks or even has any inkling as to how they might come about beyond very vague arguments. Despite this, and while I do not agree with the claimed magnitude of risk nor with the framing, I do not think anyone has prepared a proper counter-argument for why the AI Risk Crowd is mistaken (I think it&amp;#39;s possible to prepare such an argument, I&amp;#39;ve just not seen it yet). That said, while I agree that we should build kind, caring AIs which are respectful of living things and their ecosystems (weighted by their flexibility in traversing &amp;#39;information manifolds&amp;#39;, I don&amp;#39;t particularly care for the value alignment nothing else is as important framing.

This one is mostly corporations generating slogans to keep humans at ease. Boards are created, names are listed, chants are chanted but nothing meaningful will likely ever come of it. You see, it is difficult to trust an entity that cannot imagine a future in which you are not as thoroughly dependent as possible on it.

The regret/bandit/expert algorithms which featured heavily in this post are algorithms which already run the (digital) world. Although deep learning gets a lot of public attention; it&amp;#39;s some kind of bandit algorithm what places ads, manages auctions or perturbs site designs in order to squeeze money and attention from their human playmates. Although not something I personally agree with, it&amp;#39;s a non-issue in comparison to scenarios where such algorithms are turned to subverting and redirecting attention for policy issues instead of merely purchase decisions. What tools can be provided for defense? I don&amp;#39;t see anyone moving to democratize that.

And yet, as important as the above are, I think what subsumes them all is thinking about the ethics of algorithmic approaches to ensuring welfare of cities, humans, animals and ecosystems. Is it ethical to not use these approaches or is it true that there is some humanity lost by algorithmically deciding on important policy issues? It&amp;#39;s clear that algorithms can&amp;#39;t yet (maybe never) automatically operate on important decisions which will affect human agency. There&amp;#39;s however, a case to be made for a combination approach.

For example, we saw in this post that Nash Equilibria are hard to reach and might not always be great. We also saw that correlated equilibria can lead to quite fair results but while they are easy to compute, they are not obvious to set up. And even when we can, how will people take to suggestions handed down by faceless algorithms? How do we ensure such a process doesn&amp;#39;t end up as something to worship or enslave?

Getting multiple agents with differing goals to cooperate is extremely difficult. There&amp;#39;s plenty done on machine and single agent reinforcement learning but not nearly as much done on trying to learn cooperative agents. If you are worried about AI Risk then I think you should also be concerned by this imbalance. With better co-operative agent modelling we might begin to look at methods of approximating policy impact better than everything before. At some point we could design systems where those affected by policy could contribute detailed descriptions of preferences and actions. The system would then use co-operative learning agent modeling to approximately calculate some fair correlated equilibrium or aggregated welfare concept. While superficially similar, this collective decision making differs from a planned economy in that it is ad hoc and only scopes over the collective making and affected by the decisions.

When the issues are too complex for such an approach, a more bottom up approach would be to model complex agents in a scenario approximating the decision conflict. People could search over and test different reward functions in order to guide policy design whose effect will be to get people to behave optimally with respect to some shared goal (and which selfish behaviors could easily thwart). Again, all members affected can give detailed concerns and input and discuss policy in terms of how the model is effected. Remember, though the model will have far from perfect coverage over outcomes, it would still be miles better than the current approach of sparsely sampled scalar votes influenced by ever more sophisticated technologically amplified propaganda. The results will be approximate and imperfect but by making the simulations interactive with the participants, a powerful problem solving capability could be gained. While much of this is probably not near (needing either good language AIs or more people good at programming—also note this isn&amp;#39;t calling for laws as programs only for models to help predict outcomes), there are aspects which could be studied today.

One way to set up correlated equilibria is by studying various specifications of problems and noting how different reward functions lead to different results. Auditable, Traceable cryptographic smart contracts are one way to achieve this. Imagine a scenario of contracts between developed and developing cities targeting climate emissions. On the one hand, it&amp;#39;s important to reduce CO2 emissions, on the other hand the developing nation might say: &amp;quot;we are tired of being mired in poverty&amp;quot;. One way around this would be to search for a policy that would place both actors in a correlated equilibrium or better. Smart contracts allow for this—they are one way to route conditional exchanges such that no corrupt local actors could steal. Additionally: agreements preregistering purchase decisions, a time gated method tracking proper allocation of any citizen grants, measurable quotas, trades, all separately conditional on future dates and automatically verifiable obligations, just might allow for a fairer outcome for all. The contract mechanism then acts as an impartial mediator to ensure a positive correlated outcome.

By setting up mechanism design tested policies we can get favorable results with higher probability. But one distant ethical issue is in ensuring that agents don&amp;#39;t get so sophisticated, the issue of their being deserving of rights becomes ascendant. If such a scenario is possible, then there must be a computable theory of consciousness and ethics. We should ensure we figure those out before building anything too sophisticated. Sounds far fetched? Well, I&amp;#39;m not saying it&amp;#39;s now, I&amp;#39;m only saying that it would be really terrible if it happened in some distant future and caught us all off guard such that we were then motivated to continue to not notice how horrible a thing we were doing.

The other glaring issue is: &amp;quot;would we be losing our humanity by handing off important decision making to algorithms&amp;quot;? For me the answer is clear: the human mind is also just algorithms, if we can design a system to help us arrive at fairer decisions than we otherwise would have, say because due to complexity limiting our brains&amp;#39; ability to as fully traverse the space of consequences, then there&amp;#39;s nothing lost in adhering. The inputs, goals and motivations would be human decided. An objection to this, I believe, would be like not living in a house because it was built using Caterpillar heavy machinery or because it was 3D printed or prefabricated by robots. Humans were the designers and architects; the structure is to provide positive utility for the humans contained.

Furthermore, I consider the agency and individuality of humans as mythologized. Most of a person&amp;#39;s personality is settled by chance events, genetics and accumulated experience from childhood.  In adulthood, it&amp;#39;s a system of cultural expectations and traditions which strongly bind what directions any individual might take. We would not be trading away some glorious past of human agency. Not a past filled with trails of tears, slavery, indentured servitude or conscription into pointless wars. In the past, any kind of literacy was scarce, with only a tiny portion of the population able to indulge in scientific and philosophical thought. Not forgetting that under limited diffusion of ideas, people became echoes of their neighbors and even worshipped their rulers as gods. With some cultures even valuing human sacrifice.

Far from outsourcing ourselves into technology, the extension of our minds into the environment is a defining trait of humanity, ever since the invention of language. A larger portion than ever is improved in their ability to reason and handle abstraction [4]. Note that all humans are capable of  achieving this gain but consider, as an example, how understanding permutations gives a reasoning advantage. We&amp;#39;ve long outsourced thought into abacuses, tools, books, quipu, tabulating machines and lately, computers. In turn, we&amp;#39;ve gained access to more readily available knowledge and tools and the ability to administer ever more complex societies.

In this essay, I note that done correctly, auditable cryptographic smart contracts could act as neutral mediators to achieve non-trivial correlated equilibria. I give the example of cities cooperating to reduce emissions, that works no matter how corrupt the local government.

I then talk about Ethics in AI, how highlighting bias is important but algorithmically dealing with it is misguided. I mention AI Risk and my skepticism of its urgency but dissatisfaction with all counterarguments. I also mention the case of modeling agents which become too sophisticated.

If such a scenario is possible, then there must be a computable theory of consciousness and ethics. We should ensure we figure those out before building anything too sophisticated. Sounds far fetched? Well, I&amp;#39;m not saying it&amp;#39;s now, I&amp;#39;m only saying that it would be really terrible if it happened in some distant future and caught us all off guard such that we were then motivated to continue to not notice how horrible a thing we were doing.

I mentioned the difficulty of getting multiple cooperating agents and the benefits of doing more work in this area in order to guide real world policy. And moving beyond the toy, completely unrealistic scenarios favored by traditional economics. I also note that a lot of interesting work has been done in the areas of fairness and welfare, but mostly on how to better place and monetize ads. It seems to be working out well.

I argue that allowing algorithms to play a role in human decision making is no less giving up our humanity than allowing a robot to build a prefabricated home. In both cases, human labor is better applied to design and goal setting and not on energy intensive labor which humans are ill-suited to (such as lifting heavy objects or searching combinatorial spaces). I also argue that the past was not really some bastion of agency, free thinking or freedom. There&amp;#39;s more of that now than ever and augmenting our capabilities with computational devices will likely lead to better welfare outcomes.

[1] https://arxiv.org/pdf/1711.00363v1.pdf

[2] https://arxiv.org/abs/1504.06314

[3] https://arxiv.org/pdf/1609.05807.pdf</description>
    </item>
    <item>
      <title>Learning, Ethics and Game theory </title>
      <link>http://www.metarecursive.com/writings/Learning,_Ethics_and_Game_theory_.htm</link>
      <guid>http://www.metarecursive.com/writings/Learning,_Ethics_and_Game_theory_.htm</guid>
      <pubDate>Sat, 12 Aug 2017 00:00:00 GMT</pubDate>
      <description>

In the image above, I&amp;#39;ve sketched how one can show that evolution, game theory and learning are related. While I&amp;#39;ll not talk about natural selection today, I&amp;#39;ll discuss how ideas from game theory can be given an easier footing from within the context of computational learning theory. A future post will discuss evolution from the min-regret framework so it&amp;#39;s worth following the ideas presented here. Topics: AI economics (why capitalism is intractable), games, learning algorithms. I must apologize—ideally, there should have been interactive demos but alas, I hadn&amp;#39;t the time for that.

The Nash Equilibrium (NE) is a solution concept for games. Games are a framework which prescribe how to behave and coordinate optimally in a given situation, assuming some way of ranking outcomes (utility) and effectively infinite computation (among other unreasonable things). Okay to be fair, people have recently been looking into how to operate under the constraints of resource limitations. The min-regret framework has informed some of that work.

A NE can either be pure (where you play only one strategy) or mixed (where you randomize over your strategy set). A strategy is essentially a look up table (or algorithm) telling you how to behave at all decision points.  Not all games have a pure NE but all (finite) games have at least one mixed NE. At a NE, each player gains no utility from switching to a different strategy profile. Although highly celebrated, NEs are actually rarely practical for several reasons. They inhabit the PPAD complexity class (which are conjectured to be intractable), work best in 2 player zero sum (or other simple) scenarios (there can only be one winner) and will not necessarily find a fair solution concept. The utility of the the solution concept for general sum games is also questionable. Finally, although they are in some sense optimal, they do not necessarily provide the &amp;quot;best&amp;quot; results. In particular, for 2 player 0-sum games, NEs have zero expectation.

As an example, consider Rock Paper Scissors. The mixed NE for this game is to randomize evenly: play one of the pure strategies of always playing one of rock,paper or scissors with 1/3 probability (to be clear, an example of a pure strategy in this game is to always play rock).

If you always play: &amp;#128240; then I can switch to a strategy of always playing ✂. It you play a strategy like [&amp;#128074;,70% ;&amp;#128240;, 10%; ✂, 20%; ] then I can play the pure strategy of [&amp;#128240;, 100%] and perform optimally against you. This is because 70% of the time I win, 10% of the time I tie and only 20% of the time am I losing. You can expect to lose -0.5 in expectation (this might not seem like much but on average, if we play 100 rounds and bet 5&amp;#128181;/round each, then I&amp;#39;d have taken &amp;#128178;250 from you). You can check the code here, other adjustments will do worse than just playing pure paper.

Imagine you always played &amp;#128240;. Then 1/3 times you&amp;#39;ll tie with &amp;#128240;, 1/3 times you&amp;#39;ll win against &amp;#128074;, and 1/3 times you&amp;#39;ll lose vs. ✂.

There might even be runs where it will seem like you&amp;#39;re winning. If your goal is to maximize profit against an opponent, then NE might not be the correct strategy for games like RPS.

One thing you might try would be to learn patterns of your opponent and then playing a strategy exploiting that. The problem is this in turn always leaves you vulnerable. The opponent might know what you are doing and mislead you for a bit and then BAM, switch things up and collect some wins before your algorithm notices something is off. A NE has the advantage that even if your opponent knew what you were up to, they still could not (e.g.) take money off of you (in the long run).

Not all 0-sum 2 player games have the interesting quirk where it&amp;#39;s impossible to lose against an optimal player. Some games punish you for taking stupid actions. Of course, stupid is relative, and some games such as poker have action spaces so complex that it&amp;#39;s very, very difficult to know when you&amp;#39;re being stupid. For this example we&amp;#39;ll look at a simple extension of Rock, Paper, Scissors. We&amp;#39;ll add another move: telephone. How to handle rewards? If we make telephone a duplicate of paper, then we end up with a game that&amp;#39;s exactly the same as RPS save that there are now two equilibria: [&amp;#128074;,1/3 ;&amp;#128240;, 1/3; ✂, 1/3;  &amp;#128224;,0%] and [&amp;#128074;,1/3 ;&amp;#128240;, 1/6; ✂, 1/3;  &amp;#128224;,1/6]. The other aspects, where playing any mixed non-equilibrium strategy can be defeated by a pure strategy or where it is impossible to lose against an optimal player remain.

Imagine you played pure paper again. Then a NE player will either play the same mixed strategy as for RPS, with the same results or play the second NE strategy. Then: 1/3 of the time you lose against rock, 1/3 of the time you win against scissors and tie for the other two scenarios.

What if we made the telephone over-powered? That is, it ties against everything except paper, which it defeats. What&amp;#39;s the best way to play against someone who only plays telephone and what&amp;#39;s the equilibrium for this game?

Did you get it? Don&amp;#39;t play paper. As long as you don&amp;#39;t play paper then you have a zero loss expectation against a pure telephone player, which is also the pure equilibrium strategy for this game. Playing paper is the stupid action, with your loss proportional to the probability with which you play paper. In this game, it is possible to lose against an equilibrium player by playing paper.

Poker is a game like over-powered telephone, rock,paper, scissors. The optimal player only wins because there are stupid actions you can take which act essentially as donations. In our modification to RPS, it&amp;#39;s clear that playing paper is dumb. Let&amp;#39;s modify the game to make it a bit more complex. A slight mod to RPST such as &amp;#128224; loses to ✂ but defeats &amp;#128074; and &amp;#128240; yields a slightly more interesting result. Once again, playing paper is a stupid action but playing just &amp;#128224; means someone else can play just ✂. The NE is: [&amp;#128074;,1/3 ;&amp;#128240;, 0%; ✂, 1/3;  &amp;#128224;,1/3]. With some thought, you should notice that as long as we never play paper, we can&amp;#39;t lose against an optimal player. Even playing just &amp;#128074; is fine since 1/3 of the time we defeat ✂,1/3 of the time we lose to &amp;#128224; and tie with &amp;#128074; 1/3 of the time. This game has a &amp;quot;stupid&amp;quot; action but offering to play against someone out of the blue, they might not realize that paper is a trap.

The final modification will be to introduce this game: Telephone ties with telephone, Telephone loses to scissors but 50% of the time it defeats rock and 4/6 of the time it defeats paper. The game is no longer straightforward but it&amp;#39;s a lot closer to the original RPS. The NE is  [&amp;#128074;,1/3 ;&amp;#128240;, 1/3; ✂, 1/3;  &amp;#128224;,0%], other mixed strategies have a pure counter. A strategy like  [&amp;#128074;,40% ;&amp;#128240;, 40%; ✂, 10%;  &amp;#128224;,10%] can be countered with [&amp;#128240;,100%]. The expected win rate is ~0.26. This can add up:

This time, the only way to lose against an optimal player is to include telephone in your mix. Although it seems powerful (can defeat paper ~67% of the time and 50% vs rock and only loses to scissors), in the long run it loses (playing pure &amp;#128224; has a lose rate of about -0.22). If properly presented, a player can easily be tricked into thinking that &amp;#128224; is over powered.

You can look at the code relevant to this section here.

Well, I think that&amp;#39;s about enough of rock, paper scissors. Although zero sum games are well studied, this is because they are one of the few scenarios where computing a Nash Equilibrium is tractable. There are probably not many situations that are 2 player and zero sum, and the concept of zero sum is really only sensible in the two player setting. For example, people might say that markets are zero sum in the short run but that doesn&amp;#39;t make sense to me. Firstly, this requires that the traded currency amount is equal to the utility or the value placed by the respective traders. This need not be the case. Secondly, the game can really only be two player if we pretend that other players actions do not affect the available actions and information. This seems unlikely too. Beyond Games and Game Theory exercises, it&amp;#39;s hard to think up scenarios that are genuinely two player and zero sum. Sometimes I think people claim zero sum as an excuse for their selfish behavior without any real understanding of what the notion entails.

Even two player general sum games can be intractable—as I mentioned earlier general sum games can have NE that are too difficult to find because finding a NE is in the computational complexity class of PPAD.

Computational Complexity seems like a subject you&amp;#39;d get if you combined zoology, logic and a guide on how to survive the apocalypse (let&amp;#39;s prepare for the worst). There&amp;#39;s a lot of taxonomical terminology with strange names that will quickly turn any onlooker dizzy. When reading about Nash Equilibria in the context of poker bots and evolution, I nodded to myself and said &amp;quot;ah yes of course it would be PPAD...&amp;#128527;&amp;quot;. I&amp;#39;m still not confident in my understanding—there are lots of fine details—so will welcome any corrections.

Everyone who has heard of computational complexity has probably heard of polynomial time vs non-deterministic polynomial time (NP). By the way, the unwieldy naming of NP derives from the non-deterministic turing machine (NDTM) which is essentially a psychic computer and should not be confused with a proper probabilistic computer. NDTMs take choices in a way that is not determined by their state and solve NP problems in polynomial time by magic. Polynomial time (P) is in principle efficient but anything that scales faster than a factor of N^2 is truly stretching the meaning of efficient. Many useful algorithms are O(N^3) and unusable in practical settings. O(N log N) is about the limit of what is practically efficient (matrix x vector is O(N^2) but manages due to how well it parallelizes).

Problems that are NP-hard are not tractable to solve and their solutions are not necessarily efficiently verifiable. Problems in NP are efficiently verifiable and NP-complete problems are problems where a solution to one of them yields a solution to all other problems in NP. Of course P is in NP. NP-complete problems are NP-hard problems that are efficiently verifiable. P and NP actually deal with decision problems. My favorite description of the question of whether P=NP is whether search is equal to recognition/calculation. Since we can also view learning as a form of search, the question of whether P=NP can be seen as: Is learning ever necessary? From this frame, P=NP, even with a high polynomial (which would be to say, no, planning and learning are not needed in theory) seems somehow unnatural.

A decision problem is, if I asked &amp;quot;what&amp;#39;s the shortest tour through these cities&amp;quot; and you replied &amp;quot;what?&amp;quot;. And then I rephrased  it as: &amp;quot;sorry, what I really not really meant to ask, is there a tour through these cities that takes less than N number of steps?&amp;quot; to which you said &amp;quot;yes&amp;quot; and fell silent. As you can see, decision problems are very curt and business like.

Well, a corresponding class is FP and FNP which deal with function problems. This class is more literate and actually does things, like, returning an example which satisfies my touring question. FNP has the same property as NP of efficient verification. FNP might not always return an answer so there is a subclass, TFNP, dealing with total functions which always return an answer. PPAD is a subset of that class with the further requirement that the proof gadget of totality is by a directed graph. PPAD problems are conjectured as harder than FP problems and this conjecture is borne out by a lack of any polynomial time algorithms for their solution (although there are exponential lower bounds).

This is the essence of the class PPAD: search problems whose existence of solution is guaranteed by virtue of an exponentially large directed graph, implicitly represented through an algo- rithm, plus one given unbalanced point of the graph. Many problems are known to be PPAD- complete; perhaps the most fundamental such problem is related to Brouwer’s theorem
...
And it seems counterintuitive that there is always a way to telescope the search through all points of an exponentially large directed graph, to zero in the other unbalanced node, or the sink, that is sought.

Page 3 | PNAS-2014-Papadimitriou-15881-7

Many celebrated concepts of economics rely in some way on Brouwer&amp;#39;s fixed point theorem. Actually finding this fixed point is PPAD-complete, in other words, intractable. In addition to Nash Equilibria there is also the Arrow-Debreu theorem, which tells us of the existence of a price equilibrium in free markets (under realistic conditions), where allocation of resources are pareto efficient. Pareto efficiency is a concept of &amp;quot;fairness&amp;quot; which is optimal in the sense that there is no reallocation of resources that doesn&amp;#39;t leave someone worse off and unbalanced. &amp;quot;Fairness&amp;quot; because a pareto efficient allocation need not be what we would intuit as fair. In a scenario where you have 99/100 items and I have 1/100, any reallocation will leave one of us worse off.

Common attacks against markets include violations of perfect information, transaction costs, externalities, barriers, assumptions of convexity in preferences and so on.

However, even before looking at those utopian requirements, we find that the very concepts motivating markets require intractable computations to reach their notion of fairness. And these (Nash, Pareto, Arrow) are not even guaranteed to be fair in an intuitive sense. It&amp;#39;s informative to contrast these computational issues of markets with those of Planned Economies—which are at least tractable in principle (but not in practice, &amp;quot;merely&amp;quot; requiring O(N^3.5) scaling with problem size). This is not to say that markets lead to worse outcomes than planned economies, merely to point out it is technically more impossible to achieve fair outcomes with them, compared to planned economies.

At this point, the concept of what an economy entails is worth revisiting given what we now know from computational learning theory. We can do better than planned or market economies.

Although the Nash Equilibrium is much celebrated, the correlated equilibrium concept is more natural in several senses. It&amp;#39;s not just tractable but also efficiently computable to high accuracy, the dynamics of many learning algorithms converge on it (or its coarse version) and even agents acting independently can conceivably converge to its equilibrium concept. But what is it? I actually find the definition to be a bit awkward.

The correlated equilibrium is a joint (as opposed to Nash&amp;#39;s independent) probability distribution over action profiles where, the expected utility from playing according to a drawn profile, conditioned on seeing its suggested action at least matches that of any other actions&amp;#39; (it is a best response). A coarse correlated equilibrium is similar except there is no suggested action to condition on—they can lead to lame, weakly dominated suggestions however. In math, a correlated equilibrium is:

\(E_{a\sim D}[u_i(a)] \ge E_{a \sim D}[u_i(x_i,a_{-i})|a_i]\)

for every action \(x_i\). For the coarse version, there is no \(a_i\) to condition on. Algorithms which minimize external regret converge on coarse equilibria whilst those minimizing swap or internal regret converge on correlated equilibriums.

More clearly, imagine there is some neutral device. This device draws from a joint probability distribution and tells each player to play according to its suggestion: a correlated equilibrium is when there is no incentive to deviate from its suggestion. These outcomes can be better than the Nash concept since CEs act on the joint space of actions while the Nash concept is very solipsistic and self-centered, looking at signals provides no information.

The example everyone gives is a traffic light signal. When it shows GO, you know the other player sees a STOP so there is no incentive to deviate. Here are some other examples I think should count, roughly in order of how well they fit: the non (now dead) Dennard Scaling portion of Moore&amp;#39;s Law, the I give up signal in animals (such as when a dog lays on its back—it&amp;#39;s better to think of the phenotype and not any specific instantiation as the player), functioning courts and laws, rituals/tradition (however, as self identity and values shift and change, such systems quickly induce equilibriums that are inequitable).

Regret minimizing algorithms are a class of efficient on-line (meaning learn on the go) learning algorithms which given some reward function and access to some set of experts (which can also be moves such as rock in RPS), which quickly learn what actions to take in order to minimize long term regret. External regret minimizers do this with respect to a fixed strategy while internal regreters do this with respect to any swapped set of actions. The internal regret scenario lets you say when I did this, I should have done that instead. External regret says, I did almost as well as that really good advisor.

Prediction markets let people bet on binary propositions. Imagine there was a prediction market with a layer  tracking how well each bettor (expert) did. External regret could do almost as well as the best bettor and perhaps even better with randomization over all bettors, weighted according to how well they did in the past.

I believe, but am not certain, that a scenario where you could match experts to contexts: for example, a technology expert, a sports expert and so on would likely allow you to minimize something like internal regret. From this you could even extract a weighted average over hypotheses.

For more detail on regret algorithms, you can take a look at the demo code. I implement both Multiplicative Weights Update and Regret Matching.

Consider the following game: there are N people, you each choose a number between 0 and 5 and show it with your hand &amp;#128400; on a ready signal. The winner is the person displaying the lowest unique number. The winner gets a score equal to N-1 while everyone else gets a score of -1.  This makes it effectively a 2 player 0-sum game. What&amp;#39;s the Nash Equilibrium strategy? I don&amp;#39;t know but when I run this I get the following sequence:

Move

Move Probability

0

0.499

1

0.25

2

0.126

3

0.063

4

0.031

5

0.03

Which is a pattern of ~\(0.5^{x+1}\) but with some aliasing at the end. Simulation gives a mean utility of -0.006 &amp;#177; 1.31 (regret algorithms only get within epsilon of the exact equilibrium). The distribution over means is:  -0.0006 &amp;#177; 0.01. This corresponds to a win ~28.8% of the time, a loss ~56.9% of the time and a tie ~14.2% of the time. This looks close to what&amp;#39;s realistically achievable. It&amp;#39;s also interesting that in this game it is both possible to not be able to lose (if playing against two optimal players) or lose while providing donations to a single optimal player (with two off equilibrium players).

What happens when we try for a coarse correlated equilibrium? In this, I also apply regret matching on 3 players trying to maximize utility while looking at the other players. Sometimes, an interesting strategy emerges which kind of looks like collusion in that two players end up with positive expectation (e.g. 61% win, 39% lose, 0.83 expected utility; 61% lose, 39% win, 0.17 EU) and one is guaranteed losses such that it doesn&amp;#39;t matter what they do:

Move

Move Probability

0

0.395

1

0.289

2

0.132

3

0.079

4

0.053

5

0.053

GUESS STRATEGY2

Move

Move Probability

0

1

1

0

2

0

3

0

4

0

5

0

GUESS STRATEGY3

Move

Move Probability

0

0

1

0.091

2

0.091

3

0.455

4

0.182

5

0.182

Usually, a strategy emerges where one dominates: (e.g. 77% win, 23% loss,1.3 expected utility), another has moderate-large losses (23% win, 77% loss,-0.3 expected utility) and the last has guaranteed losses. This is surprisingly reflective of reality &amp;#128541;&amp;#128532;

Another strategy would be if there was some device which sampled uniformly from a joint probability distribution after eliminating any ties. This would have expectation equal to the game for all, that of zero, matching the Nash equilibrium. No one would have any incentive to deviate knowing that everyone was playing according to the suggestions of the device. Note that knowing my number also tells me something about what the others saw: if I get a 2, I know that no one else received that number. Something else worth noting is that part of this game&amp;#39;s problem is it&amp;#39;s zero sum. If the game was not zero sum, the outcomes could sometimes be even better than Nash.

Code here:

This example is a simple game where two players are at an intersection. If they both GO &amp;#128665; they both suffer a utility of -100. If one drives and the other STOPs &amp;#128721; then the goer gets a utility of 1 and the other 0. If they both &amp;#128721; then they both get a utility of 0. This is no longer a zero sum game. The regret matching algorithm learns the following strategy:

Move

Move Probability

A &amp;#128721;

0.99

A &amp;#128665;

0.01

This is pretty lame since there is a 0.01% chance of a catastrophic incident. The expected utility is ~zero for both players. If we try regret matching on two players that try to coordinate we get:

DRIVE STRATEGY

Move

Move Probability

A &amp;#128721;

0

A &amp;#128665;

1

DRIVE STRATEGY2

Move

Move Probability

A &amp;#128721;

1

A &amp;#128665;

0

In this scenario one player always goes and the other always stops.  Thus one player gets a utility of 1 per round while the other always gets 0 but there is a 0% chance of a crash. While not ideal for one player, the situation is overall better than the Nash equilibrium strategy which has an expected utility of 0 for both players. Notice that without some way to independently and reliably coordinate, it is not possible for the pair to do better than the above.

If instead we optimize over the joint space of actions we get:

DRIVE STRATEGY

Move

Move Probability

(A &amp;#128721;, A &amp;#128721;)

0

(A &amp;#128721;, A &amp;#128665;)

0.5

(A &amp;#128665;, A &amp;#128721;)

0.5

(A &amp;#128665;, A &amp;#128665;)

0

This is a correlated equilibrium and is fair for all parties, with both experiencing a positive utility. Yet there is a sense in which this result is unsatisfying. It is rare that we can compute or even know what joint probability distribution is being sample from, to speak of how to properly compute a reward on it. But as I mentioned above, agents working independently can sometimes learn to do this.

In this (scroll down to bottom to see results) modification to the driving game, I use Multiplicative weights update with two layers of weights. The first layer of weights computes the cost of adhering to the signal or listening to an inner expert. The inner expert learns regret from playing either &amp;#128721; or &amp;#128665;. Thus the agent has the ability to either heed the signal or do as it pleases. To make things interesting I vary the reliability of the signal. A signal with 90% reliability does as it pleases 10% of the time. In this case the agents learn to ignore the signal and settle on a strategy equivalent to when there is no signal (one always goes and the other always stops):

Agent1: [&amp;quot;&amp;#128678;: 0.0%&amp;quot;; &amp;quot;&amp;#128527;: 100.0% [(A &amp;#128721;, 0.0); (A &amp;#128665;, 1.0)]&amp;quot;],

Agent2: [&amp;quot;&amp;#128678;: 0.0%&amp;quot;; &amp;quot;&amp;#128527;: 100.0% [(A &amp;#128721;, 1.0); (A &amp;#128665;, 0.0)]&amp;quot;]

Sometimes however, one agent will choose to heed the signal while the other chooses to always stop:

[&amp;quot;&amp;#128678;: 0.0%&amp;quot;; &amp;quot;&amp;#128527;: 100.0% [(A &amp;#128721;, 1.0); (A &amp;#128665;, 0.0)]&amp;quot;],

[&amp;quot;&amp;#128678;: 99.44%&amp;quot;; &amp;quot;&amp;#128527;: 0.56% [(A &amp;#128721;, 0.4); (A &amp;#128665;, 0.6)]&amp;quot;]

It takes about 99.9% reliability for the agents to consistently choose a strategy that goes along with the signal, matching the correlated equilibrium.

[&amp;quot;&amp;#128678;: 99.68%&amp;quot;; &amp;quot;&amp;#128527;: 0.32% [(A &amp;#128721;, 1.0); (A &amp;#128665;, 0.0)]&amp;quot;],

[&amp;quot;&amp;#128678;: 99.72%&amp;quot;; &amp;quot;&amp;#128527;: 0.28% [(A &amp;#128721;, 1.0); (A &amp;#128665;, 0.0)]&amp;quot;]

This result still feels unsatisfactory since one might argue that there is too much hand-holding. Therefore, in this version, I decided to make a learner that must learn how to read the signals and how to coordinate. The hand holding is minimal and typical of features in Machine learning algorithms. The system is inefficient in that it tracks all possible combinations—this is clearly untenable for more players,signals, and granularity. However, it doesn&amp;#39;t take much work to modify the system to explore adding  and removing rules in an attempt to learn a Set Cover over rules. But generally, one of the hardest parts of machine learning is getting efficient representations of large or complex state spaces.

In this model, any rule that matches the current situation is filtered to, after which weights are normalized for this reduced space. An action is taken and then a reward is used to calculate regret on only the active rules. This system is richer than all the previous and so allows us to explore an interesting few scenarios. The game is also modified as such: a player acts first and then the second player decides what to do based on what the first player does.

If we have the first player choose an action without looking for a signal and then have the second player decide based on the first the following major rule pair is arrived at:

[(1.0, &amp;quot;if signal=Do &amp;#128721; &amp;amp;amp;&amp;amp;amp; other=Do &amp;#128721; then Do &amp;#128665;&amp;quot;)],
[(1.0, &amp;quot;if signal=Do &amp;#128721; &amp;amp;amp;&amp;amp;amp; other=Do &amp;#128721; then Do &amp;#128665;&amp;quot;)]

Essentially, only drive if the other driver stops. Because of how the system filters matching rules, missing values end up such that only one value is looked at but this ends up including rules which don&amp;#39;t make sense when the full conjunction is looked at. One fix would be to include an additional state representing none, but that&amp;#39;s not necessary for demonstration purposes. Nonetheless, the agents work around that issue and learn a useful set of rules. Sampling some interactions we see that most of the time the first car will do whatever (usually drive) and the second one will drive or stop depending on the other. We see that the situation learned is better than the experts above without access to a signal. The first player dominates but the second player still sees a positive reward.

[((Do &amp;#128721;, Do &amp;#128721;), 0.1); ((Do &amp;#128721;, Do &amp;#128665;), 0.36); ((Do &amp;#128665;, Do &amp;#128721;), 0.54);((Do &amp;#128665;, Do &amp;#128665;), 0.0)]

If we instead provide a signal for what the other will do as the light they see (the system can&amp;#39;t reason after all), the following rule pair is matched:

[(1.0, &amp;quot;if signal=Do &amp;#128665; &amp;amp;amp;&amp;amp;amp; other=Do &amp;#128721; then Do &amp;#128665;&amp;quot;)],

[(1.0, &amp;quot;if signal=Do &amp;#128665; &amp;amp;amp;&amp;amp;amp; other=Do &amp;#128721; then Do &amp;#128665;&amp;quot;)]

And we see that their behavior is converging on a correlated equilibrium:

[((Do &amp;#128721;, Do &amp;#128665;), 0.49); ((Do &amp;#128665;, Do &amp;#128721;), 0.5); ((Do &amp;#128665;, Do &amp;#128665;), 0.0)|]

On the other hand, if we take away the ability for the agents to attend to what the other will do they will tend to learn either a set of rules that are either nearly a Nash Equilibrium or sometimes, nearly a correlated equilibrium (something I find to be surprisingly robust).

[((Do &amp;#128721;, Do &amp;#128721;), 0.97); ((Do &amp;#128721;, Do &amp;#128665;), 0.03); ((Do &amp;#128665;, Do &amp;#128721;), 0.0)]

In essence, so long as you can jointly learn to attend to signals, and so long as there is some shared memory to draw upon (history, text) then many learning agents can efficiently converge on a correlated equilibrium, bottom up. Without any top-down enforcement of joint distributions. In the absence of anything to condition and coordinate actions by, coarse equilibria (which I expect many short term—within the life of an individual—scenarios share much in common with) can be subpar.



As an example, I think the historical knowledge of Bali Rice Farmers [1], with insects and weather as the coordination signal lead to a bottom up correlated equilibrium of planting patterns.

So far (excepting the last rule based AI scenario), the focus has only been on Normal Form (no turn taking) games but the story for imperfect information extensive (multiple turns) games is not that much more complicated. And going to repeated or iterated games is no trouble at all. It&amp;#39;s only a handful more lines to be able to handle counterfactual scenarios. This is in contrast to the study by game theory where the complexity suddenly ratchets. In fact, the methods are so complicated that only simpler toy problems can be studied. However, with learning algorithms that have been proven to converge on useful equilibria concepts, we can design scenarios and look at what happens to study multiplayer, repeated, stochastic, imperfect information extensive games. As I&amp;#39;ve not yet had need to venture into those areas, my knowledge there is limited and only of a sketch like nature.

Compared to poker the state space is very large and it&amp;#39;s difficult to imagine how the notion of information sets will apply here. Rounds are ill define and there are really no such things as turns. Within each game there are many interactions that aren&amp;#39;t zero sum. Learning a precomputed nash equilibrium strategy simply isn&amp;#39;t tenable. Especially if we want an AI that can do just as well in multiplayer scenarios with no additional fuss. Starcraft is complex enough that a low power, low data solution stands a chance, I think.





In this essay I talked about Nash Equilibria and how it is impossible to lose against an optimal player in some zero sum 2 player games. I then talked about why some 0-sum 2 player games allow for losses. I mentioned the likely intractability of Nash Equilibria and many market economy concepts. I also discussed computational complexity and in particular, the PPAD complexity of Nash Equilibria and what that roughly means. I also mentioned correlated equilibria and how they are common, efficiently computable and often lead to more preferable outcomes than Nash Equilibria.

I noted that coarse correlated equilibria are likely most common given the lack of signals to coordinate on in many real world situations. These can lead to positions for many players that are at least weakly dominated and therefore highly not preferable.

Correlated Equilibria can be learned so long as multiple agents can learn to attend to the conjunction of certain patterns, together with some memory or shared history, to reach a point where there is no incentive to deviate. I motivate min-regret learning as a way to achieve correlated equilibria using a modification to prediction markets as an example.

I mention that barring a simple hack for Starcraft, it poses a much more difficult problem than Poker or Go. It&amp;#39;s interesting to see aspects of Moravec Paradox reflected here too. It takes more effort for a computer to be better than a human in Starcraft than at Chess or Go. Despite Starcraft being considered the less intellectual game.

The full article contains links to code that you can run in your browser to explore simple non-deterministic, zero and non-zero sum games. Namely modifications of Rock-Paper-Scissors, a game where two drivers can either both stop or drive or some variation thereof. Finally, a game where 1 of 3 people has to guess a number that&amp;#39;s less than or unique compared to what everyone else guesses. I also demonstrate a rule learning AI using regret to learn in matched subspaces, how to use signals and then coordinate under minimal handholding to show how learners might achieve a correlated equilibrium.

[1] http://www.pnas.org/content/114/25/6504.full.pdf

[2] http://www.cis.upenn.edu/~aaroth/courses/slides/agt17/lect08.pdf

[3] http://modelai.gettysburg.edu/2013/cfr/cfr.pdf</description>
    </item>
    <item>
      <title>RNNs are probably not practically Turing Complete.</title>
      <link>http://www.metarecursive.com/writings/RNNs_are_probably_not_practically_Turing_Complete..htm</link>
      <guid>http://www.metarecursive.com/writings/RNNs_are_probably_not_practically_Turing_Complete..htm</guid>
      <pubDate>Fri, 25 Aug 2017 21:06:22 GMT</pubDate>
      <description>

Are RNNs Turing Complete? Is a question that has bothered me for some time. I still do not have an air tight resolution but my general belief is no, not really.

This issue came to the fore today for me, when the author of Keras (Fran&amp;#231;ois Chollet‏), a popular wrapper atop the Tensorflow deep learning library, stated that matrix multiplication is insufficient to capture general intelligence. He also made two confusing statements: Long Term Short Term Memory Networks are not Turing complete but ConvLSTMs are. Recurrent Neural networks have been proven to be Turing complete and LSTMs generalize these, therefore LSTMs should also be Turing complete (the LSTM should learn not to use the &amp;quot;forget&amp;quot; gate if it is getting in the way).

Convolutional nets are a simplification of Multilayer Neural networks—that is, they invoke several structural biases such as translational invariance and how to coarse grain to reduce the number of parameters to be learned—therefore, it is difficult to see what they could add to LSTMs that they lost in comparison to RNNs. This is answered in [2], where the authors point out a close correspondence between Neural GPUs and Convolutional LSTMs. Neural GPUs extend Gated Recurrent Units (a simplification of LSTMs) to operate on state that is a 2D grid of vectors, which are then convolved over across time steps. This allows for more parallel computation and efficient implementations.



They draw a correspondence with 2D discrete cellular automata, as a class with similar computational power. While it seems likely that Turing Complete algorithms can be less elaborately represented in this architecture, the learnability of more complex many step algorithms remains strongly in doubt. They mention difficulty multiplying decimal numbers as well as scaling to larger instance sizes of say, binary addition problems.

Those trivialities aside, I actually agree with his statement from the tweet:



Before discussing his important and very likely true point, it&amp;#39;s worth looking at the proof of the Turing Completeness of RNNs and why I don&amp;#39;t think it&amp;#39;s of practical consequence.

I found the proof difficult to unpack—it&amp;#39;s dense, there&amp;#39;s lots of notation and even outmoded terminology. It seems like it would take me days to fully understand it. But I could extract a sketch:

It is known that two Push Down Automata (a Finite state machine extended with a stack) to simulate both sides of a Turing Machine, are Turing Equivalent. The proof then sets up a dynamical system which can simulate two PDAs. The task is to simulate that system with a neural net. At this point, it&amp;#39;s worth pointing out as the paper does, that Minsky proves the existence of a Universal Turing Machine with just 7 control states and a 4 letter tape. Thus, their construction only needs a relatively small number of states to be universal.

To complete the proof, they show the composition of a series of functions (defined over the rationals) computes the dynamical system of interest. Their recurrent neural network is shown to match this. However, the construction is very particular. For example one layer computes:

\(\sigma(4q_i - 2\zeta[q_i] - 1 + \sum\limits_{j=0}^s \sum\limits_{i=1}^{16} c_{ij}\sigma(v_i \cdot\mu) - 1)\)

where ζ is a thresholding function over the subset of rationals expressible as a particular kind of Cantor set. The method also seems to need some post processing in the form of re-ordering and such like. It&amp;#39;s an elegant argument (which I don&amp;#39;t even fully understand), but what is clear is that this recurrent net bears little resemblance to systems as used in practice. I daresay it&amp;#39;s closer to cellular automata than LSTMs. For example, suppose I showed you a rule 110 automata and said: &amp;quot;you know, in principle it is possible to encode an AI in this flashing pattern of black and white dots that would want to harvest humans for carbons&amp;quot;.  You&amp;#39;d probably say &amp;quot;neat!&amp;quot; and promptly forget. It&amp;#39;s not the kind of observation that comes with any practical consequence.

Another (and the easiest) objection to make is that the proof requires rational weights while most neural networks utilize 32 bit (and soon 16 bit) floating point numbers as weights. But a similarly easy objection is that the neural nets can learn a good enough approximation. I doubt it—the rationals require a particular and precise form, else the memory would not be dependable. The nets would need to learn corrections atop the structure and stack operations. This seems unlikely.

The other objection is there&amp;#39;s no known method to learn on these nor any guarantee that your training will ever converge on such an architecture. Simulated annealing over arbitrary network architectures might get there first—and it only converges giving an infinite amount of time. A response in turn might be that the proof is not necessarily the only form a Turing Complete RNN can take. The proof only shows that such an encoding is possible and it might therefore be feasible that RNNs could arrive at easier representations. This is plausible and it leads me to my third objection, pointed out and explained here by Edward Grefenstette. The memory vector has finite state and newer inputs saturate such that RNNs only learn short ranged correlations and are not much more powerful than a Finite State machine.

Most sequence to sequence architectures come in encoder/decoder pairs. Encoders are folds and decoders are unfolds. A fold is a function that structurally recurses over and deconstructs an input.

In the last example, the s stands for the string built so far and would correspond to the &amp;quot;hidden state&amp;quot; or memory of an RNN.

An RNN is:

(A List of some kind) ▷ fold ((h,x) ⇒ \(W_{hx} x + W_{hh} h + b_h\) ▷ Vector.map tanh) [0,0,...,0]

Wxh and Whh are matrices which together represent whatever function so far best minimizes error with respect to some loss function. If the function above was calculated with respect to say, dual numbers—automatic differentiation—then one gets derivatives for free, which one can then use to adjust the weights; hence, learning. There is a great deal more plumbing to actually make things work but the above captures the essence.

The decoder is more of an unfold, which constructs possibly an infinite stream given some initial state:

unfold (state -&amp;gt; Maybe(Output,state&amp;#39;)) initial_state

Where the maybe terminates given an appropriate condition. Encode/Decode, such as in language translation (encode english, decode japanese) then becomes a fold ∘ unfold style computation.

Since the internals are just affine matrix transformations and the general structure is one of hylomorphisms, I think RNNs are at most learning total functions. The (encoder) RNN is searching for a function like (+), that combines the current state with the new input. This is a good thing—learning arbitrary programs would make training undecidable (humans run some parts in parallel—it&amp;#39;s feasible for a human to spend their entire runtime searching for structural regularities—with those processes never terminating, until a segfault).

If we bring limited memory back into the picture, we see that the number of bits representable by state is limited by the size of the vector. Since the state is degraded for distant inputs, the search will focus on functions that depend on a relatively short history. As a rough analogy: in the above where we were folding over the characters of a word, imagine instead if the string was of limited size and we wanted to build patterns based on the previous patterns. We would be limited in what we could express.

The LSTM makes the best out of a crappy situation by learning what to evict and what to keep, learning longer (but still limited) range correlations. LSTMs might be close to Pushdown automata with a faulty stack. So RNNs I think, while probably in principle (co)inductive, learn only a subset of that class of functions. The subset which are smooth and well enough behaved and that also satisfy the low weight prior of stochastic gradient descent.

RNNs don&amp;#39;t just have to learn good representations, they also have to learn operations and algorithmic transformations on the data. This means trading feature engineering time for more Joules and longer, more complicated training regiments.

The combination of few assumptions, powerful model class and large complicated space all but guarantees the requirement of a large amount of data and energy spend. Several steps have to be learned to make progress and this quickly gets very complicated by the time high level concepts are the target. This, in my opinion is wasteful—even humans come with certain inductive biases. Finding the right combination of not overly specific hand engineered features seems like one of the many things which will need to be discovered to arrive at more powerful learners.

All together, the above highlight my strong skepticism of the Turing completeness of RNNs. The worries that they&amp;#39;ll replace everything, including humans, by 2025 are verging on nonsensical.

[1] https://pdfs.semanticscholar.org/1759/4df98c222217a11510dd454ba52a5a737378.pdf

[2] https://arxiv.org/pdf/1511.08228.pdf</description>
    </item>
    <item>
      <title>On N Year Floods</title>
      <link>http://www.metarecursive.com/writings/On_N_Year_Floods.htm</link>
      <guid>http://www.metarecursive.com/writings/On_N_Year_Floods.htm</guid>
      <pubDate>Mon, 04 Sep 2017 00:00:00 GMT</pubDate>
      <description>I think several things about this video. The first is, a lot of people commit the gambler&amp;#39;s fallacy when talking about 100 year or 500 year floods. On twitter, someone thought a 500 year flood was due (100% and guaranteed by the 500th year). But that&amp;#39;s not true. A better way is to look in terms of a binomial distribution. This takes each year as a trial of a Bernoulli. For a 500 year span, we compute P = Bin(1/500, 500) in n+1, 1 - P(X ≤ n) for n in 0..3 as:

Chance of 1+ occurrences is ~63%

Chance of 2+ occurrences is ~26%

Chance of 3+ occurrences is ~8%

Chance of 4+ occurrences is ~2%

5+ is negligible. But even for a 500 year flood, a short time interval (better think a few number of trials), say 10 years (trials), the probability of 1+ successes is non-negligible.

Chance of 1+ occurrences is ~1.98%

Chance of 2+ occurrences is ~0.02%

If we look at a &amp;quot;100 year flood&amp;quot; for a span of 10 trials (years), it&amp;#39;s:

Chance of 1+ occurrences is ~9.56%

Chance of 2+ occurrences is ~0.43%

Chance of 3+ occurrences is ~0.01%

But I find not specifying any notion of uncertainty really dissatisfying. A better way to do this would be to specify a prior such as Beta(1,499) and then for each year, update with the number of occurrences of 500 year floods for the region. Over time, the distribution will shift, narrow or broaden reflecting changes in our uncertainty about what the parameter for our Bernoulli distribution should be.



Better yet, would be to model rainfall volumes directly, with say a log normal distribution (the video had a short snippet on normals but I assume this is due to their familiarity, rain fall is probably better modeled as log normal [1]). Then I might ask &amp;quot;what&amp;#39;s the probability of seeing such an amount?&amp;quot; Again, I&amp;#39;d specify a prior over possible values for the parameters of the distribution to capture my uncertainty. With another model for the frequency of rain fall, I could then talk about N year floods except I would also be able to cleanly talk about N months, N day floods etc. And most importantly, I&amp;#39;d be able to parameterize my uncertainty. The upper end of the distribution could be used to plan for worst case scenarios instead of misleading people with useless point estimates.

I expect that over time, the number of formerly rare events will increase. The link between CO2 and the greenhouse effect has been known since at least the early 1900s. With melting poles, increased moisture and higher ocean temperatures, we can expect better fed hurricanes and tropical storms. Floods, heat waves, droughts and forest fires will be more common too. The general pattern of weather in places will shift as a result of change in the &amp;quot;parameters&amp;quot; deciding climate. This in turn will make weather prediction more difficult.

Tornadoes, hailstorms and even snowstorms—can those be linked to warming? The default answer would be to state or even over-state our uncertainty and bemoan our lack of data. I think that&amp;#39;s wrong, optimizing for average case correctness is not always ideal. When lives and livelihoods are at stake, it&amp;#39;s better to minimize regret and plan for the worst. Therefore, if ever I&amp;#39;m asked is this because of climate change? My answer will be yes and summarizing the previous recent paragraphs. Then I&amp;#39;d talk about what needs to be done, so people don&amp;#39;t forcefully forget about it in hopeless despair.

[1] http://journals.ametsoc.org/doi/pdf/10.1175/1520-0450%281976%290152.0.CO%3B2</description>
    </item>
    <item>
      <title>A bit on Inference and Intelligence</title>
      <link>http://www.metarecursive.com/writings/A_bit_on_Inference_and_Intelligence.htm</link>
      <guid>http://www.metarecursive.com/writings/A_bit_on_Inference_and_Intelligence.htm</guid>
      <pubDate>Thu, 21 Sep 2017 08:06:28 GMT</pubDate>
      <description>What does it mean for a system to be intelligent. Or Artificially so? The phrase Artificial intelligence is a contentious one, and for me, the cause of a not insignificant amount of frustration in discussions because everyone brings their own special meaning. However, looking closely at what it means to learn, it&amp;#39;s clear that the notion of intelligence is not one of what but instead of what on. What&amp;#39;s important and I believe often missed, is that there is no algorithm for intelligence, instead there are algorithms and whether they yield intelligence or not is task dependent.

Artificial intelligence is a broad term and according to Markoff [1], was coined by John McCarthy who wanted a name avoiding any association with Norbert Wiener&amp;#39;s cybernetics. McCarthy also considered Shannon&amp;#39;s preferred term of automata as having too much of a mathematical framing. Of the terms from that era, I personally favor complex information processing.

Until recently (circa 2005), Artificial Intelligence (AI) was a term to be avoided, in preference for machine learning, due to stigma from the overpromises and overreach of two decades prior. Today AI can mean anything from linear regression (basically just a drawn out addition and multiplication process) to The Terminator. It&amp;#39;s a broad term and while I do not often view its use as wrong, there are usually more informative terms to use. Calling certain autocompletes AI, while not technically wrong, is in the same class of phrasing as referring to people as biological entities. This is true, people are biological entities but so are slime molds. Perhaps use a term with better specificity? Telling me there is a biological entity in front of me can require very different reactions depending on whether it&amp;#39;s a tree, person, spider or elephant. Or tiger.

One common objection is: &amp;quot;that&amp;#39;s not AI, that&amp;#39;s just counting&amp;quot; or, &amp;quot;that&amp;#39;s just algorithms&amp;quot;. There are two observations I can make about this. 1) The assumption that animals are not limited to running (computable) algorithms. This may be true but it would come with a lot of baggage (why can no one factor 9756123459891243 in their head?). Things that a human can do but a computer cannot are few (things that a human can do but a computer cannot do well are numerous). It would be very surprising to find out humans spend all their hypercomputing ability solely on emotions and being &amp;quot;conscious&amp;quot;. It&amp;#39;s too early to require such a stance.

Observation 2) is that the algorithm is not all that matters, it also matters where said algorithm is being applied. Consider the following two examples: path tracing and Bethe&amp;#39;s 1935 work on Statistical Theory of Superlattices.

Path Tracing is a way of computing lighting or illuminations so as to achieve a photorealistic rendering of a scene. At its most basic, a method known as importance sampling is used to estimate the integral of the equation that solves the light path calculations.  That same technique, when employed on data, yields a form of approximate bayesian inference. One might complain that I&amp;#39;m cheating, and really, only a way of approximating integrals is what&amp;#39;s shared. Furthermore, the algorithm is effectively making guesses on how the light likely behaved. But then again, that there should be such a close correspondence between efficient sampling and averaging of per pixel illuminance values and real photos is not obvious to me.

The link between learning and physics can be made even stronger when we look at how often bayesian inference and strategies developed by physicists to study systems with many interacting variables coincide (it&amp;#39;s no surprise then, that thermodynamics turns up often, but it is by no means the only example). Ideas that are now showing up as important in both neuroscience and machine learning derived from work, such as mean field theory, done by physicists in the early 90s and late 80s. The broader historical pattern is physicists got there first—whether studying phase transitions or whatever it is they do—and computer scientists studying inference get there a few decades later.

Take the case of inference on tree structured distributions, computing the marginal distribution over the nodes can be done exactly using a message passing algorithm by Judea Pearl known as belief propagation (invented in the early 1980s). It turns out that none other than physicist Hans Bethe had similar ideas when modeling ferromagnetism [3] in the early 1900s. From [2]:

Notice, within the historical perspective, that the Bethe solution for the spin glass on a random graph was written by Bowman and Levin [44], i.e. about 16 years before realizing that the same equation is called belief propagation and that it can be used as an iterative algorithm in a number of applications. The paper by Thouless [299], who analyzed the spin glass critical temperature, implicitly includes another very interesting algorithm, that can be summarized by equation (78) where BP is linearized. The fact that this equation has far-reaching implications when viewed as a basis for a spectral algorithm waited for its discovery even longer.

Here the connection is non-trivial. It&amp;#39;s not mere happenstance from using the same calculating tool but more fundamentally, the same core computation is in effect being performed. There&amp;#39;s a key link in that the fixed point of belief propagation corresponds to stationary points of the Bethe free energy [4]. Unfortunately, the places where belief propagation works are limited to sparse graphs and trees, so sampling based methods are often preferred for practical inference. These too, find their origin in physics (monte carlo, gibbs sampling, variational mean field) but of particular interest are the variational methods.

It is proven via computational complexity methods that bayesian inference is intractable (we can never build a computer powerful enough to handle interesting/medium-large problems exactly). In particular, the normalizing constant is hard to compute (fancy way of saying divide so that it sums to one), because the are too many possible settings of parameters to consider. As such, we must resort to approximations. The favored way to do this is to approximate the target (posterior) probability distribution with another less busy one that&amp;#39;s simpler to work with. It&amp;#39;s also required that this approximation and our target diverge as little as possible. To do this, we search over a family of distributions for a distribution whose parameters are such that we get the best approximation of the target distribution (minimizing this divergence directly is also difficult so what happens instead is yet another layer of indirect optimization). When the space of models we are searching over is effectively continuous, this means searching over a space of functions for a function, given our requirements, which best represents our target. Hence the name variational methods—which derives from work done by 19th century mathematicians seeking an alternative more flexible grounding for newtonian mechanics.

The Calculus of Variations was mostly (initially) developed by the Italian mathematician/mathematical physicist Lagrange and Swiss mathematician Euler. The calculus began in the study of mathematical problems involving finding minimal length paths (that must satisfy some condition, most famously, what path takes the shortest amount given that our point is following the constraints of gravity).



The general format is, given some function that accepts functions and returns a number (a higher order function or functional), find a function that is optimal for our functional (and goes through these two points, say). The solution, one of: a maximum, minimum or flat resting stop, is known as a stationary point. The method of calculating this derivative on functions (since derivatives are themselves higher order functions, the types must be gnarly!) involves an analogy with the regular case. The functional must be stable to minor perturbations or variations. A perturbation is how the functional output changes with minor variations instigated by another function. When this is done with respect to some detailed energy calculations, one gets the Stationarity Principle of physics.

In nature we find, things like actions and paths are often taking minimal length paths, the relatively well known principle of least action. Unfortunately, this is a bit of a misnomer since quantum mechanics tells us that what nature really has is a principle of most exhaustively wasteful effort. QM tells us that all paths are taken and we only seem to see one path because only the stationary values correspond to the values (complex numbers) where phases did not cancel. As to why that is, I do not know but if it were any other way, if nature had to actively choose and hence know which paths were stationary, I suspect we would live in a very different kind of universe. Could we build computers that could solve problems our universe sized ones couldn&amp;#39;t?

Nonetheless, the way mere humans have to calculate things is to seek out that critical point and to get that, we must use tools of optimization. These tools have turned out to be very general; so long as some internal structural requirements of the problem are satisfied, it can be used on that problem.

Parametric Polymorphic functions are algorithms that don&amp;#39;t care to look at the details of the thing they are operating on, so long as some structural requirements are met. For example, a function that gives a list of permutations doesn&amp;#39;t care if the list is of integers, movie titles or fruit baskets. The same reasoning can be used to partially explain why many ideas from physics are useful for building learning systems. Both physical objects and learning systems can be viewed as optimizing for some value, have a notion of a surface or manifold (for inference, space of models) that&amp;#39;s moved along, a notion of curvature (in inference this is indirectly related to correlation) and distance (symmetric divergence between probability distributions) and a requirement to efficiently move along these surfaces seeking minimal energy configurations.



In both, there is a need to search for a function that acts as a good minimizer given our requirements. An object trying to minimize some quantity efficiently has only so many options to make. The correct way to move efficiently on a manifold constrains what operations you can perform. These algorithms and strategies are agnostic to the internal details of the problem; put in energy and physical things you solve physics problems. Put in log probabilities and solve learning problems. However, learning algorithms did not have to do this and in fact they do not have to follow these constraints. It&amp;#39;s just that, if you wish to learn effectively, you&amp;#39;ll behave in a way parametrically equivalent to physical systems bound by a stationary principle. Physicists often arrived first because they had physical systems to study, observe and constrain methods. Computer scientists arrive later, at the same spot, asking, how do we do this more efficiently?

If this seems a bit undramatic, don&amp;#39;t be sad, as there is still a lot of mystery. The universe didn&amp;#39;t have to be this way. Phases didn&amp;#39;t have to cancel in such a way that we observe a seeming least action principle. Bayesian inference (and therefore all forms of learning, since they all reduce to approximating a probability distribution) did not have to be intractable. Or we could have lived in a universe with accessible infinities, for which computations difficult for us in our universe, were easy. But because of both these conditions, variational methods or similar have been important in getting correct results in both fields.

Nonetheless, it is a bit uncanny how often probabilistic inference on particular graphs structures often have a precise correspondence with a physical system. Consider, message passing to compute a marginal probability on one hand allows you to do inference and on something else, works out local magnetization. Why belief propagation and Beth Approximation ideas work as well as they do for computing a posterior probability distribution (a knowledge update) is not well known. Part of the uncanniness is of course, a lot of thermodynamics is also inference but that is less than a satisfactory answer. The boundary between entropy and information and as such, energy is very fuzzy for me. In a future essay I&amp;#39;ll look more into this but for now, I&amp;#39;d like to look at variational free energy and what it says about the brain&amp;#39;s computational ability.

Variational free energy is an informational concept (negative model log likelihood for the data) that slots into where (IIUC) Gibbs free energy would fit into for a physical system. In an inferential system with an internal world model, variational free energy does not tell you about the physical energy of the system. It instead tells you how well your internal model matches your sensory inputs or data inputs or historical signal inputs. You want to minimize any future discrepancies as much as possible but don&amp;#39;t want to drift too far from your prior experiences. The concept of variational free energy finds much use in machine learning and neuroscience. The better the model, the lesser the free energy on our variational approximation and the better we are doing. In other words, the less surprises we are having.

Here&amp;#39;s what&amp;#39;s key. As I point out above, variational methods are an approximation due to computational limitations. If we find that it is in fact true that the brain is also forced to optimize on some variational bounds and no better than a mere pauper Turing machine (resource bounded) , this too would be very suggestive that the brain itself is not just computable but bound by computational complexity laws!

The link between learning and thermodynamics goes deeper than simple correspondence however. In [5], Friston et al also point out how the brain will also operate to minimize its Helmholtz free energy by minimizing its complexity (giving less complex representations to highly probable states. You should not be surprised then, when we find expertise means less volume in highly trafficked areas or less energy use for mental processing of well understood things). Similarly, in the very interesting [6] Susanne Still shows that any non-equilibrium thermodynamic system being driven by an external system must alter its internal states such that there is a correspondence with the driving signal and coupling interface. This is necessary to minimize wasteful dissipation. Therefore, it must have something of a memory and state changes will implicitly predict future changes in order to operate efficiently, maintaining favorable current states. As such, efficient dynamics corresponds to efficient prediction.

Evidence for the importance of energetic efficiency is furthermore found in biomolecular machines that approach 100% efficiency when driven in a natural fashion: the stall torque for the F1-ATPase [26] and the stall force for Myosin V [27] are near the maximal values possible given the free en- ergy liberated by ATP hydrolysis and the sizes of their respective rotations and steps. These and many other biological functions require some correspondence between the environment and the systems that implement them. Therefore the memory of their instantiating systems must be nonzero. We have shown that any such system with nonzero memory must conduct predictive inference, at least implicitly, to ap- proach maximal energetic efficiency.

We thus arrive at an interesting separation. All systems we call alive (right down to bacteria) concern themselves with both variational and thermodynamic free energy but digital AIs only concern themselves with the variational concept.

In summary, those who reject certain algorithms as AI are making a fundamental mistake by assuming that the algorithm is what makes an AI. Instead, it&amp;#39;s where the algorithm is used that matters. A simple dot product (something no more complex than 5 * 3 + 6 * 2) in one condition might be a high school math problem or find use in lighting calculations in a graphics engine. In another context however, it might compare word vector representations of distilled co-occurrence statistics or encode faces in a primate. We should expect then, that an AGI or an ASI will consist of narrow AI joined together in some non-trivial fashion but still no different from math.

I additionally pointed out the correspondence between inference and physical systems is not so surprising when viewed as aspects of something more general, analogized with a polymorphic function. But it is nonetheless not obvious why things ended up as such. Computational complexity limits and energy limits turn up at the same places surprisingly often and demand similar dues of informational and physical systems.

The link goes even deeper when we realize that predictive efficiency and thermodynamic efficiency of non-equilibrium systems are inextricably linked. Not just that brains and predictive text autocomplete should count as performing inference but also, simple biological molecules. In fact, these systems might have gotten as complex as they did in order to be more effective at prediction, in order to more effectively use a positive free energy flow for say, replication or primitive metabolism.

I can now finally put forward a definition for what AI is. An AI is any algorithm that has been put to the task of computing a probability distribution for use in some downstream task (decisions, predictions), a filter which leverages the structure of what it is filtering, or performs a non-exhaustive search in some space. Autocomplete that enumerates alphabetically is not intelligent, Autocomplete that predicts what I might type next is. From the context of Intelligence amplification, an intelligent algorithm is any system that works cooperatively to reduce working memory load for the human partner.

In a future post I&amp;#39;ll look into what algorithms the brain might be running. This will involve synthesizing the following proposals (what they have in common, what they differ in and how plausible they are): Equilibrium Propagation, Sleeping experts, approximate loopy belief propagation, natural evolution strategies and random projections.

Weak AI - Weak AI are programs that learn how to perform one predictive or search task very well. They can be exceedingly good at it. Any AI is certainly a collection of weak AI algorithms.

Narrow AI - A synonym for weak AI. A better label.

Machine Learning These days, it&amp;#39;s very difficult to tell apart machine learning from Narrow AI. But a good rule of thumb is any algorithm derived from statistics, optimization or control theory put to use in the service of an AI system, with an emphasis of predictive accuracy instead of statistical soundness.

GOFAI - This stands for Good old fashion AI&amp;#39;s, expert systems and symbolic reasoners that many in the 1970 and 80s thought would lead to AI as flexible as a human. Led to the popular misconception that AIs must be perfectly logical. Another popular misconception is that GOFAI was a wasted effort. This is certainly incorrect. GOFAI led to languages like lisp, prolog, haskell. Influenced databases like datalog, rules engines and even SQL. Knowledge graph style structures underlie many of the higher order abilities of &amp;#39;Assistant&amp;#39; technologies like Siri, Google now, Alexa, Cortana, and Wolfram Alpha.

Furthermore, descendants are found in answer set programming, SMT solvers and the like that are used for secure software/hardware and verification. An incredible amount of value was generated from the detritus of those failed goals. Which should tell us how far they sought to reach. Something else interesting about symbolic reasoners is that they are the only AI based system capable of handling long complex chains of reasoning easily (neither deep learning nor even humans are exceptions to this).

True AI - This is a rarely used term that is usually synonymous with AGI but sometimes means Turing Test passing AI.

Artificial General Intelligence - This is an AI that is at least as general and flexible as a human. Sometimes used to refer to Artificial Super Intelligence.

Artificial Super Intelligence - This is the subset of AGI that are assumed to have broader and more precise capabilities than humans.

Strong AI - This has multiple meanings. Some people use it as a synonym for True AI, AGI or ASI. But others insist, near as I can tell, only biological based systems can be strong AIs. But we can alter this definition to be fairer: any AGI that also maximizes thermodynamic efficiency by maximizing energetic and memory use efficiency of prediction.

AI An ambiguous and broad term. Can refer to AGI, ASI, True AI, Turing Test passing AI, AI or Clippy. Depending on the person, their mood and the weather. Ostensibly, it&amp;#39;s just the use of math and algorithms to do filtering, prediction, inference and efficient search.

Natural Language Processing/Understanding The use of machine learning and supposedly  linguistics to try and convert the implicit structure in text to an explicitly structured representation. These days no one really pays attention to linguistics, which is not necessarily a good thing. For example, NLP people spend a lot more time on dependency parsing when constituency parsers better match human language use.

Anyways, considering the amount of embedded structure in text, it is stubbornly hard to get results that are any better than doing the dumbest thing you can think of. On second thought, this is probably due to how much structure there is in language on one hand and how flexible it is on another. For example, simply averaging word vectors with some minor corrections does almost as well and sometimes generalizes better than using a whiz bang Recurrent Neural Net. The state of NLP, in particular the difficulty of extracting anything remotely close to meaining, is the strongest indicator that Artificial General Intelligence is not near. Do not be fooled by PR and artificial tests, the systems remain as brittle to edge cases as ever. Real systems are high dimensional. As such, they are mostly edge cases.

Deep Learning These days, used as a stand in for machine learning even though it is a subset of it. Such a labeling is as useful as answering &amp;quot;what are you eating&amp;quot; with &amp;quot;food&amp;quot;. DL, using neural networks, is the representation of computer programs as a series of tables of numbers (each layer of a Neural Network is a matrix). A vector is transformed by multiplying it with a matrix and applying another function to each element. A favored function is one that clamps all negative numbers to zero and results in piecewise function approximation. Each layer learns a more holistic representation based on the layers previous, until the final layer can be a really dumb linear regressor performing nontrivial separations.

The learned transformations often represent conditional probability distributions. Learning occurs by calculating derivatives of our function and adjusting parameters to do better against a loss function. Seeking the (locally) optimal model within model space. Newer Neural networks explicitly model latent/hidden/internal variables and are as such, even closer to the variational approach mentioned above.

Speaking of latent variables, there is an unfortunate trend of obsession about the clarity of model generated images. Yet quality of generation does not necessarily equate with quality of representation. And quality of representation is what matters. Consider humans, the majority of our vision is peripheral (we get around this by saccading and joining small sections together). Ruth Rosenholtz has shown a good model of peripheral vision is of capturing summary statistics. Although people complain that the visual quality of variational autoencoder is poor due to fuzziness, their outputs are not so far from models of peripheral vision.



https://pdfs.semanticscholar.org/cb87/dfabb88f37114c4ca6c64ff938ae32f00c74.pdf



The obsession is even more questionable when we consider that internal higher order representations have lost nearly all but the most important core required for visual information. Lossy Compression is lazy = good = energy efficient. Considering their clear variational motivation and the connection to the Information Bottleneck Principle, I feel it a bit unfortunate that work on VAEs has dropped so, in favor of Adversarial Networks.



[1] Page: 67 | John Markoff, Machines of Loving Grace: The Quest for Common Ground Between Humans and Robots

[2] Page: 40 | 1511.02476.pdf

[3] http://rspa.royalsocietypublishing.org/content/royprsa/150/871/552.full.pdf

[4] Page 1 | WellerEtAl_uai14.pdf

[5] Page: 9 | Information and Efficiency in the Nervous System

[6] Page: 4 | 1203.3271v3



Path traced image

http://madebyevan.com/webgl-path-tracing/</description>
    </item>
    <item>
      <title>A High Level Introduction to Differential Privacy and Information</title>
      <link>http://www.metarecursive.com/writings/A_High_Level_Introduction_to_Differential_Privacy_and_Information.html</link>
      <guid>http://www.metarecursive.com/writings/A_High_Level_Introduction_to_Differential_Privacy_and_Information.html</guid>
      <pubDate>Wed, 27 Sep 2017 17:46:56 GMT</pubDate>
      <description>I&amp;#39;ve known of the term differential privacy for a long while, but only vaguely. Recently, I decided to look a bit more into the topic and also thought it a good place to start/try out interactive explanations. As it turns out, differential privacy is essentially about probabilities and information, which means an excuse to experiment with interactive explanations of relevant areas from probability theory (and an excuse to play with a discrete probability monad).

Of course, there is a great deal more to the subject of differential privacy than I can cover (or looked into) but I think I am satisfied with this as providing a decent high level overview.

One early precursor to DP is the method of randomized response. Proposed by S. L. Warner in 1965 [1], it&amp;#39;s a method of confidentially surveying a population.

Suppose you were surveying the population about something controversial and wanted to do so in a manner allowing plausible deniability. You could use the following procedure:

Flip a coin, if it&amp;#39;s heads, the responder must answer truthfully and if it&amp;#39;s tails they must answer yes. Note that this leaks some information about the responder: if they answer no then you know that they definitely have not performed said action. If they answer, yes, however, you have no way (at that instance in time) of distinguishing between whether it was truthful or the result of a coin flip. But across the entire population, since you control the procedure, you can work backwards to get the true distribution.

Suppose for example, you were surveying individuals about whether they love or hate bacon&amp;#129363;. They flip a coin and if it&amp;#39;s heads they answer truthfully. If it&amp;#39;s tails, they must say they Hate Bacon. Using this procedure, the surveyed number of those that love bacon is always ~half the true number in the population. This is because, for bacon loving responses, all the results are true but only get reached half the time. And for bacon hating answers (the protected class), half the time, the answers were truthful while the other half were I love bacon answers converted to I hate the consumption of bacon answers.

In the example below, you can adjust the slider to see how the surveyed numbers change.

True Proportion of Population that Hates Bacon:

In the below, assume p is the true proportion that hates bacon. Then:

p = 

Like Bacon: 0.5 * (1 - ) = 

Hate Bacon: 0.5 + 0.5 *   = 

True Against: 2 * ((q=) - 0.5) = 

Which you can subtract from 1 to get the proportion that enjoys bacon. If none of this makes sense, play with the slider and it should start to.

Something to note is that if some (ahem, barbaric) human says they love bacon, you definitely know they are speaking the truth (the End Bacon Now controversial but clearly more appropriate true belief is protected). Suppose we wanted to adjust this to be more anonymous?

Differential Privacy was initially expanded upon and given a solid mathematical footing by the prolific computer scientist/cryptographer Cynthia Dwork. It is a large field so we&amp;#39;ll only be taking a broad overview of it.

In the example for this section, we&amp;#39;ll be surveying people about their favorite sandwich. To keep things simple we&amp;#39;ll assume the true preferences of sandwiches are:

Best Sandwich

Share of Favorites

Hotdog &amp;#127789;

10%

Sandwich &amp;#129366;

30%

Vegan Hamburger&amp;#127828;

60%

How to tally votes without risking shame or ridicule for your belief that hotdogs are the best sandwich? A simple modification of randomized response allows for this. This time we don&amp;#39;t demand a specific answer--if the coin lands heads you speak truthfully but if it lands on tails, you sample uniformly (choose randomly) from among the choices. We can also allow the coin to be loaded or weighted. For example, we can use a coin that comes up heads 1% of the time. As long as we are only interested in population level things, despite the high levels of randomization, we can fully recover the original proportions.

With some algebra, I was able to work out that computing the following, for each possible answer recovers the true underlying percentages:
  \[p_{true} = \frac{p_{survey} - \frac{1}{|C|}(1 - p_{heads})}{p_{heads}}\]

Where |C| stands for total number of choices in the set C = {choice1,..,choicen}. This time, the slider will control how biased our coin is.

Coin Bias: 
   40%
  

Coin Bias: 
   40%
  

Differential Privacy is not an impenetrable seal of protection; it is possible to introduce leaks. Two ways that I could think of are attacks involving remembering queries and by asking multiple correlated questions.

If the queries do not retain any data on what each individual response was, privacy remains protected. If instead the responses were recorded, the collector can revisit the data to make new inferences. For example, suppose we were surveying whether people were for or against some action and that against is the protected class. After the population estimates of the proportions have been worked out, one can condition to just those who said against and work out the probability that those who said against truly are against.

In our randomized response scenario, if the proportion of the population that is against is 41%, the probability that those who answered against truly are against is ~59%. With the second differential privacy method, if it were 36% against at the population level, then those responding against are truly against with a 63% chance. This is a large change in probability! However, if a biased coin was instead used, say one that turns up tails 95% of the time, the worst case scenario would only involve going from 49% to 51%. The population level true values are still as precise but the individuals are much more protected.

The amount of information leaked depends on the underlying population probability and increases from zero and then decreases. Here&amp;#39;s a graph for the randomized response scenario:

As you can see, if the purpose is to secure the privacy of individual responses, then retaining the data of responses is subideal, especially when 30%-60% of the populace is against. If the results are to be retained, we can at least demand a high bias or a low probability of requiring a truthful response (most differential privacy work is biased towards the concerns of the data collector so they might not agree with my suggestion).

Another manner where which the implementer can cheat is by retaining responses and querying with either the same or a very similar set of questions. If the survey giver keeps asking the same questions, they can get ever more confident as to the true value of the responses. But that is not the only way to act in bad faith. If the survey process constructs different questions whose responses are correlated, they can become fairly certain about true answers in just two queries (or the first if enough different questions are asked).

In our final scenario, we will visit a world of dogs, mice and cats ruled by fat cats. The Fat Cats are performing what is ostensibly a demographic survey. To respect the right to anonymity of the denizens, they tell everyone they&amp;#39;re implementing differential privacy. Those extra few probing questions? To provide better services, they say. In actuality, they want to figure out who to increase insurance premiums for (you see, dogs are much too playful and mice keep getting injured by cats).

We will take the perspective of a single animal being queried. In addition to &amp;quot;which species are you&amp;quot;, we will also ask: what is your favorite food (fish, meat or cheese) and what is your favorite toy (bone, yarn,egg carton or cardboard box)? There is a predictor, a bayesian, that doesn&amp;#39;t get to see our selected species. We simulate it asking questions each time the button is pressed (you can also think of it as different phrasings each time). The Fat Cats are sly and their coin is slightly rigged--52% of the time it requires truth and 48% of the time allows randomization. Directly below (for comparison) we will also simulate our change in predicted species from asking the same single question of are you a dog or cat or mouse? a number of times equal to button presses.

Click the query button to see how our bayesian changes its confidence in its predictions.

Select your species:
  
    cat&amp;#128008;
    dog&amp;#128054;
    mouse&amp;#128045;
  

Relationship between questions:


  Correlated | 
  Independent

Times asked: 1
     | 

If you try reset a few times you might notice that mice are hardest to guess (since each of their non-food tastes share a bit with one of the other species). You might also notice that a surprising number of times, the correlator guesses correctly in one try (but can, especially for mice, fixate on the wrong species).

In this article we&amp;#39;ve covered only a small part of Differential Privacy, there remain many more sophisticated methods to inject noise and protect the user. Nonetheless, we were able to explore its core aspect. While Differential Privacy is a way to maintain privacy when data must be collected, it&amp;#39;s no panacea. If there&amp;#39;s no way to audit the process, an element of trust will always be needed. A hostile actor might technically be offering privacy but by retaining answers, using weights biased against the user, multiple identical queries (an issue in the digital world where devices can act on your behalf without your knowing the details) or designing queries so as to leverage correlations much more information than naively assumed can be leaked. All that said, properly implemented Differential Privacy strikes a balance between the needs of the user and the polling entity.

The following section is a bit more technical and assumes programming knowledge.

The likelihoods for the bayesians were chosen lazily. In particular, the correlator&amp;#39;s likelihood is not even properly sensible: it simply uses the joint probability of seeing those particular items together and so is very jumpy. Works well enough for this demonstration&amp;#39;s purposes though.

For the multiple asker:

I&amp;#39;d originally wanted to cover mutual information in the main text but realized I could not do it at the level of detail I preferred and so moved it here. Mutual information is an important concept, it&amp;#39;s definition is:

\[\sum_{(x,y)\in X \times Y} p(x,y) \log\left(\frac{p(x,y)}{p(x)p(y)}\right)\]

When X and Y are independent we get log(1) = 0. But a more motivated definition is: I(X;Y) = H(X) - H(X|Y) where H(X) stands for the entropy or our uncertainty about the random variable X. Mutual information then is, how uncertain we remain about X given that we know Y. If X and Y are independent of each other then knowing Y changes nothing about our uncertainty around X. But when they are correlated, knowing one tells us something and reduces our uncertainty about the other. In our Differential Privacy example, positive mutual information between the subjects of our questions allows us to narrow down and reduce our uncertainty about attributes that in principle, should have been private.

Entropy is roughly, our uncertainty about possible outcomes. We want this concept to be low when the bulk of probability is concentrated on a few outcomes and high when it&amp;#39;s diffuse. For a binary proposition, this is a function that&amp;#39;s low for low probability and high probability events (i.e. 1% means we are very certain this thing will not happen). Additionally, we want this uncertainty to change smoothly with changes in probability and to not depend on the order in which the probabilities are presented. Finally and most importantly, is the notion of coarse graining or throwing away detail (going from a shiny green toy car to a toy car).

If we have that the entropy at the coarse grained level is equal to that of our full level of detail minus the branching distinctions we don&amp;#39;t care about, there is essentially only one form entropy can take. That is, the entropy of our coarse graining should be less than or equal to that of the fine grained level. It is less exhausting to communicate at a high level than to finely go over pedantic distinctions (programming can be tedious).

If we have a set {a,b,c}, sets A = {a,b}, B = {c} and we want a function H:Distribution -&amp;gt; Real such that H({a,b,c}) = H({A,B}) + (pAH({A}={a,b}) + pBH({B} = {c})), the function which solves this is (for discrete systems): -Σx(p(x) * log p(x)). In code:

Why this is has been derived in many places but to boost intuition I will expand upon our simple example. Let&amp;#39;s define:

This filters our space to matching conditions and ensures the probabilities sum to one. Let&amp;#39;s encode our example.

Notice that A has an 80% chance of occurring and B, a 20% chance. The entropy of our full system is ~1.48 bits and our coarse system is 0.72 bits. In code our above requirement is:



The coarse graining requirements says the fine grained entropy should equal our coarse graining and the entropy of each of the contained subsystems weighted by their probability. This makes sense but a lack of appreciation for this can lead people astray when attempting to define emergence.

We can now move on to conditional entropy. Conditioning effectively means filtering to some condition as we showed above. Thus, the conditional entropy is, given we&amp;#39;ve filtered to some subspace that matches our condition, what is the entropy of that subspace?

projectWith is a function we pass in to select on a tuple. For example, if we have (a,b,c) then projectWith = third will give us c Our conditional entropy is then conditioning on each possible value that the projected variable can take on, calculating the entropy of that space and then multiplying it by the probability of the current value. It&amp;#39;s the average entropy from conditioning our random variable of focus across possible values of another random variable. We can now define:

An explicit example always helps. We define the below joint distribution based on the animal example above and visualize its joint distribution:

Relative entropy or KL Divergence (DKL(P||Q)) is a measure of how well one distribution codes another distribution. Or the loss (in bits) incurred from using the wrong distribution to code for another distribution. If DKL(Posterior||Prior) represents the change in our distribution after updating with new information, then it can be viewed as surprise. Indeed, the notion probably well matches the colloquial use of the term meaningful information when constrained to state changes the agent experiences in practice.

DKL(P||Q) is related to mutual information. I(X;Y) = DKL(P(X,Y)||P(X)P(Y)). Their forms in our discrete space are fairly similar:

Although there are derivations for why entropy should be as it is, they require an uncommon level of mathematical sophistication. The following might be a bit more helpful for a start as an intuition builder.

For a start, we will look at addressing items in terms of indexing some of their properties. Suppose you had 4 things and wanted to label them. One way to do this would be to number them: e.g. Items #0-#3. If we were to use the base 2 numbering system instead of base 10, we would have the added advantage that our labelling system could also be looked at in terms of addressing each item with respect to whether it possessed or not some property. For example:

Imagine our labels as the base 2 numbers: #00, #01, #10 and #11.

Is it green? Then Item #1 OR Is it Blue? Then Item #2 OR Is It Green AND Blue? Then Item #3. Else It must be Item #0.

In terms of base 2, with 3 things, we would need at most 2^2 = 4 labels. With 10 things we would need at most 2^4 = 16 labels. In general, N will be ≤ 2 ^(ceil log2 N), where ceiling rounds up always (5.2 -&amp;gt; 6 or 5.8 -&amp;gt; 6). Essentially, all we&amp;#39;re doing is computing the maximum power 2 needs to be raised to in order to be greater than or equal to N. If we have b=ceil log2 N we can simply say N items require no more than b properties to address or distinguish between them. In our example, that was checking whether green or blue to distinguish between 4 items.

We can also look at it in terms of asking yes/no questions (this is gone over in clear detail in [3]). This creates a (balanced?) binary tree. If we have N items, we can address or look them up in the tree using no more than log2 N steps. Imagine playing a guessing game; even if you had to choose between a billion numbers, it would take no more than 30 guesses if you kept slicing the possibilities in half. For our questions, the rare and more surprising items will be deeper in the tree.

Intuitively, things that occur rarely should be more surprising and so we should devote more space or attention to them. This can be viewed as difficulty distinguishing that item and requiring many questions to be confident about it. If we have each split at 0.5 probability then each node at depth d will have 1/2depth or 2-depth reachability probability (if there is more than one path to the node, we take their sum).

Suppose we have an object that can be in {A,B} and &amp;#39;B&amp;#39; has probability 12.5%, then we should devote -log2(0.125) or 3 bits of uncertainty space (technical term is surprisal) to it. Meanwhile &amp;#39;A&amp;#39;, with p = 87.5%, gets about 0.19 bits of surprisal...not as much. Entropy is our (weighted) average surprisal over possible states (0.54 bits for this example). For a high entropy (uniform) situation, I imagine a deep tree with many nodes at the bottom, each having an equal probability of being reached.



You can most easily play with the probabilistic programming code by copy and pasting code sections in https://gist.github.com/sir-deenicus/d8183d73ed12c2aa7d57f621c8a99ad1 into http://fable.io/repl/

[1] https://en.wikipedia.org/wiki/Randomized_response

[2] https://en.wikipedia.org/wiki/Differential_privacy

[3] http://tuvalu.santafe.edu/~simon/it.pdf</description>
    </item>
    <item>
      <title>Colloquial Information, Meaning and Fluffy Puff</title>
      <link>http://www.metarecursive.com/writings/Colloquial_Information,_Meaning_and_Fluffy_Puff.htm</link>
      <guid>http://www.metarecursive.com/writings/Colloquial_Information,_Meaning_and_Fluffy_Puff.htm</guid>
      <pubDate>Fri, 29 Sep 2017 09:08:11 GMT</pubDate>
      <description>The information in information theory does not capture the concept of meaning. The intuitive notion of information can nonetheless be written using information theory concepts. Information in the everyday sense has some properties it must satisfy. Consider a book.

1) If it is written in a language I do not understand then the book has zero information for me. This tells me that meaningful information ought to have a subjective aspect.

2) If I already know everything in the book then the book has no new information for me.

3) Even if I know the language a book is written in, I need some relevant knowledge before I can understand it.

With some thought, I arrived at the following as a general but computable algorithmic definition of useful information:

UsefulInformation I K = if can_decode(I) then relative_entropy(K, patch K (delta_decode K (decode I))) else 0

decode takes a symbol and returns a complex inner representation in language. delta_decode takes knowledge and the linguistic data and returns a distribution over interpretations in terms of K of the linguistic data.Patch computes a program for integration of the information. Relative entropy compares how much the distribution (or state of knowledge) has changed.

I&amp;#39;ve mixed and matched corresponding computer science and math concepts such that presentation is clearest (at least it was for me). Just as compression and entropy are related, data differencing and relative entropy are also related. The compression analogue of Patch corresponds to decompression. Patch, relative_entropy, and delta_decode are really different ways of talking about the same thing but their focuses differ. Patch emphasizes a change of state in knowledge, delta_decode emphasizes that it is not so much that new knowledge was gained as the fact that the relatively compressed information has been &amp;quot;relatively decompressed&amp;quot;. Relative entropy means we care mostly about comparing our old state to this new state.

Let&amp;#39;s expand. K here stands for the probability distribution which represents the reader&amp;#39;s state of knowledge. In the brain, it might be a collection of neurons as an ensemble code. In an AI, it might be a factor graph—it doesn&amp;#39;t matter—what matters is that one can straightforwardly represent state of knowledge in terms of a complex distribution.

Can Decode means that the sequence of symbols can be understood. In animals it means more than visual modules are activated, and in a non-trivial way, specifically attention is engaged and conscious state is modified. Again, the details of decoding do not matter, only that decoding the symbols affects internal state such that complex associations are triggered. In short, you speak the language the text is written in. Decoding takes the sequence and converts it to a rich inner representation.

The inner part: delta_decode K I is most interesting (remember, K stands for your state of knowledge). It captures requirement (3). Effectively, it puts forward that the information in books is actually compressed, but relative to some shared or assumed state of knowledge. Take a genome. You can take an arbitrary genome. Then for each new genome, instead of storing it or even compressing it, you can simply tell how it differs from the reference genome. Someone can then patch that genome to get to your genome. Instead of transferring gigabytes of data you can transfer a handful of megabytes. Or if you have ten pictures that are mostly the same then you can take one as a reference and describe the others based on how they differ from it. Doing this well is difficult but that&amp;#39;s irrelevant to the main point here. Which is that the information in text is not self-contained. Relevant knowledge is required in order for understanding to even be attempted.

This is seen most in math, where you cant make much progress because there is a huge amount of relative compression per symbol (This is actually common everywhere. Jump in the middle of a soap opera and you will have no clue what you are looking at but math requires much more active maintenance of fine grained information).

Conjecture: people&amp;#39;s brains barely differ in ability . Instead it&amp;#39;s the ability to do decode then patch which depend on attention, that is difficult. Couple this with the multiplicative nature of knowledge (the more you know the easier it is to decode then patch) and we see the seemingly wide gaps of reality. Things which engage attention are conscious activities and despite not using any more energy than watching TV, learning something difficult strongly dominates attention resources and the best our subconscious can do is map it to the feeling of a strenuous activity (when it is certainly not, metabolically).

Observation: this is why there is a sweet spot in how well people learn new information. The inner decode must be feasible but the change in state (the outer most relative entropy) needs to be noticeable. Each repetition can be viewed as getting better and better at delta decoding information similar to the given input.

The outer patch: patch K (delta_decode K I) might actually be better considered as solving a transport problem.

The text is essentially a specification and tells or constrains you in how to best modify your knowledge to arrive at the new state. Because there might be different solutions, people don&amp;#39;t arrive at the same new state. Most people will be blocked by proper delta decoding but once you&amp;#39;ve properly expanded you must then modify your state. Although I use patch, it&amp;#39;s better thought of as effectively searching for an explicit solution to something similar to a transport problem, getting from state probability distribution K to K&amp;#39;.

Of course, simply computing the conversion between two distributions cannot be the whole story. There&amp;#39;s a further step to be considered.  First, it seems reasonable to assume that not all of knowledge is available per instance of data. The earlier mentioned association cascade from decoding (relative and linguistic) are likely a subset of what is known to the agent. So what does patch do? Unfortunately I cannot say, but I can sketch at two possibilities. Searching across parameter values and testing with:

\[\textbf{Divergence} (\textbf{Decompress} (\textbf{Compress} (\ I_{decoded},  K_{sub})), \ I_{decoded})\]

This says the knowledge is tested as the discrepancy between the decompressed compression with respect to the current activated knowledge and the external instance. For example, in learning a new topic, the ability to generate a fuller description from a simpler set of facts and comparing it with what is recorded displays a fuller understanding. More concretely, if someone were to say: &amp;quot;there is a haggard man with a flower pot for a hat peering furtively from behind a rosemary shrub&amp;quot; , you might instead relay that as &amp;quot;there is a strange man hiding behind that weird cactusy plant&amp;quot;. The telephone game as a distortion from decompression errors.

The other aspect can be illustrated with an example. Suppose I say: John is in the kitchen. Your state of knowledge is updated but the new instance is isolated. But there are associated with it all manner of hypothesis as to John&amp;#39;s intention, next action, time of day and so on. The knowledge is backed both by a set of relations over actions and a large number of hypotheses. If I next say, John Closed the cupboard. The hypotheses as to what was probably just done and the previous actions are now slightly more precise. John&amp;#39;s Cat mewed impatiently.  And so on.

Compression is a sign of having understood, especially for larger and more complex information but there seems an aspect of a constantly modifying probabilistic model. Better metacognition would mean tracking uncertainty better. How compression is handled and the hypotheses are set, how the action state graph is tuned, is as of now, unknown to me beyond that each is an important aspect of how we learn and predict the things in the world.

The relative entropy between the new and old is the information gain and represents meaningful information. It is subjective but can be approximated objectively by averaging over all valid (exclude rocks but allow simple AI) readers of the book.

Watching a movie involves decoding the visual, audio and symbolics of the viewed scenes. Hypotheses are maintained about each actor&amp;#39;s mental state. Knowledge of the world, genre and the story itself inform how to constrain the consumer&amp;#39;s hypothesis space. Updates are usually small but a good story might have surprises (higher relative entropy in the local state—with respect to the story) that are easily coded with respect to the knowledge gained from the story so far. A poor movie is not well coded and &amp;quot;random&amp;quot;.  However, if the correct prior expectations are set, and there are some interesting long ranging correlations, this random can instead be viewed as funny.

Math mostly taxes relative decoding. Getting good at this is what some call mathematical maturity. Lots of new base knowledge items must be added  to our knowledge graph but they  mostly form their own clusters. The proper decoding and access is likely taxing on attention and recall. Being able to search long enough to find representations that are well suited to the older powerful modules is why some people are better at math.

Aside I say knowledge graph but one can more flexibly state it as a space, many projections and relation operators. Simple to flexible programs could be built in terms of these. It&amp;#39;s not as popular now but Pragmatic Reasoning Schemas in humans could be seen as a rough approximation to that kind of symbolic reasoning.

Physics is actually not difficult in the same sense as mathematics (conjecture). I argue that most of its difficulty is with the patching aspect. It requires competition with an existing, older physics model and requires many, long ranging non-local modifications to be made.

Teaching or writing involves holding a model of the student and attempting to select a shared code that minimizes rate distortion in the student&amp;#39;s decoding and maximizes correct patching probability. Bad teachers simply use themselves as the model.

Elegance and Beauty first requires the ability to relatively decode. This is why experts and beginner&amp;#39;s rarely hook on the same thing as elegant. Elegance is likely something that is easily compressed with minimal effort in terms of an existing code book, basis vectors or similar. This likely applies to many areas, from math to dance to aesthetic and beauty judgements and might explain some of why agreement strongly weights existing background.

A Uniformly Random String of Digits has high information or leaves you mostly uncertain but it is not informative. There is no relative decoding to do as the string is isolated. Updating too, is isolated and changes do not modify existing structures. If the individual were to incorporate this information there would be surprise but not much. If instead, the number was revealed as your credit card number, you can now code more readily. If the number was revealed to you by the man with a flower pot hat it is all the more surprising (rel entropy). However, you might still only compress the information as the man with the flower pot hat knows your credit card number and the card has a,b,c as the first three digits.

There is a further concept to point out, and that is attention. Attention tells you where to direct computational resources and what to focus on. Incorporating new atoms instead of coding in terms of existing is wasteful, when up keep is considered. Although this formulation does not consider attention, a random string of digits will have close to zero in state changes, due to attention mechanisms diverting away from coding and hence resulting in little meaningful information. More important concepts might also be difficult to code relatively until enough exposure and forced attention are directed towards acquisition. Beyond rel decoding and patching a large subspace, this is another sense in which a subject can be difficult. Conjecture Not only does attention mediate focus and so affects surprise together with expectation, it also affects what is retained by computing why should I store or learn this?

This can be extended to finding new research. We can leave out decode and relative entropy.  Instead we are doing recombination of basis with constraints such that Divergence from X to X + new combinations is minimal while satisfying various constraints both with respect to old knowledge and new knowledge.

How do we do restricted combinations?

How do we build constraints?

Veronica jumped over Fluffy Puff and into the pit of lima beans

Fluffy Puff swiped a paw and uprooted a tree.

A bonsai tree.

How big is Fluffy Puff?

In this essay I&amp;#39;ve put forward an algorithm that is a sketch for the colloquial notion of information. It suggests that information is subjective.  While the details are scant, the general form is possibly on the right track—not just for information but also meaning. The method can be used to make predictions and explain some things about not just humans but learning in intelligent agents in general. The lines of thought have even inspired me to come up with some simple algorithms that are surprisingly useful. Nonetheless, this seems all rather obvious, not much more than a restatement in a slightly more formal way. Even given that, it has been useful in compacting a lot of previously disconnected ideas—after having thought up the framework, many things fit neatly within it. Perhaps others might find it interesting too.</description>
    </item>
    <item>
      <title>In which I use watching a movie to explain Bayesian Probability. </title>
      <link>http://www.metarecursive.com/writings/In_which_I_use_watching_a_movie_to_explain_Bayesian_Probability._.htm</link>
      <guid>http://www.metarecursive.com/writings/In_which_I_use_watching_a_movie_to_explain_Bayesian_Probability._.htm</guid>
      <pubDate>Fri, 13 Oct 2017 10:51:15 GMT</pubDate>
      <description>(10/13/2017: ~4.5 years ago, I wrote this article to explain bayesianism without invoking any math. Bayes Theorem is not particular to bayesianism although it is true that the rule underpins the technique when you zoom out far enough. But the essence of what bayesian computation consists of has, in my opinion, never been properly explicated at a high level. Looking back, I think I did well but for a few misunderstandings I have corrected in parenthesized italics).

Friday, February 22, 2013

Subjective (as of now, I consider the notion of subjective to be superfluous) Bayesian Probability is a very powerful but partially impractical philosophy about the most sensible way to handle information. It was basically invented by this guy called Laplace. Quite often, when people explain Bayesian probability, the focus of their efforts gets tripped up because of the name. So they end up spending way too much time talking about Bayes Theorem or Rule. Which is actually not at all special to Bayesian Probability.

The proper way to view Bayesian probability is as the idea that everything can be assigned a probability (this is not the best frame, it&amp;#39;s much better to think of it as repeated conditioning on a space/collection of hypotheses. That is, slicing away and eliminating falsified hypotheses as well as up or down weighting supported hypotheses or not, as approrpriate). In this way we are all natural Bayesians since a typical response would go like, &amp;#39;eh? you mean you can&amp;#39;t assign probabilities to everything?&amp;#39; You can&amp;#39;t but people act this way anyways; talking about likely, unlikely, betting and percent chance. You can think of it as subsuming logic. Where each proposition has a value between 0 and 1, with false as 0 and 1 as true. Then I can ask you if you think you&amp;#39;re at risk of being eaten by bigfoot and you could give me an answer between yes and no. Bayesian Probability is also naturally extensible to quantum mechanics and makes a lot of (but not all) things less counterintuitive. But how to understand it?

Well consider a movie. We can view a particular aspect of watching a movie as a game of prediction. The fun of the game is to try and get as close a guess to the movie&amp;#39;s final outcome as possible, while the purpose of the director is to try and surprise as much as possible. This surprise is a measure of how not boring the movie is. So you come into the movie with your past experiences, the title, poster and maybe trailers. You&amp;#39;ve got a bunch of ideas of what will happen. As each scene unfolds you get more data and you lower the score of some guesses, while giving other guesses more weight. Each guess is a hypothesis and its weight is how likely you think it is, i.e. its probability. You can then think of the whole bunch of guesses with weights as your prior distribution (the purpose of the prior is to keep you grounded, rooting future predictions to be consistent with past knowledge). As you get more data or scenes you need to do an update of your beliefs.

That update is where the famous Bayes rule comes in. You can think of it as rebalancing your guesses with a hard to beat sanity check. Where, you have how likely each guess is (prior), check if the scenes so far match the guess (likelihood) (I would rephrase that as how likely each guess rates the scene by assuming itself true, the guesses that didn&amp;#39;t do well get pushed down) and then balance it with how much sense the movie makes now (probability of data (more specifically, think of it as after slicing some things away, squishing others and inflating the rest, you need to rebalance the existing hypotheses with respect to the new space so they remain as probabilities)). What you get out is a posterior distribution. As it turns out, we are not consciously good at being Bayesians (this isn&amp;#39;t quite accurate. It&amp;#39;s true we rely on brittle heuristics to make up for the small scratch space but being bayesian is intractable anyways). We don&amp;#39;t do the update step properly. But our unconscious mind can approximate it very well (the truth is more that the natural world is highly structured and hierarchical, therefore forgiving of many hacks). This is good because you are not consciously guessing what will happen, it just happens in the background. So people as doing bayesian movie watching is a pretty good description. There&amp;#39;s one more concept to talk about. Nonparametricity. It sounds complicated but you can think of it (roughly) like this. When you start the movie you don&amp;#39;t have all possible scenarios already figured out, as you get more data/scenes you grow the size and complexity of your hypothesis space/bag.

Now, at the end of the movie one hypothesis ends up as true. If you did well then the difference between your final posterior distribution and your prior before that—where you ended up concentrating most of the weight—should be minimal. This is a typical movie where you are not very surprised. However, good directors and writers have a handful of choices to trip you up with.

Directors and writers can overwhelm you with information or use non intuitive methods such as playing the film backwards or jumping around scenes or write a script with lots of possible scenarios. So you are not able to incorporate data properly and your distribution goes out of whack.

They can downplay certain cues so you don&amp;#39;t give certain guesses the proper weight till near the end. If they do it right, you would hold the guess in your bag of guesses but not count it for much and if they do it wrong they will just outright give an unjustified conclusion that was not at all in your prior. This is often upsetting and out of nowhere. A Movie can also be so nonsensical that you reject it because of the probability of the data. Maybe there is another semantic or logical meta-layer (this is hierarchical bayes which is necessary in the real world—note the probabilistic programming baysian community also and IMO confusingly calls nested parameter dependencies hierarchical) that looks at the consistency of the movie, compares it to experience and says: wtf m8?

Another option is to come up with an original story that is totally out of sync with your priors that your updates don&amp;#39;t end up converging/on target in time for the movie end. These are those super rare &amp;quot;make you think movies&amp;quot;. A typical good movie is one which surprises you but ends up with an explanation that you did not overly discount and with a collection of scenes which are consistent with their internal logic.

Now, there is a controversy between Bayesians and non-Bayesians. Which basically reduces to dissatisfaction that a great deal of the time, a Director can come up with a final story so completely outside your prior that you never converge despite it being completely consistent and with you updating all the data properly. You can only ever have imperfect coverage over all possibilities (There is a more common but less defensible series of arguments I omitted. A lot of people like to argue about objective vs subjective priors. This makes no sense since objective priors merely hide subjectivity. There is also a misapprehension leading to a belief that non-informative priors are superior. In reality, use of informative priors is vital to get learning going and to guard against conspiracy theories).

Another problem is that it is very hard to compute. So it must be estimated. In reality, a respectable Bayesian will use non-Bayesian methods as sanity checks or guides. Beware of the Rabid Rationalist Zealots.

An aside: our brains optimized on a particular tract that did well in its proper environment but now shine as &amp;quot;biases&amp;quot;. Speaking of which. This example also gives somewhat of an idea of a basic model of evolution. Replace guesses with particular versions of genes (alleles), likelihood as number of organisms with that gene (I don&amp;#39;t like this phrasing. Since likelihood reweights things, I&amp;#39;d amend it as a process that yields the new proportion of each gene in organisms that survived to reproduction) and surprise as number of deaths. Indeed Bayesian updating is just a special case of evolutionary learning (which fuels my belief that methods like Holland&amp;#39;s deserve more study (To clarify: it&amp;#39;s general in the sense of being less strict on what constitutes an update, i.e. you can do more involved things than simoply conditioning on observations. You can mutate and recombine hypotheses and not be strict about things adding up to or being probabilities—not as strict on normalizing in order to understand what is going on, local.).

(Another criticism is that the prior has to match the data generation process but this is true for any learning method—every learner equips structural priors which must match the data regularities or do no better than random. Sanity checks, testing and noting the lack of separability between the likelihood and prior acknowledge the limitations of bayes in reality. Bayesianism, or more usefully, probabilistic programming is ideal as it&amp;#39;s a simple way to reason about uncertainty, regularize, package hypothesis and condition on new information. It&amp;#39;s preferable in exactly the same way that a statically typed functional programming is preferable to C (there are less ways to shoot yourself in the foot and more ways to guard against simple but impactful mistakes, despite not being a panacea. Probabilistic programming also has the advantage of extensibility, generalizability and modularization between inference procedure and problem declearation. Furthermore, a bayesian system that can generate hypotheses online, that is not limited to some fixed set of hypotheses and has some &amp;quot;fast learning&amp;quot; trap door, and that learns in a hierarchical manner is probably the most powerful learner this universe can host.)

And, taking this further you can look at evolution as a type of intelligence1. The universe learning about itself doing inference and making predictions about its own rules. Life as a computation trying to figure out what combination of elements lead to the most stable and (entropy exporting) persistent structures. Why, If I was being poetic I would say Life is the universe thinking about itself (recursively self modeling—Good Regulator invokes self-similarity!).

And there you have it. I&amp;#39;ve taken some liberties but I think this is a comprehensible but still representative explanation.

Deen Abiola</description>
    </item>
    <item>
      <title>Fields as Curried Functions</title>
      <link>http://www.metarecursive.com/writings/Fields_as_Curried_Functions.html</link>
      <guid>http://www.metarecursive.com/writings/Fields_as_Curried_Functions.html</guid>
      <pubDate>Sat, 07 Apr 2018 06:21:29 GMT</pubDate>
      <description>Wednesday, January 13, 2016

In a 1994 paper, Gary T Leavens explained, in a wonderfully graspable manner, that Fields in physics can actually be viewed as curried functions. If you understand what curried functions are, then you already have a good idea of what fields are&amp;amp;#8212;these sort of bridges are incredibly helpful to those of us peering in from both sides of the divide (for me, from the CS side). The examples were in scheme and so the syntax inversion might make it a bit difficult for the unpracticed (such as myself) to follow, hence my translating (and hopefully, helpful to you too). My examples here are in F#, where units of measure are especially helpful in making things clearer.

First, some definitions:

This is a curried function since it&amp;#39;s a function that takes a value and returns a function specialized to the passed in value.

Returning to our function grav_force, when we put it into F# we got the type:

val grav_force &amp;amp;#8712; m1&amp;amp;#8712;float&amp;amp;lt;kg&amp;amp;gt; &amp;amp;#10233; r&amp;amp;#8712;float&amp;amp;lt;m&amp;amp;gt; &amp;amp;#10233; m2&amp;amp;#8712;float&amp;amp;lt;kg&amp;amp;gt; &amp;amp;#10233; float&amp;amp;lt;N&amp;amp;gt;

So everything looks good, it takes a mass (such as Earth) and returns a function that takes a distance (e.g. at the surface) which itself returns a function: one from masses (your mass) to Forces (your weight). This function is in fact the definition of a (scalar) gravitational field but more on that later. We can specialize the function and get earth&amp;#39;s field.

Similarly, we can specialize to the surface (by applying a distance to earth_grav) or we can compute the mass of an object at a particular distance by passing in a distance and a mass. For example, noticing that N = kg m/s&amp;amp;#178;, we can compute Earth&amp;#39;s accelaration at surface simply by passing in 1kg and the Earth&amp;#39;s radius for a value of 9.802877992 N (which is just m/s&amp;amp;#178; for this case). But we can do more interesting things with curried functions. We can flip the field equation to compute the gravitational force at a list of distances from earth&amp;#39;s surface (a bit more flexible than how physics is typically taught isn&amp;#39;t it):

The map applies our function to each distance in the list to get a list of forces. So at a distance of 1000km, the gravitational acceleration is ~7.32 m/sec&amp;amp;#178;.

Below you can look at the gravitational acceleration on the surface of the moon and Mars. The moon has a mere acceleration of 1.62 m/s&amp;amp;#178; and Mars is only just over 2x that&amp;amp;#8212;it is for this reason that many are concerned over the health effects (on skeletomuscular integrity) of an extended mission to Mars.

Additionally, we can look at the Earth/Moon system. The Moon and the Earth both exert a force of $2 \times10^{20}\,N$ on each other. Both are forever falling towards each other, with the condition that&amp;amp;#8212;on the odd chance that&amp;amp;#8212;if ever they should meet, things would not end well for both of them. A tragic love story if ever there was one.

To make things a bit more concrete than a list of numbers, I graph some commonly known locations and the gravitational acceleration felt there. Looking at these, I can&amp;#39;t help but think of the common conception (held by myself for a long time too) that space is &amp;amp;quot;out there&amp;amp;quot; and far away; when really, space (LEO, at least) is literally within walking distance. Most satellites, the International Space Station included, aren&amp;#39;t floating in space; they&amp;#39;re still deep within Earth&amp;#39;s gravitational well!

There is an art, it says, or rather, a knack to flying. The knack lies in learning how to throw yourself at the ground and miss. Pick a nice day, [The Hitchhiker&amp;#39;s Guide to the Galaxy] suggests, and try it.

The first part is easy. All it requires is simply the ability to throw yourself forward with all your weight, and the willingness not to mind that it&amp;#39;s going to hurt.

That is, it&amp;#39;s going to hurt if you fail to miss the ground. Most people fail to miss the ground, and if they are really trying properly, the likelihood is that they will fail to miss it fairly hard.

Clearly, it is the second part, the missing, which presents the difficulties...&amp;amp;quot;

--Douglas Adams

Even basic physics has a lot of ideas that grind against intuition. For example, the idea that objects in motion will remain in motion, unless acted on by a force, is one. The idea that gravity and the concept of weight as we feel it in the everyday is fictious is another counter-intuitive notion. Orbits combine these two. Firstly, because of a relative lack of friction, a large horizontal velocity can last a long time (meanwhile, we are used to large velocities requiring a continuous application of force). An object can continue to miss the ground without any extra expenditure of energy. Together with the idea of weightlessness as the more natural concept for two objects interacting in gravitational field, orbits as falling becomes a bit clearer to grasp (as you fall you follow the curvature of the earth&amp;amp;#8212;it also helps to imagine it as an animation, frame by frame; going down the frame makes it look more like falling&amp;amp;#8212;almost like we&amp;#39;re unrolling the interaction through time).

At this point, I can&amp;#39;t help but note that I&amp;#39;ve veered a bit far afield. The original intention of this piece was to connect curried functions from functional programs to fields in physics and yet, here I am talking about how falling is flying with no pesky forces in the way. Nonetheless, I hope the above has been an effective demonstration of the advantage of a computational approach to learning topics commonly thought of as challenging (there&amp;#39;ll be more such demonstrations in the below). In reality, much of the difficulty is incidental instead of necessary: for the student, it is grappling with inconsistent and often unmotivated notation, as well as plain general unfamiliarity or counter-intuitiveness and for the teacher: the baggage of being wedded to centuries old tradition of how subjects must be taught. Much pointless complexity arises from the interaction of all those variables.

Right, on topic. The real world has more than just one dimension. Depending on whom you ask, it can be anywhere from 3 or 4 to 11 to 26 or more or less. Most real world (classical) problems settle on 3 however. And here, again we see the advantage of a computational approach. It takes only a few lines to generalize our methodology to vectors (and though it&amp;#39;s general to N-dimensions, only for 3&amp;amp;#8212;or 2 in a few places&amp;amp;#8212;does it really makes sense for the operations we&amp;#39;re performing).

The function is the same as before except that, instead of returning a single number, we now return a list of numbers, representing our force vector. The input for distance has also been replaced with a vector. Below, we apply random masses at different locations to get force vectors. Everything is working correctly.

The functional way of viewing fields is that they are curried functions; through partial application they can either return more specialized functions or with enough inputs, (e.g.) a vector of forces. For example, the common static field is a function that takes an object of some appropriate type (say, Coulombs or Kg) and returns a function that accepts position vectors which itself returns a function that takes objects (of the same type) and maps them to forces. So by applying (e.g.) a mass, M, to the gravitational field function we specialize to talking about the gravitational field around M. Further specifying a position vector allows us to talk about the force at that distance for various other masses.

In short, fields are really functions. And the way they are used makes them the same as using curried functions with partial application.

In quantum mechanics things known as observables can&amp;#39;t be avoided. They have an exact mathematical definition as self adjoint operators but for our purposes we can think of them as functions. This is tricky because whereas before, our field function simply took a vector of reals (such as $\mathbb{R}^3$) for position, in quantum mechanics, position is an operator and so is instead something like ($ \mathcal{S} \rightarrow \mathbb{R}$ ). Thinking it through, I realized that the most sensible notion of variables as functions are random variables! A quick search reveals that indeed, &amp;amp;quot;real-valued quantum random variables correspond to self-adjoint operators affiliated with $\mathcal{A}$ [a von Neumann algebra on operators over a Hilbert Space], as postulated in quantum mechanics&amp;amp;quot;. One can also apply the notion of Observables to classical mechanics, and there also, they are functions that smell like random variables. And so, measurement can be thought of as an evaluation and hence, computation. Working backwards and having everything fit this way is really nice.

Quantum Field Theory further muddies our picture because there, people talk about fields as if they were a real thing and particles as excitations in this field. If fields are actually curried functions with specializations obtained through partial application, what does it mean for a function to be excited? However, it&amp;#39;s worth remembering that at the bottom of it all, there is a computation that&amp;#39;s carried out; a fluctuating field suggests that there&amp;#39;s a computation unfolding&amp;amp;#8212;in other words, evaluating the field gives different values over time. This is also suggestive of another curiosity&amp;amp;#8212;fields in physics are actually two different things. There is the field as function and then there is the underlying computation that is eventually evaluated (or calculation or phenomenon*, the &amp;#39;real&amp;#39; thing).

In the classical world, measurement yields simple things like vectors, in the quantum realm we get (things whose square magnitude are) probability distributions. There exist computations whose outputs are also probability distributions: probabilistic programs. Putting all these together, when a physicist says fields are interacting or oscillating, we can think of a probabilistic program (on noncommuting random variables) whose inputs are curried functions and, in some sense, mutually recursive with other &amp;#39;fields&amp;#39;, all of which unfolds some computation (over time).

*If for whatever reason, you do not like the idea of a computable reality, at the least our only interface with it through testable predictions must be.


The graph above is a visualization of a portion of the Moon&amp;#39;s, Earth&amp;#39;s and Jupiter&amp;#39;s gravitational fields. 
It&amp;#39;s not the clearest but stacked like this, we can compare them by zooming in and rotating. 
The X axis is the distance from the surface, ranging from 0-100,000km. 
The Y&amp;#39;s are masses, ranging from 10kg - 100,000kg. 
And the Z axis consists of forces felt in Newtons. 
Graphed like this, we can sort of get an idea the shape of the different fields.

The graphing package I used (Elegans), didn&amp;#39;t make it 
obvious how to label or format axes and I didn&amp;#39;t want to waste time hunting for them. Apologies
for unclear labelling.

Looking at the types for a gravitational and electrostatic field makes it clear that both fields can be represented by a single function parameterized by some Constant! By looking at the types, the similarity between the two fields is really brought into focus (more so than the similarity between the equations IMO) because the ability to specify both fields with a single function really forces one to take notice of the equivalence.

The type tells us that essentially, a static field is &amp;#39;some_constant * &amp;#39;some_quantity&amp;amp;#178;/m&amp;amp;#178;, capturing the inverse square aspect of the relationship. We can then write the gravitational field in terms of static field and all the types work out and the function works as it should:

We can define electric fields in the same manner. From that, it&amp;#39;s clear that mass and charge are similar abstractions. The difference between the two fields is that 1) the applied constants have opposite signs and differ by some 20 orders of magnitude, 2) charges are like masses that are allowed to go negative. That opposite attracts is a mere fact of arithmetic, what is more fundamental is that charges are less restricted in that they can be negative.

At this point, it&amp;#39;s instructive to highlight an example of the unnecessary obtuseness that abounds in physics equations. Given some test charge q, and a set of discrete charges, the electric field felt by q is given by: $$F(r) = q \cdot k \sum\limits_{i=1}^n {q_i\frac{r-r_i}{|r-r_i|^3}}$$

The problem of course, is that the Equation does not make clear that the function $F$ should take a pair of variables: the charge and its vector. It&amp;#39;s assumed implicity, and thus one more thing to be confused about. The same function is given by efield_force where the type makes it clear that the field is parameterized by both a position vector and a charge.

In this essay, I looked at the connection between fields and currying in functional languages. I used as examples, fields as found in gravity and electrostatics to elucidate the concept with a couple simple visualizations. I also traced where this road leads when followed along through to quantum mechanics. I observed that in quantum field theory, particles can (be loosely) labeled as oscillations of a field, making the field concept more primary. Yet, in what sense is a function—which has been only partially specialized by application of a few parameters—fluctuating? The field concept must have a second meaning here, referring to the space over which the field function can be applied. A further complication is that the field is defined over functions so the issue remains, in what sense is a function fluctuating? It is not clear to me but, these functions corresponding to observables are analogous to random variables, which in turn index a structure, mapping it to some measurable space. A probability distribution can be generated from them and presumably, interactions and measurements must in some sense &amp;quot;sample&amp;quot; from our &amp;quot;random variable&amp;quot;.

Recalling that the fluctuations are periodic, we can dispense with particles to focus on &amp;quot;fields&amp;quot;, a function whose computational representation would have (recursive, because it persists) periodic state changes and defined over an appropriate space. The field would be defined over functions that yield distributions, reminiscent of a function which indexes into a space of probabilistic programs. Virtual particles would themselves be analogized by transitory state changes (in computing terms, imagine values which do not persist once the scope of a function is exited) across message passing persistent computations (our approximation of fields).

I&amp;#39;ve taken many liberties in drawing the analogies above and, beyond the field/curry link, and, while speculation on the nature of quantum fields are not exactly correct, I do think they inhabit a useful plane of accuracy somewhere between precise knowledge and lay intuition.</description>
    </item>
  </channel>
</rss>