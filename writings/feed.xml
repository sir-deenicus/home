<rss version="2.0">
  <title>Thoughts Feed</title>
  <link>http://www.metarecursive.com/writings</link>
  <description>Trying to get my head around some things</description>
  <language>en-us</language>
  <channel>
    <item>
      <title>The Current Best Hypothesis is that the Brain is Computable</title>
      <link>The_Current_Best_Hypothesis_is_that_the_Brain_is_Computable.htm</link>
      <guid>The_Current_Best_Hypothesis_is_that_the_Brain_is_Computable.htm</guid>
      <pubDate>Fri, 14 Aug 2015 00:00:00 GMT</pubDate>
      <description>&amp;lt;img src=&amp;quot;http://imgs.xkcd.com/comics/turing_test.png&amp;quot; alt=&amp;quot;turing test&amp;quot;&amp;gt;
There are many (most?) people who dispute the idea that the brain is computable—there is something different and special about the human brain, they say. It is not possible to dispute this for now, but my own stance is a basic one: You may be right that the brain is somehow magical but my position is simpler and, all things being equal, more likely to end up as the correct one.
The argument that the brain is not a machine broadly rests on three ideas: those who lean to science and say: something, something quantum or another (quantum gravity if you want to be really fancy), or those who think it something magical, such as possessing a soul. The final group simply argue that the brain is not computable.
It is not uncommon to see the argument put forward that the brain is not computable, that what computers do is mere mechanistic cranking of mathematical algorithms. This is true, but who&amp;#39;s to say the brain is also not doing this?
Occam&amp;#39;s razor, Bayesian smoothing and regularization are all tools to keep one from over-fitting the evidence and failing to generalize. They are not laws, but tools to help you minimize your regret—make the fewest learning mistakes—over time. They do not say your idea must be simple, only that it does not say more than is possible given the data. The idea that the brain is computable fits within this regime as the hypothesis that is the simplest fit to the data. Why?
I often hear the point made that since people once compared the brain to clockwork and steam engines—comparisons we now know to be false—what makes you think an equivalence (and not just analogy) with computers won&amp;#39;t show the same failing in time? Small aside: steam engines and the brain, thanks to the link between thermodynamics and information, is actually more interesting than what one might at first think.
Turing Machines are, unlike a clock, universal. They can emulate any machine or procedure that is &amp;quot;effectively calculable&amp;quot;. Our physical theories might use &amp;lt;em&amp;gt;crutches&amp;lt;/em&amp;gt; such as real numbers or infinities but are, at the end of the day, only testable using computable procedures and numbers. This is what sets Turing Machines apart: any testable quantitative theory about the universe we can expect to devise will be simulatable (given enough time) on a Turing Machine (note: this is not the same thing as The Church Turing Thesis, instead of placing the restriction on the universe as CT does, it places it on any testable theory that compresses data. That is, more than a map from observation to expected outcome).
Even for the case that some physical things like the brain cannot be computed, it is simpler to believe that whatever non-computability the brain exploits is not unique to the exact biochemical make up of brains.
Interestingly, Occam&amp;#39;s Razor applies here too, and my argument is short. Even if Souls are a property of the universe unexplainable by science, it is still simpler to believe that the pattern and arrangement of matter that ends up with things acquiring souls is not unique to a gelatin soup of fats and proteins. Something that thinks and acts as if it is conscious, is (in essence, I drop the extra requirement that the object must also be an organic human brain like thing). That, in a nutshell, is also Turing&amp;#39;s argument.
But what is fascinating is that computer science has made the idea of a soul a scientific and testable hypothesis. If we do build intelligence (and maybe some of them will be more intelligent than humans in every way measureable) and yet they never wake up or attain consciousness or anything resembling (that is, nothing ever passes for consistently conscious but humans), then this is very suggestive of something unique and special about human beings. Until then, that hypothesis is unnecessarily complex.
Quantum mechanics is the go to argument for people who want to appear scientific even while talking nonsense. However, it is possible that the brain does something that our current machines cannot.
It is overwhelmingly unlikely that the brain is a &amp;lt;strong&amp;gt;Quantum Computer&amp;lt;/strong&amp;gt;. What we know about quantum mechanics makes this highly unlikely considering how wet, noisy and hot the brain is. It is implausible that coherent and entangled states could remain in such a situation. Additionally, humans do poorly at things we expect Quantum Computers will be good at (things such as factoring, perceiving quantum interactions intuitively—simulating quantum evolution). In fact, regular Turing Machines already outpace us in many areas; we don&amp;#39;t focus as much on the fact that we&amp;#39;re terrible at deductive reasoning, arithmetic or enumerating the possibilities of a large search space; for those things, it did not take long for computers to surpass human ability.
But, suppose the brain was not quantum mechanical but still leveraged quantum mechanical artifacts for its functioning—artifacts unavailable to our machines—then it is possible that current efforts will not lead to AGI.
In a certain trivial sense everything is quantum mechanical in that an agent adhering to predictions based on the theory will be able to explain the world with the highest accuracy. Of course, with such a broad definition then even the computer you are currently reading this on is a Quantum one. Not at all a helpful distinction.
Yet there is also a non-trivial sense in which quantum effects can be leveraged. We see this with our current processors; part of the difficulty with getting higher speeds and lower power is that (amongst other reasons) quantum tunneling effects are getting in the way. Biological homing mechanisms and photosynthesis have also been implicated with taking advantage of quantum effects.
Evolution is extremely powerful at coming up with unexpected uses to subtle phenomenon. Consider the following, from a fascinating &amp;lt;a href=&amp;quot;http://cs.nyu.edu/courses/fall11/CSCI-GA.2965-001/geneticalgex&amp;quot;&amp;gt;article&amp;lt;/a&amp;gt;:
A program is a sequence of logic instructions that the computer applies to the 1s and 0s as they pass through its circuitry.  So the evolution that is driven by genetic algorithms happens only in the virtual world of a programming language. What would happen, Thompson asked, if it were possible to strip away the digital constraints and apply evolution directly to the hardware?  Would evolution be able to exploit all the electronic properties of silicon components in the same way that it has exploited the biochemical structures of the organic world?
In order to ensure that his circuit came up with a unique result, Thompson deliberately left a clock out of the primordial soup of components from which the circuit evolved.  Of course, a clock could have evolved. The simplest would probably be a &amp;quot;ring oscillator&amp;quot;-—a circle of cells that change their output every time a signal passes through.
But Thompson reckoned that a ring oscillator was unlikely to evolve because only 100 cells were available.  So how did evolution do it—and without a clock? When he looked at the final circuit, Thompson found the input signal routed through a complex assortment of feedback loops.  He believes that these probably create modified and time-delayed versions of the signal that interfere with the original signal in a way that enables the circuit to discriminate between the two tones. &amp;quot;But really, I don&amp;#39;t have the faintest idea how it works,&amp;quot; he says.  One thing is certain: the FPGA is working in an analogue manner.
Up until the final version, the circuits were producing analogue waveforms, not the neat digital outputs of 0 volts and 5 volts.  Thompson says the feedback loops in the final circuit are unlikely to sustain the 0 and 1 logic levels of a digital circuit. &amp;quot;Evolution has been free to explore the full repertoire of behaviours available from the silicon resources,&amp;quot; says Thompson.
Although the configuration program specified tasks for all 100 cells, it transpired that only 32 were essential to the circuit&amp;#39;s operation.  Thompson could bypass the other cells without affecting it. A further five cells appeared to serve no logical purpose at all—there was no route of connections by which they could influence the output.  And yet if he disconnected them, the circuit stopped working. It appears that evolution made use of some physical property of these cells—possibly a capacitive effect or electromagnetic inductance—to influence a signal passing nearby.  Somehow, it seized on this subtle effect and incorporated it into the solution.
But how well would that design travel?  To test this, Thompson downloaded the fittest configuration program onto another 10 by 10 array on the FPGA. The resulting circuit was unreliable. Another challenge is to make the circuit work over a wide temperature range. On this score, the human digital scheme proves its worth.  Conventional microprocessors typically work between -20 0C and 80 0C. Thompson&amp;#39;s evolved circuit only works over a 10 0C range—the temperature range in the laboratory during the experiment.  This is probably because the temperature changes the capacitance, resistance or some other property of the circuit&amp;#39;s components.
Although this is the result of a genetic algorithm, a similarity with its natural counterpart is found: the exploitation of subtle effects and specificity to the environment it was evolved within. The article shows us two things: how evolution is not bounded by man&amp;#39;s windowed creativity but also, that, even if our current designs do not leverage some subtle effect while brains do, there&amp;#39;s no reason why we could not build a process that searches over hardware to leverage similar powerful processes. The search could be more guided; instead of random mutations, we have something else that is learning via reinforcement what actions to take for a given state of components and connections (we could have another suggesting components to inject freshness) then we select the best performing programs from the pool as the basis of the next round and appropriately reward the proposal generators.
Returning to the quantum, what, if there were something subtle about ion-channels or neuron vesicles, that allowed more powerful computation than one might expect. Perhaps something akin to a very noisy quantum annealing process is available to all animal brain&amp;#39;s optimization and problem solving processes? The advantage need not even be quantum it might even be that perhaps subtle electromagnetic effects or whatever are leveraged in a way that allows more efficient computation per unit time. This argument is one I&amp;#39;ve never seen made—yet, still, it consists of much extra speculation. Plausible though it is, I will only shift the weight of my hypotheses in that direction if we hit some insurmountable wall in our attempts to build thinking machines. For now, after seeing how very inherently mathematical the operations we perform with &amp;lt;a href=&amp;quot;http://www.iro.umontreal.ca/~memisevr/dlss2015/talk_Montreal_part2_pdf.pdf#page=28&amp;quot;&amp;gt;&amp;lt;em&amp;gt;our&amp;lt;/em&amp;gt; language are&amp;lt;/a&amp;gt; (some may dispute that this is cherry picking but that is irrelevant because the point is the fact that this is possible at all is highly suggestive and strongly favors moving away from skepticism and), it is premature to hold such (and other) needlessly complex hypotheses on the uniqueness of the human brain.
I have not argued against the soul or that the brain is incomputable or somehow special, instead I&amp;#39;ve argued that such hypotheses are unnecessary given what we know today. And even indirectly, when we look at history, we see one where assumptions of specialness have tended not to hold. The Earth is not the center of the universe, the speed of light is finite, simultaneity is undefined, what can be formally proven in any given theory is limited, a universal optimal learner is impossible, most things are computationally intractable, entropy is impossible to escape, most things are incomputable, most things are unlearnable (and not interesting), there is only a finite amount of information that can be stored within a particular volume (which is dependent on surface area and not volume), the universe is expanding, baryonic matter makes up only a fraction of the universe, earth like planets are common, some animals are capable of some mental feats that humans are not, the universe is fundamentally limited to being knowable by probabilistic means (this is not the same thing as the universe is non-deterministic)!
While one cannot directly draw any conclusions on the brain from these, when constructing our prior (beliefs) it perhaps behooves us to take these as evidence suggesting a weighting away from hypotheses reliant on exception and special clauses.</description>
    </item>
    <item>
      <title>Fields as Curried Functions</title>
      <link>Fields_as_Curried_Functions.html</link>
      <guid>Fields_as_Curried_Functions.html</guid>
      <pubDate>Fri, 01 Jan 2016 22:42:10 GMT</pubDate>
      <description>In a 1994 paper, Gary T Leavens explained, in a wonderfully graspable manner, that Fields in physics can actually be viewed as curried functions. If you understand what curried functions are, then you already have a good idea of what fields are&amp;amp;#8212;these sort of bridges are incredibly helpful to those of us peering in from both sides of the divide (for me, from the CS side). The examples were in scheme and so the syntax inversion might make it a bit difficult for the unpracticed (such as myself) to follow, hence my translating (and hopefully, helpful to you too). My examples here are in F#, where units of measure are especially helpful in making things clearer.
First, some definitions:
This is a curried function since it&amp;#39;s a function that takes a value and returns a function specialized to the passed in value.
Returning to our function &amp;lt;code&amp;gt;grav_force&amp;lt;/code&amp;gt;, when we put it into F# we got the type:
&amp;lt;code&amp;gt;val grav_force &amp;amp;#8712; m1&amp;amp;#8712;float&amp;amp;lt;kg&amp;amp;gt; &amp;amp;#10233; r&amp;amp;#8712;float&amp;amp;lt;m&amp;amp;gt; &amp;amp;#10233; m2&amp;amp;#8712;float&amp;amp;lt;kg&amp;amp;gt; &amp;amp;#10233; float&amp;amp;lt;N&amp;amp;gt;&amp;lt;/code&amp;gt;
So everything looks good, it takes a mass (such as Earth) and returns a function that takes a distance (e.g. at the surface) which itself returns a function: one from masses (your mass) to Forces (your weight). This function is in fact the definition of a (&amp;lt;em&amp;gt;scalar&amp;lt;/em&amp;gt;) gravitational field but more on that later. We can &amp;lt;em&amp;gt;specialize&amp;lt;/em&amp;gt; the function and get earth&amp;#39;s field.
Similarly, we can specialize to the surface (by applying a distance to &amp;lt;code&amp;gt;earth_grav&amp;lt;/code&amp;gt;) or we can compute the mass of an object at a particular distance by passing in a distance &amp;lt;em&amp;gt;and&amp;lt;/em&amp;gt; a mass. For example, noticing that &amp;lt;code&amp;gt;N = kg m/s&amp;amp;#178;&amp;lt;/code&amp;gt;, we can compute Earth&amp;#39;s accelaration at surface simply by passing in &amp;lt;em&amp;gt;1kg&amp;lt;/em&amp;gt; and the Earth&amp;#39;s radius for a value of 9.802877992 N (which is just m/s&amp;amp;#178; for this case). But we can do more interesting things with curried functions. We can flip the field equation to compute the gravitational force at a list of distances from earth&amp;#39;s surface (a bit more flexible than how physics is typically taught isn&amp;#39;t it):
The map applies our function to each distance in the list to get a list of forces. So at a distance of 1000km, the gravitational acceleration is ~7.32 m/sec&amp;amp;#178;.
Below you can look at the gravitational acceleration on the surface of the moon and Mars. The moon has a mere acceleration of 1.62 m/s&amp;amp;#178; and Mars is only just over 2x that&amp;amp;#8212;it is for this reason that many are concerned over the health effects (on skeletomuscular integrity) of an extended mission to Mars.
Additionally, we can look at the Earth/Moon system. The Moon and the Earth both exert a force of $2 \times10^{20}\,N$ on each other. Both are forever falling towards each other, with the condition that&amp;amp;#8212;on the odd chance that&amp;amp;#8212;if ever they should meet, things would not end well for both of them. A tragic love story if ever there was one.
To make things a bit more concrete than a list of numbers, I graph some commonly known locations and the gravitational acceleration felt there. Looking at these, I can&amp;#39;t help but think of the common conception (held by myself for a long time too) that space is &amp;amp;quot;out there&amp;amp;quot; and far away; when really, space (LEO, at least) is literally within walking distance. Most satellites, the International Space Station included, aren&amp;#39;t floating in space; they&amp;#39;re still deep within Earth&amp;#39;s gravitational well!
There is an art, it says, or rather, a knack to flying. The knack lies in learning how to throw yourself at the ground and miss. Pick a nice day, [The Hitchhiker&amp;#39;s Guide to the Galaxy] suggests, and try it.
The first part is easy. All it requires is simply the ability to throw yourself forward with all your weight, and the willingness not to mind that it&amp;#39;s going to hurt.
That is, it&amp;#39;s going to hurt if you fail to miss the ground. Most people fail to miss the ground, and if they are really trying properly, the likelihood is that they will fail to miss it fairly hard.
Clearly, it is the second part, the missing, which presents the difficulties...&amp;amp;quot;
--Douglas Adams
Even basic physics has a lot of ideas that grind against intuition. For example, the idea that objects in motion will remain in motion, unless acted on by a force, is one. The idea that gravity and the concept of weight as we feel it in the everyday is fictious is another counter-intuitive notion. Orbits combine these two. Firstly, because of a relative lack of friction, a large horizontal velocity can last a long time (meanwhile, we are used to large velocities requiring a continuous application of force). An object can continue to miss the ground without any extra expenditure of energy. Together with the idea of weightlessness as the more natural concept for two objects interacting in gravitational field, orbits as falling becomes a bit clearer to grasp (as you fall you follow the curvature of the earth&amp;amp;#8212;it also helps to imagine it as an animation, frame by frame; going down the frame makes it look more like falling&amp;amp;#8212;almost like we&amp;#39;re unrolling the interaction through time).
At this point, I can&amp;#39;t help but note that I&amp;#39;ve veered a bit far afield. The original intention of this piece was to connect curried functions from functional programs to fields in physics and yet, here I am talking about how falling is flying with no pesky forces in the way. Nonetheless, I hope the above has been an effective demonstration of the advantage of a computational approach to learning topics commonly thought of as challenging (there&amp;#39;ll be more such demonstrations in the below). In reality, much of the difficulty is incidental instead of necessary: for the student, it is grappling with inconsistent and often unmotivated notation, as well as plain general unfamiliarity or counter-intuitiveness and for the teacher: the baggage of being wedded to centuries old tradition of how subjects must be taught. Much pointless complexity arises from the interaction of all those variables.
Right, on topic. The real world has more than just one dimension. Depending on whom you ask, it can be anywhere from 3 or 4 to 11 to 26 or more or less. Most real world (classical) problems settle on 3 however. And here, again we see the advantage of a computational approach. It takes only a few lines to generalize our methodology to vectors (and though it&amp;#39;s general to N-dimensions, only for 3&amp;amp;#8212;or 2 in a few places&amp;amp;#8212;does it really makes sense for the operations we&amp;#39;re performing).
The function is the same as before except that, instead of returning a single number, we now return a list of numbers, representing our force vector. The input for distance has also been replaced with a vector. Below, we apply random masses at different locations to get force vectors. Everything is working correctly.
The functional way of viewing fields is that they are curried functions; through partial application they can either return more specialized functions or with enough inputs, (e.g.) a vector of forces. For example, the common static field is a function that takes an object of some appropriate type (say, Coulombs or Kg) and returns a function that accepts position vectors which itself returns a function that takes objects (of the same type) and maps them to forces. So by applying (e.g.) a mass, M, to the gravitational field function we specialize to talking about the gravitational field around M. Further specifying a position vector allows us to talk about the force at that distance for various other masses.
In short, fields are really functions. And the way they are used makes them the same as using curried functions with partial application.
In quantum mechanics things known as observables can&amp;#39;t be avoided. They have an exact mathematical definition as self adjoint operators but for our purposes we can think of them as functions. This is tricky because whereas before, our field function simply took a vector of reals (such as $\mathbb{R}^3$) for position, in quantum mechanics, position is an operator and so is instead something like ($ \mathcal{S} \rightarrow \mathbb{R}$ ). Thinking it through, I realized that the most sensible notion of variables as functions are random variables! A quick &amp;lt;a href=&amp;quot;https://www.encyclopediaofmath.org/index.php/Quantum_probability#Generalization&amp;quot;&amp;gt;search&amp;lt;/a&amp;gt; reveals that indeed, &amp;lt;em&amp;gt;&amp;amp;quot;real-valued quantum random variables correspond to self-adjoint operators affiliated with $\mathcal{A}$ [a von Neumann algebra on operators over a Hilbert Space], as postulated in quantum mechanics&amp;lt;/em&amp;gt;&amp;amp;quot;. One can also apply the notion of Observables to classical mechanics, and there also, they are functions that smell like random variables. And so, measurement can be thought of as an evaluation and hence, computation. Working backwards and having everything fit this way is really nice.
Quantum Field Theory further muddies our picture because there, people talk about fields as if they were a real thing and particles as excitations in this field. If fields are actually curried functions with specializations obtained through partial application, what does it mean for a function to be excited? However, it&amp;#39;s worth remembering that at the bottom of it all, there is a computation that&amp;#39;s carried out; a fluctuating field suggests that there&amp;#39;s a computation unfolding&amp;amp;#8212;in other words, evaluating the field gives different values over time. This is also suggestive of another curiosity&amp;amp;#8212;fields in physics are actually two different things. There is the field as function and then there is the underlying computation that is eventually evaluated (or calculation or phenomenon*, the &amp;#39;real&amp;#39; thing).
In the classical world, measurement yields simple things like vectors, in the quantum realm we get probability distributions. There exist computations whose outputs are also probability distributions: probabilistic programs. Putting all these together, when a physicist says fields are interacting or oscillating, we can think of a probabilistic program (on noncommuting random variables) whose inputs are curried functions and, in some sense, mutually recursive with other &amp;#39;fields&amp;#39;, all of which unfolds some computation (over time).
*&amp;lt;em&amp;gt;If for whatever reason, you do not like the idea of a computable reality, at the least our only interface with it through testable predictions must be.&amp;lt;/em&amp;gt;

The graph above is a visualization of a portion of the Moon&amp;#39;s, Earth&amp;#39;s and Jupiter&amp;#39;s gravitational fields. 
It&amp;#39;s not the clearest but stacked like this, we can compare them by zooming in and rotating. 
The &amp;lt;b&amp;gt;X&amp;lt;/b&amp;gt; axis is the distance from the surface, ranging from &amp;lt;b&amp;gt;0-100,000km&amp;lt;/b&amp;gt;. 
The &amp;lt;b&amp;gt;Y&amp;#39;s&amp;lt;/b&amp;gt; are masses, ranging from &amp;lt;b&amp;gt;10kg - 100,000kg&amp;lt;/b&amp;gt;. 
And the &amp;lt;b&amp;gt;Z&amp;lt;/b&amp;gt; axis consists of forces felt in &amp;lt;i&amp;gt;Newtons&amp;lt;/i&amp;gt;. 
Graphed like this, we can sort of get an idea the shape of the different fields.
The graphing package I used (Elegans), didn&amp;#39;t make it 
obvious how to label or format axes and I didn&amp;#39;t want to waste time hunting for them. Apologies
for unclear labelling.
Looking at the types for a gravitational and electrostatic field makes it clear that both fields can be represented by a single function parameterized by some &amp;lt;em&amp;gt;Constant&amp;lt;/em&amp;gt;! By looking at the types, the similarity between the two fields is really brought into focus (more so than the similarity between the equations IMO) because the ability to specify both fields with a single function really forces one to take notice of the equivalence.
The type tells us that essentially, a static field is &amp;lt;em&amp;gt;&amp;#39;some_constant * &amp;#39;some_quantity&amp;amp;#178;/m&amp;amp;#178;&amp;lt;/em&amp;gt;, capturing the inverse square aspect of the relationship. We can then write the gravitational field in terms of static field and all the types work out and the function works as it should:
We can define electric fields in the same manner. From that, it&amp;#39;s clear that mass and charge are similar abstractions. The difference between the two fields is that 1) the applied constants have opposite signs and differ by some 20 orders of magnitude, 2) charges are like masses that are allowed to go negative. That opposite attracts is a mere fact of arithmetic, what is more fundamental is that charges are less restricted in that they can be negative.
At this point, it&amp;#39;s instructive to highlight an example of the unnecessary obtuseness that abounds in physics equations. Given some test charge q, and a set of discrete charges, the electric field felt by q is given by: $$F(r) = q \cdot k \sum\limits_{i=1}^n {q_i\frac{r-r_i}{|r-r_i|^3}}$$
The problem of course, is that the Equation does not make clear that the function $F$ should take a pair of variables: the charge and its vector. It&amp;#39;s assumed implicity, and thus one more thing to be confused about. The same function is given by &amp;lt;code&amp;gt;efield_force&amp;lt;/code&amp;gt; where the type makes it clear that the field is parameterized by both a position vector &amp;lt;em&amp;gt;and&amp;lt;/em&amp;gt; a charge.
animate gravity
electric and grav of proton
negative mass
discrete probability
conjugate
information being
information geometry
oscillators,gliders, computation model not reality</description>
    </item>
    <item>
      <title>Colloquial Information, Meaning and Fluffy Puff</title>
      <link>Colloquial_Information,_Meaning_and_Fluffy_Puff.htm</link>
      <guid>Colloquial_Information,_Meaning_and_Fluffy_Puff.htm</guid>
      <pubDate>Mon, 01 May 2017 02:16:19 GMT</pubDate>
      <description>The information in information theory does not capture the concept of meaning. The intuitive notion of information can nonetheless be written using information theory concepts. Information in the everyday sense has some properties it must satisfy. Consider a book.
1) If it is written in a language I do not understand then the book has zero information for me. This tells me that meaningful information ought to have a subjective aspect.
2) If I already know everything in the book then the book has no new information for me.
3) Even if I know the language a book is written in, I need some relevant knowledge before I can understand it.
With some thought, I arrived at the following as a general but computable algorithmic definition of useful information:
&amp;lt;code&amp;gt;UsefulInformation I K = if can_decode(I) then relative_entropy(K, patch K (delta_decode K (decode I))) else 0&amp;lt;/code&amp;gt;
&amp;lt;code&amp;gt;decode&amp;lt;/code&amp;gt; takes a symbol and returns a complex inner representation in language. &amp;lt;code&amp;gt;delta_decode&amp;lt;/code&amp;gt; takes knowledge and the linguistic data and returns a distribution over interpretations in terms of K of the linguistic data.&amp;lt;code&amp;gt;Patch&amp;lt;/code&amp;gt; computes a program for integration of the information. Relative entropy compares how much the distribution (or state of knowledge) has changed.
I&amp;#39;ve mixed and matched corresponding computer science and math concepts such that presentation is clearest (at least it was for me). Just as compression and entropy are related, data differencing and relative entropy are also related. The compression analogue of Patch corresponds to decompression. Patch, relative_entropy, and delta_decode are really different ways of talking about the same thing but their focuses differ. Patch emphasizes a change of state in knowledge, delta_decode emphasizes that it is not so much that new knowledge was gained as the fact that the relatively compressed information has been &amp;quot;relatively decompressed&amp;quot;. Relative entropy means we care mostly about comparing our old state to this new state.
Let&amp;#39;s expand. K here stands for the probability distribution which represents the reader&amp;#39;s state of knowledge. In the brain, it might be a collection of neurons as an ensemble code. In an AI, it might be a factor graph—it doesn&amp;#39;t matter—what matters is that one can straightforwardly represent state of knowledge in terms of a complex distribution.
Can Decode means that the sequence of symbols can be understood. In animals it means more than visual modules are activated, and in a non-trivial way, specifically attention is engaged and conscious state is modified. Again, the details of decoding do not matter, only that decoding the symbols affects internal state such that complex associations are triggered. In short, you speak the language the text is written in. Decoding takes the sequence and converts it to a rich inner representation.
The inner part: &amp;lt;code&amp;gt;delta_decode K I&amp;lt;/code&amp;gt; is most interesting (remember, K stands for your state of knowledge). It captures requirement (3). Effectively, it puts forward that the information in books is actually compressed, but relative to some shared or assumed state of knowledge. Take a genome. You can take an arbitrary genome. Then for each new genome, instead of storing it or even compressing it, you can simply tell how it differs from the reference genome. Someone can then patch that genome to get to your genome. Instead of transferring gigabytes of data you can transfer a handful of megabytes. Or if you have ten pictures that are mostly the same then you can take one as a reference and describe the others based on how they differ from it. Doing this well is difficult but that&amp;#39;s irrelevant to the main point here. Which is that the information in text is not self-contained. Relevant knowledge is required in order for understanding to even be attempted.
This is seen most in math, where you cant make much progress because there is a huge amount of relative compression per symbol (This is actually common everywhere. Jump in the middle of a soap opera and you will have no clue what you are looking at but math requires much more active maintenance of fine grained information).
&amp;lt;em&amp;gt;Conjecture&amp;lt;/em&amp;gt;: people&amp;#39;s brains barely differ in ability . Instead it&amp;#39;s the ability to do decode then patch which depend on attention, that is difficult. Couple this with the multiplicative nature of knowledge (the more you know the easier it is to decode then patch) and we see the seemingly wide gaps of reality. Things which engage attention are conscious activities and despite not using any more energy than watching TV, learning something difficult strongly dominates attention resources and the best our subconscious can do is map it to the feeling of a strenuous activity (when it is certainly not, metabolically).
&amp;lt;em&amp;gt;Observation&amp;lt;/em&amp;gt;: this is why there is a sweet spot in how well people learn new information. The inner decode must be feasible but the change in state (the outer most relative entropy) needs to be noticeable. Each repetition can be viewed as getting better and better at delta decoding information similar to the given input.
The outer patch: &amp;lt;code&amp;gt;patch K (delta_decode K I)&amp;lt;/code&amp;gt; might actually be better considered as solving a transport problem.
The text is essentially a specification and tells or constrains you in how to best modify your knowledge to arrive at the new state. Because there might be different solutions, people don&amp;#39;t arrive at the same new state. Most people will be blocked by proper delta decoding but once you&amp;#39;ve properly expanded you must then modify your state. Although I use patch, it&amp;#39;s better thought of as effectively searching for an explicit solution to something similar to a transport problem, getting from state probability distribution K to K&amp;#39;.
Of course, simply computing the conversion between two distributions cannot be the whole story. There&amp;#39;s a further step to be considered.  First, it seems reasonable to assume that not all of knowledge is available per instance of data. The earlier mentioned association cascade from decoding (relative and linguistic) are likely a subset of what is known to the agent. So what does patch do? Unfortunately I cannot say, but I can sketch at two possibilities. Searching across parameter values and testing with:
&amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\[\textbf{Divergence} (\textbf{Decompress} (\textbf{Compress} (\ I_{decoded},  K_{sub})), \ I_{decoded})\]&amp;lt;/span&amp;gt;
This says the knowledge is tested as the discrepancy between the decompressed compression with respect to the current activated knowledge and the external instance. For example, in learning a new topic, the ability to generate a fuller description from a simpler set of facts and comparing it with what is recorded displays a fuller understanding. More concretely, if someone were to say: &amp;quot;&amp;lt;em&amp;gt;there is a haggard man with a flower pot for a hat peering furtively from behind a rosemary shrub&amp;lt;/em&amp;gt;&amp;quot; , you might instead relay that as &amp;quot;&amp;lt;em&amp;gt;there is a strange man hiding behind that weird cactusy plant&amp;lt;/em&amp;gt;&amp;quot;. The telephone game as a distortion from decompression errors.
The other aspect can be illustrated with an example. Suppose I say: &amp;lt;em&amp;gt;John is in the kitchen&amp;lt;/em&amp;gt;. Your state of knowledge is updated but the new instance is isolated. But there are associated with it all manner of hypothesis as to John&amp;#39;s intention, next action, time of day and so on. The knowledge is backed both by a set of relations over actions and a large number of hypotheses. If I next say, &amp;lt;em&amp;gt;John Closed the cupboard&amp;lt;/em&amp;gt;. The hypotheses as to what was probably just done and the previous actions are now slightly more precise. &amp;lt;em&amp;gt;John&amp;#39;s Cat mewed impatiently&amp;lt;/em&amp;gt;.  And so on.
Compression is a sign of having understood, especially for larger and more complex information but there seems an aspect of a constantly modifying probabilistic model. Better metacognition would mean tracking uncertainty better. How compression is handled and the hypotheses are set, how the action state graph is tuned, is as of now, unknown to me beyond that each is an important aspect of how we learn and predict the things in the world.
The relative entropy between the new and old is the information gain and represents meaningful information. It is subjective but can be approximated objectively by averaging over all valid (exclude rocks but allow simple AI) readers of the book.
&amp;lt;strong&amp;gt;Watching a movie&amp;lt;/strong&amp;gt; involves decoding the visual, audio and symbolics of the viewed scenes. Hypotheses are maintained about each actor&amp;#39;s mental state. Knowledge of the world, genre and the story itself inform how to constrain the consumer&amp;#39;s hypothesis space. Updates are usually small but a good story might have surprises (higher relative entropy in the local state—with respect to the story) that are easily coded with respect to the knowledge gained from the story so far. A poor movie is not well coded and &amp;quot;random&amp;quot;.  However, if the correct prior expectations are set, and there are some interesting long ranging correlations, this random can instead be viewed as funny.
&amp;lt;strong&amp;gt;Math&amp;lt;/strong&amp;gt; mostly taxes relative decoding. Getting good at this is what some call mathematical maturity. Lots of new base knowledge items must be added  to our knowledge graph but they  mostly form their own clusters. The proper decoding and access is likely taxing on attention and recall. Being able to search long enough to find representations that are well suited to the older powerful modules is why some people are better at math.
&amp;lt;em&amp;gt;Aside&amp;lt;/em&amp;gt; I say knowledge graph but one can more flexibly state it as a space, many projections and relation operators. Simple to flexible programs could be built in terms of these. It&amp;#39;s not as popular now but Pragmatic Reasoning Schemas in humans could be seen as a rough approximation to that kind of symbolic reasoning.
&amp;lt;strong&amp;gt;Physics&amp;lt;/strong&amp;gt; is actually not difficult in the same sense as mathematics (conjecture). I argue that most of its difficulty is with the patching aspect. It requires competition with an existing, older physics model and requires many, long ranging non-local modifications to be made.
&amp;lt;strong&amp;gt;Teaching&amp;lt;/strong&amp;gt; or writing involves holding a model of the student and attempting to select a shared code that minimizes rate distortion in the student&amp;#39;s decoding and maximizes correct patching probability. Bad teachers simply use themselves as the model.
&amp;lt;strong&amp;gt;Elegance and Beauty&amp;lt;/strong&amp;gt; first requires the ability to relatively decode. This is why experts and beginner&amp;#39;s rarely hook on the same thing as elegant. Elegance is likely something that is easily compressed with minimal effort in terms of an existing code book, basis vectors or similar. This likely applies to many areas, from math to dance to aesthetic and beauty judgements and might explain some of why agreement strongly weights existing background.
&amp;lt;strong&amp;gt;A Uniformly Random String of Digits&amp;lt;/strong&amp;gt; has high information or leaves you mostly uncertain but it is not informative. There is no relative decoding to do as the string is isolated. Updating too, is isolated and changes do not modify existing structures. If the individual were to incorporate this information there would be surprise but not much. If instead, the number was revealed as your credit card number, you can now code more readily. If the number was revealed to you by the man with a flower pot hat it is all the more surprising (rel entropy). However, you might still only compress the information as the man with the flower pot hat knows your credit card number and the card has a,b,c as the first three digits.
There is a further concept to point out, and that is attention. Attention tells you where to direct computational resources and what to focus on. Incorporating new atoms instead of coding in terms of existing is wasteful, when up keep is considered. Although this formulation does not consider attention, a random string of digits will have close to zero in state changes, due to attention mechanisms diverting away from coding and hence resulting in little meaningful information. More important concepts might also be difficult to code relatively until enough exposure and forced attention are directed towards acquisition. Beyond rel decoding and patching a large subspace, this is another sense in which a subject can be difficult. &amp;lt;em&amp;gt;Conjecture&amp;lt;/em&amp;gt; Not only does attention mediate focus and so affects surprise together with expectation, it also affects what is retained by computing why should I store or learn this?
This can be extended to finding &amp;lt;strong&amp;gt;new research&amp;lt;/strong&amp;gt;. We can leave out decode and relative entropy.  Instead we are doing recombination of basis with constraints such that Divergence from X to X + new combinations is minimal while satisfying various constraints both with respect to old knowledge and new knowledge.
How do we do restricted combinations?
How do we build constraints?
Veronica jumped over Fluffy Puff and into the pit of lima beans
Fluffy Puff swiped a paw and uprooted a tree.
A bonsai tree.
How big is Fluffy Puff?
In this essay I&amp;#39;ve put forward an algorithm that is a sketch for the colloquial notion of information. It suggests that information is subjective.  While the details are scant, the general form is possibly on the right track—not just for information but also meaning. The method can be used to make predictions and explain some things about not just humans but learning in intelligent agents in general. The lines of thought have even inspired me to come up with some simple algorithms that are surprisingly useful. Nonetheless, this seems all rather obvious, not much more than a restatement in a slightly more formal way. Even given that, it has been useful in compacting a lot of previously disconnected ideas—after having thought up the framework, many things fit neatly within it. Perhaps others might find it interesting too.</description>
    </item>
    <item>
      <title>Levels of Understanding</title>
      <link>Levels_of_Understanding.htm</link>
      <guid>Levels_of_Understanding.htm</guid>
      <pubDate>Thu, 04 May 2017 23:15:43 GMT</pubDate>
      <description>&amp;lt;img src=&amp;quot;Images/development.png&amp;quot; alt=&amp;quot;alt text&amp;quot;&amp;gt;
&amp;lt;a href=&amp;quot;http://www.freepik.com&amp;quot;&amp;gt;Freepik CC 3.0 BY&amp;lt;/a&amp;gt;
I believe one can usefully represent and capture levels of understanding with 3 levels. The 3 levels being: memorization, topic extraction and compression. The first level is the domain of computers—searching, indexing, clipping etc. Humanwise, I believe savants dominate memorization. And temporarily, though far less reliably, also the level most students achieve on tested subjects. Recently, this is also how most people operate online. &amp;#39;Cyborgs&amp;#39; stitching information snippets together without any deep comprehension. There are a lot of them on the internet.
Level 2 is topic extraction. This is the form of a lot of human knowledge. Where, you don&amp;#39;t remember quite what you read this but: &amp;#39;you know, it had to do with this or maybe it was that and...are you sure you didn&amp;#39;t read it too? Oh well, anyways it was interesting.&amp;#39;
Thanks to search tools like Google, you can often use these clues to quickly reconstruct the original information—with varying success rates (but much greater than before). In this way, we combine computers—which are good at memorization—with humans, who can extract topics, to augment recall. The combined system allows better and deeper recall than either system alone. At this point I&amp;#39;d like to emphasize that this storing of information outside the brain is nothing new. It is actually a core aspect of how the brain works.
For example, it is very common to forget something but then trivially remember it when at the same location you thought it up. Or leaving something by the door lest you forget—these triggers and associative recall are how the brain efficiently manages memory and shuffles priorities into and away from conscious action by outsourcing them into the environment. Search is but a continuation of a long tradition. It is not by any means making us &amp;#39;dumber&amp;#39; (what&amp;#39;s making us dumber is the increase in noise, from scientific research to blog posts, the majority are confusion). It is in fact helping the brain do what it has always done better than ever before.
There is still a lot of friction however; in that search, clipping etc. are still expensive and active processes. In addition, extracting relevant information from results is still a non-trivial expense. There&amp;#39;s a cost such that search is not always undertaken—how else can you explain people saying &amp;#39;IIRC, this is blah blah&amp;#39; when that information is readily available online? [footnote: to those of you who remember libraries, this is not about being spoilt but to emphasize that the shorter the time between thought and feedback, the greater the boon to cognition and that relationship is highly non-linear].
The most sophisticated level of understanding is compression. I conjecture understanding to be a direct search for first what combination of existing bases best represent some concept (&amp;lt;em&amp;gt;in other words, metaphor and analogy are key to reasoning&amp;lt;/em&amp;gt;, another corollary is that we never start from scratch there are some in built structures) which are then used to represent this new knowledge. Mastery is the ability to arbitrarily form linear combinations of the basis concepts in this new space. Importantly and in addition, the dictionary/basis set is not necessarily fixed—allowing additional and often superfluous (not linearly independent) vectors as necessary.  Hypergraphs or relations and projections,  I conjecture, best represents the link between different spaces. The spaces are likely fluid, contracting and expanding as needed, mapping between themselves and building structures (combination of new concepts) based on functions that worked in other spaces.
A corollary is, while learning new things expands the mind, understanding is a contraction of bases. It reduces dimensionality. Understanding can identify that two previously thought independent concepts are actually codependent and thus can be expressed in terms of some other more fundamental concepts/basis concepts. On the other hand, learning can require additional bases (optimization, search) to represent truly foreign concepts. This suggests that the bases are not truly orthogonal and understanding is a search for this.
Other than being a useful metaphor, there is real work in this direction. When you create a jpeg, it is in a sense, a very local &amp;#39;understanding&amp;#39; of the image—throwing away useless aspects to allow reconstruction. More complex neural net based image generators or compressors learn something of not just colors but also correlations that map to what we might call textures or sections of objects. The algorithms could be turned to learning what aspect of color is relevant for human vision, given an appropriate loss (note: correlations not abstractions, let&amp;#39;s leave abstractions as correlations converted to symbols used for unrelated reasoning by an intelligence).
Vision in the brain also works similarly, except the bases are overcomplete (using far larger dimensionalities than that of the signal), hence the vectors are sparse. There the learned bases are akin to Dictionaries and visual signals are sparsely coded with respect to this dictionary. While the mind is certainly more than a vector space, the fact that vision is so well modeled, and with how evolution tends to conserve and reuse, causes me to think of this model of understanding as key.
A common objection to AI is that it has not displayed true understanding, it&amp;#39;s just performing such and such mathematical function. In my view, the correct to response to this is: &amp;quot;why would it be any other way, for anything else&amp;quot;? The idea that happiness and anger might in fact have a mathematical description seems unacceptable to many. On the other hand, it does mean that emotions are algorithms that satisficed some optimality criterion and are not merely dismissible as irrationality or brokenness.
There is a sense however, in which the expressed disagreements about whether AI is yet creative can be seen as not wrong. Imagine an object trying to regulate itself. A dynamical system trying to maintain homeostasis against some non-static background must have (even a simple) model of that background.
In order for a prediction machine to operate effectively it needs to capture statistical properties of the world around it. Vision correlates with the outside world but should not be mistaken for anything other than an uncertain representation thereof. It is in this sense the statistical can be said to correlate with the true signal. This property can be very general, as can be found in pigeons able to learn to discriminate between words and non-words: &amp;lt;a href=&amp;quot;http://www.pnas.org/content/113/40/11272.full&amp;quot;&amp;gt;(see: Orthographic processing in pigeons (Columba livia)&amp;lt;/a&amp;gt;.
Consider a robot arm connected to a computer. The robot was trained on drawings and can now generate drawings it has never before seen. The robot arm was not programmed, it generates things that it was never trained on and is not random. The robot is like nothing we have ever seen before and while many are happy to call it creative, this does not sit well with most. I too was once happy to call such programs creative but recently stopped, upon arriving at a distinction. Ada Lovelace once eloquently stated:
The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths.
Our robot arm (buildable today), can do things we did not order it to perform and it can be said to originate. But it still has no power of anticipating analytical relations or truths because the arm cannot exit the manifold its learning algorithms had sought. Any movement too far leads to collapse and the generation of noise, because as the Lady Ada presaged, analytical relations cannot yet be anticipated.
A picture can be taken as a snapshot or projection of reality. The robot arm could similarly be considered a projection, echo or ghost of the drawing ability of the countless poorly paid souls which provided its training data. That robot is animate but not in the manner that a human is. Yet, as a projection of the thoughts of a thinking thing, it is something more than a mere picture or a recording.
When learning, humans seem somehow to be able to develop higher order conceptions of the nature of a search space. We learn not just an embedding or lower dimensional submanifold, as algorithms of today but also can detect non-trivial symmetries, invariances and even algebras. We likely operate on statistical manifolds, moving from model to model and somehow able to constrain movement in the space of models such that a surprising percentage of moves are fruitful. This might be why we are good at poker, chess and Go using far less resources at learning and at play.  Flexibility in adapting to underlying structure displays a level of understanding far beyond what any algorithm to date has achieved.
For example, we can notice that the weather and brains are both describable in terms of dynamical systems. Learning about physics of weather and unshared properties with the brain can serve to constrain hypotheses about the brain. Or consider constructing a general object and deducing many things about it. After which, you find some new thing, prove it has some property that makes it like the object you deduced many things about and instantly get knowledge about your new thing. Whether deduction or some sophisticated way of conditioning on a space, these kinds of long range connections are things we just barely have inklings of how to achieve.
Some promising directions today are gaussian processes and generative models that well consider complexity across a model class.
Another difference can be found when things mean something to us. Something that might feed into that is our ability to work not just off correlations and conditional distributions but also, in that we bind signals to percepts and then reason with them. We evolved in a world with actors where many things have causes and there is a need to model internal states of other actors. The object binding, together with a ratcheting of complexity in modelling and recursivity likely feeds into our notion of meaningful. Beyond compression, we are able to better appreciate the general structure of a search space as well as bind signals to more abstract/aggregated/usefully coarse grained and recursively modeled symbols.
It&amp;#39;s worth noting that humans are not perfect and suffer large limitations. Humans function best in rich spaces with lots of structure such as when signals have a hierarchical structure that is well decomposable. When the space is rich enough, this yields great results but human reasoning can really fall apart in more general domains. Nonetheless, it is in the above mentioned two very important senses ( 1. we can deduce things much beyond what we experience 2. when we create we can anticipate other agents in a rich way in terms of symbols and use that to guide our choices) that robots do not understand nor create.
The architecture of computers means they are better suited to certain kinds of computations and humans to another. We assume that because we are intelligent and conscious, that is the ideal form. But such is not necessarily true. Spiders and ants are very successful, without also being in danger of self-termination. A thing that was hyeprspecialized at say nuclear physics or genetics without being causally oriented or conscious would probably do more good than a conscious thing whose eventual greed we might have cause to be wary of.
The shared ancestor of humans and chimps had something very unique about its genetics. Chimps diverged and lost it but some developed it further down the hominid line; competed, merged and resulted in a type of consciousness known as human. There is no indication that throughout the long history of life on the planet, anything else like it ever developed. It might be that our type of self-modelling consciousness is a rare and difficult to arrive at modality.
Some might imagine there is some kind of hierarchy, that other AIs will be human like—capable of goals and self direction but it might just as well be that the space of intelligences is mostly one of highly specialized decidable compression algorithms that are really good at solving one particular class of problem.</description>
    </item>
    <item>
      <title>A bit on Inference and Intelligence</title>
      <link>A_bit_on_Inference_and_Intelligence.htm</link>
      <guid>A_bit_on_Inference_and_Intelligence.htm</guid>
      <pubDate>Tue, 13 Jun 2017 19:14:37 GMT</pubDate>
      <description>What does it mean for a system to be intelligent. Or Artificially so? The phrase Artificial intelligence is a contentious one, and for me, the cause of a not insignificant amount of frustration in discussions because everyone brings their own special meaning. However, looking closely at what it means to learn, it&amp;#39;s clear that the notion of intelligence is not one of what but instead of what on. What&amp;#39;s important and I believe often missed, is that there is no algorithm for intelligence, instead there are algorithms and whether they yield intelligence or not is task dependent.
Artificial intelligence is a broad term and according to Markoff [1], was coined by John McCarthy who wanted a name avoiding any association with Norbert Wiener&amp;#39;s &amp;lt;em&amp;gt;cybernetics&amp;lt;/em&amp;gt;. McCarthy also considered Shannon&amp;#39;s preferred term of automata as having too much of a mathematical framing. Of the terms from that era, I personally favor &amp;lt;em&amp;gt;complex information processing&amp;lt;/em&amp;gt;.
Until recently (circa 2005), Artificial Intelligence (AI) was a term to be avoided, in preference for machine learning, due to stigma from the overpromises and overreach of two decades prior. Today AI can mean anything from linear regression (basically just a drawn out addition and multiplication process) to &amp;lt;em&amp;gt;The Terminator&amp;lt;/em&amp;gt;. It&amp;#39;s a broad term and while I do not often view its use as wrong, there are usually more informative terms to use. Calling certain autocompletes AI, while not technically wrong, is in the same class of phrasing as referring to people as biological entities. This is true, people are biological entities but so are slime molds. Perhaps use a term with better specificity? Telling me there is a biological entity in front of me can require very different reactions depending on whether it&amp;#39;s a tree, person, spider or elephant. Or tiger.
One common objection is: &amp;quot;that&amp;#39;s not AI, that&amp;#39;s just counting&amp;quot; or, &amp;quot;that&amp;#39;s just algorithms&amp;quot;. There are two observations I can make about this. 1) The assumption that animals are not limited to running (computable) algorithms. This may be true but it would come with a lot of baggage (why can no one factor 9756123459891243 in their head?). Things that a human can do but a computer cannot are few (things that a human can do but a computer cannot do well are numerous). It would be very surprising to find out humans spend all their hypercomputing ability solely on emotions and being &amp;quot;conscious&amp;quot;. It&amp;#39;s too early to require such a stance.
Observation 2) is that the algorithm is not all that matters, it also matters where said algorithm is being applied. Consider the following two examples: path tracing and Bethe&amp;#39;s 1935 work on Statistical Theory of Superlattices.
&amp;lt;em&amp;gt;&amp;lt;/em&amp;gt;Path Tracing&amp;lt;em&amp;gt; &amp;lt;/em&amp;gt;is a way of computing lighting or illuminations so as to achieve a photorealistic rendering of a scene. At its most basic, a method known as importance sampling is used to estimate the integral of the equation that solves the light path calculations.  That same technique, when employed on data, yields a form of approximate bayesian inference. One might complain that I&amp;#39;m cheating, and really, only a way of approximating integrals is what&amp;#39;s shared. Furthermore, the algorithm is effectively making guesses on how the light likely behaved. But then again, that there should be such a close correspondence between efficient sampling and averaging of per pixel illuminance values and real photos is not obvious to me.
The link between learning and physics can be made even stronger when we look at how often bayesian inference and strategies developed by physicists to study systems with many interacting variables coincide (it&amp;#39;s no surprise then, that thermodynamics turns up often, but it is by no means the only example). Ideas that are now showing up as important in both neuroscience and machine learning derived from work, such as mean field theory, done by physicists in the early 90s and late 80s. The broader historical pattern is physicists got there first—whether studying phase transitions or whatever it is they do—and computer scientists studying inference get there a few decades later.
Take the case of inference on tree structured distributions, computing the marginal distribution over the nodes can be done exactly using a message passing algorithm by Judea Pearl known as belief propagation (invented in the early 1980s). It turns out that none other than physicist Hans Bethe had similar ideas when modeling ferromagnetism [3] in the early 1900s. From [2]:
Notice, within the historical perspective, that the Bethe solution for the spin glass on a random graph was written by Bowman and Levin [44], i.e. about 16 years before realizing that the same equation is called belief propagation and that it can be used as an iterative algorithm in a number of applications. The paper by Thouless [299], who analyzed the spin glass critical temperature, implicitly includes another very interesting algorithm, that can be summarized by equation (78) where BP is linearized. The fact that this equation has far-reaching implications when viewed as a basis for a spectral algorithm waited for its discovery even longer.
Here the connection is non-trivial. It&amp;#39;s not mere happenstance from using the same calculating tool but more fundamentally, the same core computation is in effect being performed. There&amp;#39;s a key link in that the fixed point of belief propagation corresponds to stationary points of the Bethe free energy [4]. Unfortunately, the places where belief propagation works are limited to sparse graphs and trees, so sampling based methods are often preferred for practical inference. These too, find their origin in physics (monte carlo, gibbs sampling, variational mean field) but of particular interest are the variational methods.
It is proven via computational complexity methods that bayesian inference is intractable (we can never build a computer powerful enough to handle interesting/medium-large problems exactly). In particular, the normalizing constant is hard to compute (fancy way of saying divide so that it sums to one), because the are too many possible settings of parameters to consider. As such, we must resort to approximations. The favored way to do this is to approximate the target (posterior) probability distribution with another less busy one that&amp;#39;s simpler to work with. It&amp;#39;s also required that this approximation and our target diverge as little as possible. To do this, we search over a family of distributions for a distribution whose parameters are such that we get the best approximation of the target distribution (minimizing this divergence directly is also difficult so what happens instead is yet another layer of indirect optimization). When the space of models we are searching over is effectively continuous, this means searching over a space of functions for a function, given our requirements, which best represents our target. Hence the name variational methods—which derives from work done by 19th century mathematicians seeking an alternative more flexible grounding for newtonian mechanics.
The Calculus of Variations was mostly (initially) developed by the Italian mathematician/mathematical physicist Lagrange and Swiss mathematician Euler. The calculus began in the study of mathematical problems involving finding minimal length paths (that must satisfy some condition, most famously, what path takes the shortest amount given that our point is following the constraints of gravity).
&amp;lt;img src=&amp;quot;Images/Brachistochrone.gif&amp;quot; alt=&amp;quot;Brachistochrone image By Robert ferr&amp;#233;ol&amp;quot;&amp;gt;
The general format is, given some function that accepts functions and returns a number (a higher order function or functional), find a function that is optimal for our functional (and goes through these two points, say). The solution, one of: a maximum, minimum or flat resting stop, is known as a stationary point. The method of calculating this derivative on functions (since derivatives are themselves higher order functions, the types must be gnarly!) involves an analogy with the regular case. The functional must be stable to minor perturbations or variations. A perturbation is how the functional output changes with minor variations instigated by another function. When this is done with respect to some detailed energy calculations, one gets the Stationarity Principle of physics.
In nature we find, things like actions and paths are often taking minimal length paths, the relatively well known principle of least action. Unfortunately, this is a bit of a misnomer since quantum mechanics tells us that what nature really has is a principle of most exhaustively wasteful effort. QM tells us that all paths are taken and we only seem to see one path because only the stationary values correspond to the values (complex numbers) where phases did not cancel. As to why that is, I do not know but if it were any other way, if nature had to actively choose and hence know which paths were stationary, I suspect we would live in a very different kind of universe. Could we build computers that could solve problems our universe sized ones couldn&amp;#39;t?
Nonetheless, the way mere humans have to calculate things is to seek out that critical point and to get that, we must use tools of optimization. These tools have turned out to be very general; so long as some internal structural requirements of the problem are satisfied, it can be used on that problem.
Parametric Polymorphic functions are algorithms that don&amp;#39;t care to look at the details of the thing they are operating on, so long as some structural requirements are met. For example, a function that gives a list of permutations doesn&amp;#39;t care if the list is of integers, movie titles or fruit baskets. The same reasoning can be used to partially explain why many ideas from physics are useful for building learning systems. Both physical objects and learning systems can be viewed as optimizing for some value, have a notion of a surface or manifold (for inference, space of models) that&amp;#39;s moved along, a notion of curvature (in inference this is indirectly related to correlation) and distance (symmetric divergence between probability distributions) and a requirement to efficiently move along these surfaces seeking minimal energy configurations.
&amp;lt;img src=&amp;quot;Images/saddle_point_evaluation_optimizers.gif&amp;quot; alt=&amp;quot;Gradient descent algorithms seeking a minimum, from Sebastian Ruder&amp;quot;&amp;gt;
In both, there is a need to search for a function that acts as a good minimizer given our requirements. An object trying to minimize some quantity efficiently has only so many options to make. The correct way to move efficiently on a manifold constrains what operations you can perform. These algorithms and strategies are agnostic to the internal details of the problem; put in energy and physical things you solve physics problems. Put in log probabilities and solve learning problems. However, learning algorithms did not have to do this and in fact they do not have to follow these constraints. It&amp;#39;s just that, if you wish to learn effectively, you&amp;#39;ll behave in a way parametrically equivalent to physical systems bound by a stationary principle. Physicists often arrived first because they had physical systems to study, observe and constrain methods. Computer scientists arrive later, at the same spot, asking, how do we do this more efficiently?
If this seems a bit undramatic, don&amp;#39;t be sad, as there is still a lot of mystery. The universe didn&amp;#39;t have to be this way. Phases didn&amp;#39;t have to cancel in such a way that we observe a seeming least action principle. Bayesian inference (and therefore all forms of learning, since they all reduce to approximating a probability distribution) did not have to be intractable. Or we could have lived in a universe with accessible infinities, for which computations difficult for us in our universe, were easy. But because of both these conditions, variational methods or similar have been important in getting correct results in both fields.
Nonetheless, it is a bit uncanny how often probabilistic inference on particular graphs structures often have a precise correspondence with a physical system. Consider, message passing to compute a marginal probability on one hand allows you to do inference and on something else, works out local magnetization. Why belief propagation and Beth Approximation ideas work as well as they do for computing a posterior probability distribution (a knowledge update) is not well known. Part of the uncanniness is of course, a lot of thermodynamics is also inference but that is less than a satisfactory answer. The boundary between entropy and information and as such, energy is very fuzzy for me. In a future essay I&amp;#39;ll look more into this but for now, I&amp;#39;d like to look at variational free energy and what it says about the brain&amp;#39;s computational ability.
Variational free energy is an informational concept (negative model log likelihood for the data) that slots into where (IIUC) Gibbs free energy would fit into for a physical system. In an inferential system with an internal world model, variational free energy does not tell you about the physical energy of the system. It instead tells you how well your internal model matches your sensory inputs or data inputs or historical signal inputs. You want to minimize any future discrepancies as much as possible but don&amp;#39;t want to drift too far from your prior experiences. The concept of variational free energy finds much use in machine learning and neuroscience. The better the model, the lesser the free energy on our variational approximation and the better we are doing. In other words, the less surprises we are having.
Here&amp;#39;s what&amp;#39;s key. As I point out above, variational methods are an approximation due to computational limitations. If we find that it is in fact true that the brain is also forced to optimize on some variational bounds and no better than a mere pauper Turing machine (resource bounded) , this too would be very suggestive that the brain itself is not just computable but bound by computational complexity laws!
The link between learning and thermodynamics goes deeper than simple correspondence however. In [5], Friston et al also point out how the brain will also operate to minimize its Helmholtz free energy by minimizing its complexity (giving less complex representations to highly probable states. You should not be surprised then, when we find expertise means less volume in highly trafficked areas or less energy use for mental processing of well understood things). Similarly, in the very interesting [6] Susanne Still shows that any non-equilibrium thermodynamic system being driven by an external system must alter its internal states such that there is a correspondence with the driving signal and coupling interface. This is necessary to minimize wasteful dissipation. Therefore, it must have something of a memory and state changes will implicitly predict future changes in order to operate efficiently, maintaining favorable current states. As such, efficient dynamics corresponds to efficient prediction.
Evidence for the importance of energetic efficiency is furthermore found in biomolecular machines that approach 100% efficiency when driven in a natural fashion: the stall torque for the F1-ATPase [26] and the stall force for Myosin V [27] are near the maximal values possible given the free en- ergy liberated by ATP hydrolysis and the sizes of their respective rotations and steps. These and many other biological functions require some correspondence between the environment and the systems that implement them. Therefore the memory of their instantiating systems must be nonzero. We have shown that any such system with nonzero memory must conduct predictive inference, at least implicitly, to ap- proach maximal energetic efficiency.
We thus arrive at an interesting separation. All systems we call alive (right down to bacteria) concern themselves with both variational and thermodynamic free energy but digital AIs only concern themselves with the variational concept.
In summary, those who reject certain algorithms as AI are making a fundamental mistake by assuming that the algorithm is what makes an AI. Instead, it&amp;#39;s where the algorithm is used that matters. A simple dot product (something no more complex than 5 * 3 + 6 * 2) in one condition might be a high school math problem or find use in lighting calculations in a graphics engine. In another context however, it might compare word vector representations of distilled co-occurrence statistics or encode faces in a primate. We should expect then, that an AGI or an ASI will consist of narrow AI joined together in some non-trivial fashion but still no different from math.
I additionally pointed out the correspondence between inference and physical systems is not so surprising when viewed as aspects of something more general, analogized with a polymorphic function. But it is nonetheless not obvious why things ended up as such. Computational complexity limits and energy limits turn up at the same places surprisingly often and demand similar dues of informational and physical systems.
The link goes even deeper when we realize that predictive efficiency and thermodynamic efficiency of non-equilibrium systems are inextricably linked. Not just that brains and predictive text autocomplete should count as performing inference but also, simple biological molecules. In fact, these systems might have gotten as complex as they did in order to be more effective at prediction, in order to more effectively use a positive free energy flow for say, replication or primitive metabolism.
I can now finally put forward a definition for what AI is. An AI is any algorithm that has been put to the task of computing a probability distribution for use in some downstream task (decisions, predictions), a filter which leverages the structure of what it is filtering, or performs a non-exhaustive search in some space. Autocomplete that enumerates alphabetically is not intelligent, Autocomplete that predicts what I might type next is. From the context of Intelligence amplification, an intelligent algorithm is any system that works cooperatively to reduce working memory load for the human partner.
In a future post I&amp;#39;ll look into what algorithms the brain might be running. This will involve synthesizing the following proposals (what they have in common, what they differ in and how plausible they are): Equilibrium Propagation, Sleeping experts, approximate loopy belief propagation, natural evolution strategies and random projections.
&amp;lt;strong&amp;gt;Weak AI&amp;lt;/strong&amp;gt; - Weak AI are programs that learn how to perform one predictive or search task very well. They can be exceedingly good at it. Any AI is certainly a collection of weak AI algorithms.
&amp;lt;strong&amp;gt;Narrow AI&amp;lt;/strong&amp;gt; - A synonym for weak AI. A better label.
&amp;lt;strong&amp;gt;Machine Learning&amp;lt;/strong&amp;gt; These days, it&amp;#39;s very difficult to tell apart machine learning from Narrow AI. But a good rule of thumb is any algorithm derived from statistics, optimization or control theory put to use in the service of an AI system, with an emphasis of predictive accuracy instead of statistical soundness.
&amp;lt;strong&amp;gt;GOFAI&amp;lt;/strong&amp;gt; - This stands for Good old fashion AI&amp;#39;s, expert systems and symbolic reasoners that many in the 1970 and 80s thought would lead to AI as flexible as a human. Led to the popular misconception that AIs must be perfectly logical. Another popular misconception is that GOFAI was a wasted effort. This is certainly incorrect. GOFAI led to languages like lisp, prolog, haskell. Influenced databases like datalog, rules engines and even SQL. Knowledge graph style structures underlie many of the higher order abilities of &amp;#39;Assistant&amp;#39; technologies like Siri, Google now, Alexa, Cortana, and Wolfram Alpha.
Furthermore, descendants are found in answer set programming, SMT solvers and the like that are used for secure software/hardware and verification. An incredible amount of value was generated from the detritus of those failed goals. Which should tell us how far they sought to reach. Something else interesting about symbolic reasoners is that they are the only AI based system capable of handling long complex chains of reasoning easily (neither deep learning nor even humans are exceptions to this).
&amp;lt;strong&amp;gt;True AI&amp;lt;/strong&amp;gt; - This is a rarely used term that is usually synonymous with AGI but sometimes means Turing Test passing AI.
&amp;lt;strong&amp;gt;Artificial General Intelligence&amp;lt;/strong&amp;gt; - This is an AI that is at least as general and flexible as a human. Sometimes used to refer to Artificial Super Intelligence.
&amp;lt;strong&amp;gt;Artificial Super Intelligence&amp;lt;/strong&amp;gt; - This is the subset of AGI that are assumed to have broader and more precise capabilities than humans.
&amp;lt;strong&amp;gt;Strong AI&amp;lt;/strong&amp;gt; - This has multiple meanings. Some people use it as a synonym for True AI, AGI or ASI. But others insist, near as I can tell, only biological based systems can be strong AIs. But we can alter this definition to be fairer: any AGI that also maximizes thermodynamic efficiency by maximizing energetic and memory use efficiency of prediction.
&amp;lt;strong&amp;gt;AI&amp;lt;/strong&amp;gt; An ambiguous and broad term. Can refer to AGI, ASI, True AI, Turing Test passing AI, AI or Clippy. Depending on the person, their mood and the weather. Ostensibly, it&amp;#39;s just the use of math and algorithms to do filtering, prediction, inference and efficient search.
&amp;lt;strong&amp;gt;Natural Language Processing/Understanding&amp;lt;/strong&amp;gt; The use of machine learning and supposedly  linguistics to try and convert the implicit structure in text to an explicitly structured representation. These days no one really pays attention to linguistics, which is not necessarily a good thing. For example, NLP people spend a lot more time on dependency parsing when constituency parsers better match human language use.
Anyways, considering the amount of embedded structure in text, it is stubbornly hard to get results that are any better than doing the dumbest thing you can think of. On second thought, this is probably due to how much structure there is in language on one hand and how flexible it is on another. For example, simply averaging word vectors with some minor corrections does almost as well and sometimes generalizes better than using a whiz bang Recurrent Neural Net. The state of NLP, in particular the difficulty of extracting anything remotely close to meaining, is the strongest indicator that Artificial General Intelligence is not near. Do not be fooled by PR and artificial tests, the systems remain as brittle to edge cases as ever. Real systems are high dimensional. As such, they are mostly edge cases.
&amp;lt;strong&amp;gt;Deep Learning&amp;lt;/strong&amp;gt; These days, used as a stand in for machine learning even though it is a subset of it. Such a labeling is as useful as answering &amp;quot;what are you eating&amp;quot; with &amp;quot;food&amp;quot;. DL, using neural networks, is the representation of computer programs as a series of tables of numbers (each layer of a Neural Network is a matrix). A vector is transformed by multiplying it with a matrix and applying another function to each element. A favored function is one that clamps all negative numbers to zero and results in piecewise function approximation. Each layer learns a more holistic representation based on the layers previous, until the final layer can be a really dumb linear regressor performing nontrivial separations.
The learned transformations often represent conditional probability distributions. Learning occurs by calculating derivatives of our function and adjusting parameters to do better against a loss function. Seeking the (locally) optimal model within model space. Newer Neural networks explicitly model latent/hidden/internal variables and are as such, even closer to the variational approach mentioned above.
Speaking of latent variables, there is an unfortunate trend of obsession about the clarity of model generated images. Yet quality of generation does not necessarily equate with quality of representation. And quality of representation is what matters. Consider humans, the majority of our vision is peripheral (we get around this by saccading and joining small sections together). Ruth Rosenholtz has shown a good model of peripheral vision is of capturing summary statistics. Although people complain that the visual quality of variational autoencoder is poor due to fuzziness, their outputs are not so far from models of peripheral vision.
&amp;lt;img src=&amp;quot;Images/opera_2017-06-13_15-37-26.png&amp;quot; alt=&amp;quot;mongrel images&amp;quot;&amp;gt;
&amp;lt;a href=&amp;quot;https://pdfs.semanticscholar.org/cb87/dfabb88f37114c4ca6c64ff938ae32f00c74.pdf&amp;quot;&amp;gt;https://pdfs.semanticscholar.org/cb87/dfabb88f37114c4ca6c64ff938ae32f00c74.pdf&amp;lt;/a&amp;gt;
&amp;lt;img src=&amp;quot;Images/opera_2017-06-13_15-37-46.png&amp;quot; alt=&amp;quot;VAE generated&amp;quot;&amp;gt;
The obsession is even more questionable when we consider that internal higher order representations have lost nearly all but the most important core required for visual information. Lossy Compression is lazy = good = energy efficient. Considering their clear variational motivation and the connection to the Information Bottleneck Principle, I feel it a bit unfortunate that work on VAEs has dropped so, in favor of Adversarial Networks.
&amp;lt;img src=&amp;quot;Images/cae4a658173ffe1f0960c3f9332f1f0f.jpg&amp;quot; alt=&amp;quot;picasso&amp;quot;&amp;gt;
[1] Page: 67 | John Markoff, Machines of Loving Grace: The Quest for Common Ground Between Humans and Robots
[2] Page: 40 | 1511.02476.pdf
[3] &amp;lt;a href=&amp;quot;http://rspa.royalsocietypublishing.org/content/royprsa/150/871/552.full.pdf&amp;quot;&amp;gt;http://rspa.royalsocietypublishing.org/content/royprsa/150/871/552.full.pdf&amp;lt;/a&amp;gt;
[4] Page 1 | WellerEtAl_uai14.pdf
[5] Page: 9 | Information and Efficiency in the Nervous System
[6] Page: 4 | 1203.3271v3
&amp;lt;img src=&amp;quot;Images/opera_2017-06-10_20-44-17.png&amp;quot; alt=&amp;quot;Path traced image&amp;quot;&amp;gt;
&amp;lt;em&amp;gt;Path traced image&amp;lt;/em&amp;gt;
&amp;lt;a href=&amp;quot;http://madebyevan.com/webgl-path-tracing/&amp;quot;&amp;gt;http://madebyevan.com/webgl-path-tracing/&amp;lt;/a&amp;gt;</description>
    </item>
    <item>
      <title>On the Breakdown of the Bicameral Mind and the Importance of Diversity </title>
      <link>On_the_Breakdown_of_the_Bicameral_Mind_and_the_Importance_of_Diversity_.htm</link>
      <guid>On_the_Breakdown_of_the_Bicameral_Mind_and_the_Importance_of_Diversity_.htm</guid>
      <pubDate>Sat, 12 Aug 2017 00:00:00 GMT</pubDate>
      <description>Have you heard of the &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Bicameralism_(psychology)&amp;quot;&amp;gt;bicameral mind&amp;lt;/a&amp;gt;? If you haven&amp;#39;t, it&amp;#39;s worth checking out. While I don&amp;#39;t think it correct, I think it takes only a slight modification to get us something highly probable. In this view, like in the Bicameral setting, we don&amp;#39;t all experience consciousness the same way. The core mediator is the differences in sophistication of theory of mind. The brain is plastic and different percepts affect both attentional priors and how language is used for self-modelling. The more complex stories we told, the better the listeners had to be at modeling those scenarios and the better the brain had to be at representing states at a meta level.
The break-down occurred through diffusion and cultural interchange leading to richer stories which in turn called for more complex modelling. The better you are at modelling others&amp;#39; mental states, the better you will be at modelling your own.
&amp;lt;em&amp;gt;Note: The next paragraph is conditional on the occurrence of a breakdown like event but the core message is independent of it. The compositionality of culture and knowledge means the gains to interaction are multiplicative which in turn affect the creativity of the citizens by the widening of available mental tools and percepts. The similarity between my exposition and the bicameral mind is that like in the bicameral mind, there was a phase change resulting in an enriching of the manner in which consciousness was instantiated in humans. The difference is that this was not due to evolution or an inferiority of previous brains but rather, a side effect of learning to learn and sharing between sufficiently diverse cultures combined with/multiplied by language and yielding a more powerful theory of mind ability&amp;lt;/em&amp;gt;.
The breakdown of the bicameral mind was not one of evolution but due to the brain&amp;#39;s plasticity. Increasing intellectual demands from the compositionality of culture, combined with a general learning engine of the brain, resulted in more flexible approaches to thought. We know language is linked to consciousness and higher order reasoning (see [1] on how metaphor shapes reasoning or [4] for how language augments numeracy as examples) and also that it&amp;#39;s one area where the brain must learn how to learn. While the individuals were not less conscious nor less intelligent, the lack of richness of available percepts severely circumscribed their ability for ideation. I doubt there&amp;#39;s anything falsifiable that could be said of an ancient left/right brain breakdown, but it&amp;#39;s certainly true that  available default abstractions would strongly impact a person&amp;#39;s manner of cognizing about the world, theorizing of events, arranging causation and most importantly, reasoning about themselves. This could plausibly affect how consciousness was realized in the individual (emphasizing that there is no objective hierarchy or ordering). Although clearly a good thing, this sort of multi-cultural richness is not without its own set of hurdles.
Well, differences in ability to predict mental states yields difficulty in collaboration. If two group&amp;#39;s mental organization is so different as to make communication very exhaustive then coordination and &amp;lt;em&amp;gt;attending to the same signals, required for positive welfare outcomes will be difficult to achieve&amp;lt;/em&amp;gt;.
The answer however, is not less diversity, it&amp;#39;s more cooperation. Genetic, ecosystem and hypothesis diversity (&amp;lt;em&amp;gt;Principle of Epicurus&amp;lt;/em&amp;gt;) are unequivocally good. Neural nets and even humans [2] suffer from mode collapse, a form of lack of diversity leading to reasoning errors, which can lead to suboptimal exploration and poor decisions. If the human mind is computable then a group of humans is not other than a highly bottlenecked parallel computer.
Again, considering [2], humans which share too much in common will be akin to starting a sampler near similar locations. If we seek to converge on the part of the landscape with peak probability, it pays to sample in parallel (via multiple people) from many start locations. If the brain is a sampling based explorer, with active inference to mitigate autocorrelation, then it&amp;#39;s clear that lived experiences leads to richer exploration than any other source of diversity (and as long as it&amp;#39;s computable, there&amp;#39;s &amp;lt;a href=&amp;quot;http://www.sciencedirect.com/science/article/pii/S0278262615300038&amp;quot;&amp;gt;very good reason [5]&amp;lt;/a&amp;gt; to believe that it is a sampler in the large and possibly variational locally). Ideological diversity must be retained however, since it does provide different attentional weightings—it&amp;#39;s just that the gains are not as large as from a diversity of parallel active samplers.
The only sense in which diversity can be considered harmful is when they make coordination more difficult (while I do not agree with their approach, I think China understands this on some level, that and economic protectionism explains many of their policies). Ideological diversity is probably especially prone to this. Ideological diversity, I should point out, is not just what politics or economic policies you adhere to but also, what kind of music and hobbies you like! Focusing on other forms of diversity can only increase ideological diversity.
However, short range correlations (siloing into isolated cultures) does not lead to interesting structures. Recall that one plausible way for the phase change in theory of mind to have occurred is from the richness in stories from cultural interaction and sharing. Resulting then, in a gain in sophistication on conscious reasoning about agents. Wider experiences leads to richer representations—though each human possesses only a degraded average over encounters—it is yet a comparatively richer set of examplar states to draw samples from. In essence, a certain group gets the &amp;quot;problem&amp;quot; of diversity &amp;lt;em&amp;gt;exactly&amp;lt;/em&amp;gt; backwards. Diversity issues stem not from physical differences but from experiential and ideological heterogeneity, leading to different priorities and attentional weightings. Furthermore, experience is a stronger mediator of ideology than whatever labels (e.g. centrist) is currently fashionable to apply to one&amp;#39;s self. All the kinds of diversity which concerns this certain crowd are in fact good but the kind which they view as important (as do I), provides the most trouble.
So what then? I think the answer is to raise human children emphasizing shared commonalities, more sharing of culture (yes, I&amp;#39;m pro cultural appropriation) and how to empathize (that is, not dehumanizing the &amp;lt;em&amp;gt;other&amp;lt;/em&amp;gt;) with other beings. Additionally, a lot of problems are caused by fortifying identity by dredging for differences. If we can create a society such that there are sufficient avenues and means for collaboration and sharing—whether on art, stories or music, any kind of creation—the need to find meaning by belonging to something, should lessen by a large amount.
Meaning from sharp boundaries group identity reduces a human&amp;#39;s ability to reason, replacing it with pattern matched stock responses and confabulations. This is an example of when correlations can go wrong, trapping members in equilibria even if the members could have long since drifted past the original goals and utilities.
&amp;lt;strong&amp;gt;Prediction&amp;lt;/strong&amp;gt;: If children all over the world grow up watching each other&amp;#39;s culture&amp;#39;s cartoons, this will have a significant ameliorative effect on any ill coordinative effects of cultural diversity. Of course, some might question the extent to which this is a prediction given the broad arch of history.
It is unlikely that humanity cannot learn to cooperate broadly and arbitrarily; this is rather, a problem of learning how to escape pernicious attractor states and achieving positive social utility through mechanism design.
In this essay, I point out that a computable brain performing sampling based inference as suggested in [2], and a group of humans yields a parallel sampler. Connecting this with the observation in [3] that autocorrelation is minimized by &amp;lt;em&amp;gt;active&amp;lt;/em&amp;gt; inference tells us that to get better exploration and representative posterior probability distributions with respect to reality, we want a wider range of physical experiences. As a result, diversity is important. I point out however, that since ideological diversity is cognitive and attentional, it is broader than politics—increasing general diversity must increase ideology. Secondly, it can be negative, due to a reduced shared ability to attend to the same signals and efficiently communicate (coming off such different attentional weights).
I argue however, that diversity is worth it by pointing out how a scenario like the break-down of the bicameral mind might have been precipitated by trade and cultural diffusion leading to richer stories and hence better and more practicing of theory of mind (I suggested replacing &amp;lt;em&amp;gt;origin&amp;lt;/em&amp;gt; of consciousness with enriching of consciousness, based on the compositionality of culture together with plasticity from our ability to learn how to learn). I also suggest cultural appropriation and lesser emphasis on identity/belonging and more on collaboration to counter ideological clashes of diversity.
I bring up the issue of ideologies because careful manipulation of attended to signals can lead to feedback loops which sort matched humans into clusters of sharper and wider boundaries, divorced from reality—a general human failing which gets in the way of coordination—and therefore handicaping our ability to achieve positive welfare outcomes globally. The issue is tribal (geographically correlated attentional priors and utilities) in nature and happens in every country with multiple ethnicities. Which is almost every country. I&amp;#39;ll note that the &amp;lt;em&amp;gt;EU&amp;lt;/em&amp;gt; was a response to one such.
[1] &amp;lt;a href=&amp;quot;http://www.sciencedirect.com/science/article/pii/S1364661317301535&amp;quot;&amp;gt;http://www.sciencedirect.com/science/article/pii/S1364661317301535&amp;lt;/a&amp;gt;
[2] &amp;lt;a href=&amp;quot;http://www.sciencedirect.com/science/article/pii/S1364661316301565&amp;quot;&amp;gt;http://www.sciencedirect.com/science/article/pii/S1364661316301565&amp;lt;/a&amp;gt;
[3] &amp;lt;a href=&amp;quot;http://www.biorxiv.org/content/biorxiv/early/2017/02/17/109355.full.pdf&amp;quot;&amp;gt;http://www.biorxiv.org/content/biorxiv/early/2017/02/17/109355.full.pdf&amp;lt;/a&amp;gt;
[4] &amp;lt;a href=&amp;quot;http://langcog.stanford.edu/papers/FEFG-cognition.pdf&amp;quot;&amp;gt;http://langcog.stanford.edu/papers/FEFG-cognition.pdf&amp;lt;/a&amp;gt;
[5] &amp;lt;a href=&amp;quot;http://www.sciencedirect.com/science/article/pii/S0278262615300038&amp;quot;&amp;gt;http://www.sciencedirect.com/science/article/pii/S0278262615300038&amp;lt;/a&amp;gt;</description>
    </item>
    <item>
      <title>A Sketching Look at AI Ethics</title>
      <link>A_Sketching_Look_at_AI_Ethics.htm</link>
      <guid>A_Sketching_Look_at_AI_Ethics.htm</guid>
      <pubDate>Sat, 12 Aug 2017 00:00:00 GMT</pubDate>
      <description>It&amp;#39;s common to mock economics as useless and completely detached from reality. Yet, the fact that algorithms as simple as regret matching or multiplicative weights update, converge on various important equilibrium concepts—algorithms we believe frequently occur in some form in nature—tells us that the core ideas of game theory are in fact very useful. The mistake that has been made is confusing the simplifying assumptions selected so human economist could hand wave a defense for flawed policy advice—unrealistic utility functions, simple two player games, zero sum, intractable equilibrium concepts—for proof of a lack of utility of the subject. In actuality, the game theory is extremely natural, merely poorly applied. We should move from boring discussions on 0-sum games, nash equilibria, prisoner&amp;#39;s dilemma, tit for tat (which does not scale), to discussions on coordination, cooperation, welfare and fairness. And as it turns out, there has been a lot of recent work done in that area—the only problem is the focus on ad markets and auctions—there is no reason it can&amp;#39;t be adapted to improve human lives. Some aspects of modern economics are indistinguishable from online machine learning or computational game theory. It&amp;#39;s a large subject deserving of more attention. If we choose this path, the question then arises: is it ethical to turn over important decision making to machines?
The debate on AI ethics is currently dominated by 3 archetypes. I&amp;#39;ll summarize them before arguing for another area that&amp;#39;s at least as and possibly more important.
In this line of work, researchers point out how blind following of algorithms can lead to bad outcomes by amplifying bias and strengthening inequality. Although a few look at how to ensure that AI doesn&amp;#39;t end up as yet another tool to suppress humans (by working on energy efficient or decentralized learning algorithms), most of the work is in how algorithms amplify bias.
Text is highly structured, in particular, it crystalizes projections on thought and embeds the way humans use language and thus simple algorithms can display high competence with little understanding. As an example, consider the output of a generative text model (I recently came up with—not a markov chain, not an rnn, similar to both—post coming soon), such that when prompting it with the phrase &amp;quot;&amp;lt;em&amp;gt;the prefrontal cortex is&amp;lt;/em&amp;gt;&amp;quot; yields the following (not cherry picked):
the prefrontal cortex is the attention to prediction of the world
the prefrontal cortex is the control in the mind
the prefrontal cortex is the control of the general factor of intelligence
the prefrontal cortex is the control attention
the prefrontal cortex is the control in the human brain
None of these appear anywhere in my corpus of training documents but it&amp;#39;s eerie in how good of a summary it is of the relevant literature. Training it on a voat corpus and then prompting it on blacks or Jews compared to a wikipedia corpus would yield very different results. This is not reflective of reality, it&amp;#39;s reflective of how humans of a culture use language and what they write about. What humans think with their = 10 Watts of conscious cognition (assuming uniform process/energy use, 20 Watts, most processing not conscious) is very limited and can be divorced from reality.
My main criticism of the bias work is with the idea that there can be algorithmic correction. There&amp;#39;s a great deal of danger from a false sense of complacency, thinking a job well done with biases &amp;quot;corrected&amp;quot;. Worse, who is to say those corrections are even appropriate when applied inflexibly at scale? Indeed, beyond an inability to effectively enumerate the space of biases,the larger, more pressing problem is in the combinatorial possibilities of seemingly innocuous individual actions. No system, that trains till &amp;lt;em&amp;gt;average&amp;lt;/em&amp;gt; losses are minimized, therefore assuming some stationary distribution, will be able to match the need for nuance and context in reality.
So you&amp;#39;ve handled race and gender issues. Have you corrected the sentiment bias of &amp;lt;em&amp;gt;short bus&amp;lt;/em&amp;gt;? What about the countless other combinations you&amp;#39;re unaware of? Or, consider correcting for bias in health outcomes when an insurer is focused on above normal profits (and going beyond the bare minimum require to stay running). That would be putting the cart before the horse. Rather than play a continuous game of whack a mole, it would be better to create a culture of not blindly deferring to AI judgement in any area that could materially affect a human life. The policy aspect will make a lot more difference than most anything that can be done with algorithms (until we get AI&amp;#39;s that aren&amp;#39;t blindly performant).
The general idea here is to slow down work on AI to ensure no one develops AIs which view humans as inconsequential. MIRI leads algorithmic work on the topic and places like the Future of Life Institute lead the policy aspect. However, I do not believe anyone knows how to mitigate these claimed risks or even has any inkling as to how they might come about beyond very vague arguments. Despite this, and while I do not agree with the claimed magnitude of risk nor with the framing, I do not think anyone has prepared a proper counter-argument for why the AI Risk Crowd is mistaken (I think it&amp;#39;s possible to prepare such an argument, I&amp;#39;ve just not seen it yet). That said, while I agree that we should build kind, caring AIs which are respectful of living things and their ecosystems (weighted by their flexibility in traversing &amp;#39;information manifolds&amp;#39;, I don&amp;#39;t particularly care for the value alignment nothing else is as important framing.
This one is mostly corporations generating slogans to keep humans at ease. Boards are created, names are listed, chants are chanted but nothing meaningful will likely ever come of it. You see, it is difficult to trust an entity that cannot imagine a future in which you are not as thoroughly dependent as possible on it.
The regret/bandit/expert algorithms which featured heavily in &amp;lt;a href=&amp;quot;Learning,_Ethics_and_Game_theory_.htm&amp;quot;&amp;gt;this&amp;lt;/a&amp;gt; post are algorithms which already run the (digital) world. Although deep learning gets a lot of public attention; it&amp;#39;s some kind of bandit algorithm what places ads, manages auctions or perturbs site designs in order to squeeze money and attention from their human playmates. Although not something I personally agree with, it&amp;#39;s a non-issue in comparison to scenarios where such algorithms are turned to subverting and redirecting attention for policy issues instead of merely purchase decisions. What tools can be provided for defense? I don&amp;#39;t see anyone moving to democratize that.
And yet, as important as the above are, I think what subsumes them all is thinking about the ethics of algorithmic approaches to ensuring welfare of cities, humans, animals and ecosystems. Is it ethical to not use these approaches or is it true that there is some humanity lost by algorithmically deciding on important policy issues? It&amp;#39;s clear that algorithms can&amp;#39;t yet (maybe never) automatically operate on important decisions which will affect human agency. There&amp;#39;s however, a case to be made for a combination approach.
For example, we saw in &amp;lt;a href=&amp;quot;Learning,_Ethics_and_Game_theory_.htm&amp;quot;&amp;gt;this post&amp;lt;/a&amp;gt; that Nash Equilibria are hard to reach and might not always be great. We also saw that correlated equilibria can lead to quite fair results but while they are easy to compute, they are not obvious to set up. And even when we can, how will people take to suggestions handed down by faceless algorithms? How do we ensure such a process doesn&amp;#39;t end up as something to worship or enslave?
Getting multiple agents with differing goals to cooperate is extremely difficult. There&amp;#39;s plenty done on machine and single agent reinforcement learning but not nearly as much done on trying to learn cooperative agents. If you are worried about AI Risk then I think you should also be concerned by this imbalance. With better co-operative agent modelling we might begin to look at methods of approximating policy impact better than everything before. At some point we could design systems where those affected by policy could contribute detailed descriptions of preferences and actions. The system would then use co-operative learning agent modeling to approximately calculate some fair correlated equilibrium or aggregated welfare concept. While superficially similar, this collective decision making differs from a planned economy in that it is ad hoc and only scopes over the collective making and affected by the decisions.
When the issues are too complex for such an approach, a more bottom up approach would be to model complex agents in a scenario approximating the decision conflict. People could search over and test different reward functions in order to guide policy design whose effect will be to get people to behave optimally with respect to some shared goal (and which selfish behaviors could easily thwart). Again, all members affected can give detailed concerns and input and discuss policy in terms of how the model is effected. Remember, though the model will have far from perfect coverage over outcomes, it would still be miles better than the current approach of sparsely sampled scalar votes influenced by ever more sophisticated technologically amplified propaganda. The results will be approximate and imperfect but by making the simulations interactive with the participants, a powerful problem solving capability could be gained. While much of this is probably not near (needing either good language AIs or more people good at programming—also note this isn&amp;#39;t calling for laws as programs only for models to help predict outcomes), there are aspects which could be studied today.
One way to set up correlated equilibria is by studying various specifications of problems and noting how different reward functions lead to different results. Auditable, Traceable cryptographic smart contracts are one way to achieve this. Imagine a scenario of contracts between developed and developing cities targeting climate emissions. On the one hand, it&amp;#39;s important to reduce CO2 emissions, on the other hand the developing nation might say: &amp;quot;we are tired of being mired in poverty&amp;quot;. One way around this would be to search for a policy that would place both actors in a correlated equilibrium or better. Smart contracts allow for this—they are one way to route conditional exchanges such that no corrupt local actors could steal. Additionally: agreements preregistering purchase decisions, a time gated method tracking proper allocation of any citizen grants, measurable quotas, trades, all separately conditional on future dates and automatically verifiable obligations, just might allow for a fairer outcome for all. The contract mechanism then acts as an impartial mediator to ensure a positive correlated outcome.
By setting up mechanism design tested policies we can get favorable results with higher probability. But one distant ethical issue is in ensuring that agents don&amp;#39;t get so sophisticated, the issue of their being deserving of rights becomes ascendant. If such a scenario is possible, then there must be a computable theory of consciousness and ethics. We should ensure we figure those out before building anything too sophisticated. Sounds far fetched? Well, I&amp;#39;m not saying it&amp;#39;s now, I&amp;#39;m only saying that it would be really terrible if it happened in some distant future and caught us all off guard such that we were then motivated to continue to not notice how horrible a thing we were doing.
The other glaring issue is: &amp;quot;would we be losing our humanity by handing off important decision making to algorithms&amp;quot;? For me the answer is clear: the human mind is also just algorithms, if we can design a system to help us arrive at fairer decisions than we otherwise would have, say because due to complexity limiting our brains&amp;#39; ability to &amp;lt;em&amp;gt;as&amp;lt;/em&amp;gt; fully traverse the space of consequences, then there&amp;#39;s nothing lost in adhering. The inputs, goals and motivations would be human decided. An objection to this, I believe, would be like not living in a house because it was built using Caterpillar heavy machinery or because it was 3D printed or prefabricated by robots. Humans were the designers and architects; the structure is to provide positive utility for the humans contained.
Furthermore, I consider the agency and individuality of humans as mythologized. Most of a person&amp;#39;s personality is settled by chance events, genetics and accumulated experience from childhood.  In adulthood, it&amp;#39;s a system of cultural expectations and traditions which strongly bind what directions any individual might take. We would not be trading away some glorious past of human agency. Not a past filled with trails of tears, slavery, indentured servitude or conscription into pointless wars. In the past, any kind of literacy was scarce, with only a tiny portion of the population able to indulge in scientific and philosophical thought. Not forgetting that under limited diffusion of ideas, people became echoes of their neighbors and even worshipped their rulers as gods. With some cultures even valuing human sacrifice.
Far from outsourcing ourselves into technology, the extension of our minds into the environment is a defining trait of humanity, ever since the invention of language. A larger portion than ever is improved in their ability to reason and handle abstraction [4]. &amp;lt;em&amp;gt;Note that all humans are capable of  achieving this gain but consider, as an example, how understanding permutations gives a reasoning advantage&amp;lt;/em&amp;gt;. We&amp;#39;ve long outsourced thought into abacuses, tools, books, quipu, tabulating machines and lately, computers. In turn, we&amp;#39;ve gained access to more readily available knowledge and tools and the ability to administer ever more complex societies.
In this essay, I note that done correctly, auditable cryptographic smart contracts could act as neutral mediators to achieve non-trivial correlated equilibria. I give the example of cities cooperating to reduce emissions, that works no matter how corrupt the local government.
I then talk about Ethics in AI, how highlighting bias is important but algorithmically dealing with it is misguided. I mention AI Risk and my skepticism of its urgency but dissatisfaction with &amp;lt;em&amp;gt;all&amp;lt;/em&amp;gt; counterarguments. I also mention the case of modeling agents which become too sophisticated.
&amp;lt;em&amp;gt;If such a scenario is possible, then there must be a computable theory of consciousness and ethics. We should ensure we figure those out before building anything too sophisticated. Sounds far fetched? Well, I&amp;#39;m not saying it&amp;#39;s now, I&amp;#39;m only saying that it would be really terrible if it happened in some distant future and caught us all off guard such that we were then motivated to continue to not notice how horrible a thing we were doing&amp;lt;/em&amp;gt;.
I mentioned the difficulty of getting multiple cooperating agents and the benefits of doing more work in this area in order to guide real world policy. And moving beyond the toy, completely unrealistic scenarios favored by traditional economics. I also note that a lot of interesting work &amp;lt;em&amp;gt;has&amp;lt;/em&amp;gt; been done in the areas of fairness and welfare, but mostly on how to better place and monetize ads. It seems to be working out well.
I argue that allowing algorithms to play a role in human decision making is no less giving up our humanity than allowing a robot to build a prefabricated home. In both cases, human labor is better applied to design and goal setting and not on energy intensive labor which humans are ill-suited to (such as lifting heavy objects or searching combinatorial spaces). I also argue that the past was not really some bastion of agency, free thinking or freedom. There&amp;#39;s more of that now than ever and augmenting our capabilities with computational devices will likely lead to better welfare outcomes.
[1] &amp;lt;a href=&amp;quot;https://arxiv.org/pdf/1711.00363v1.pdf&amp;quot;&amp;gt;https://arxiv.org/pdf/1711.00363v1.pdf&amp;lt;/a&amp;gt;
[2] &amp;lt;a href=&amp;quot;https://arxiv.org/abs/1504.06314&amp;quot;&amp;gt;https://arxiv.org/abs/1504.06314&amp;lt;/a&amp;gt;
[3] &amp;lt;a href=&amp;quot;https://arxiv.org/pdf/1609.05807.pdf&amp;quot;&amp;gt;https://arxiv.org/pdf/1609.05807.pdf&amp;lt;/a&amp;gt;</description>
    </item>
    <item>
      <title>Learning, Ethics and Game theory </title>
      <link>Learning,_Ethics_and_Game_theory_.htm</link>
      <guid>Learning,_Ethics_and_Game_theory_.htm</guid>
      <pubDate>Sat, 12 Aug 2017 21:06:14 GMT</pubDate>
      <description>&amp;lt;img src=&amp;quot;Images/graph.png&amp;quot; alt=&amp;quot;graph&amp;quot;&amp;gt;
In the image above, I&amp;#39;ve sketched how one can show that evolution, game theory and learning are related. While I&amp;#39;ll not talk about natural selection today, I&amp;#39;ll discuss how ideas from game theory can be given an easier footing from within the context of computational learning theory. A future post will discuss evolution from the min-regret framework so it&amp;#39;s worth following the ideas presented here. Topics: AI Ethics (why everyone gets some part wrong), economics (why capitalism is intractable), games, learning algorithms. I must apologize—ideally, there should have been interactive demos but alas, I hadn&amp;#39;t the time for that.
The Nash Equilibrium (NE) is a solution concept for games. Games are a framework which prescribe how to behave and coordinate optimally in a given situation, assuming some way of ranking outcomes (utility) and effectively infinite computation (among other unreasonable things). Okay to be fair, people have recently been looking into how to operate under the constraints of resource limitations. The min-regret framework has informed some of that work.
A NE can either be pure (where you play only one strategy) or mixed (where you randomize over your strategy set). A strategy is essentially a look up table (or algorithm) telling you how to behave at all decision points.  Not all games have a pure NE but all (finite) games have at least one mixed NE. At a NE, each player gains no utility from switching to a different strategy profile. Although highly celebrated, NEs are actually rarely practical for several reasons. They inhabit the PPAD complexity class (which are conjectured to be intractable), work best in 2 player zero sum (or other simple) scenarios (there can only be one winner) and will not necessarily find a fair solution concept. The utility of the the solution concept for general sum games is also questionable. Finally, although they are in some sense optimal, they do not necessarily provide the &amp;quot;best&amp;quot; results. In particular, for 2 player 0-sum games, NEs have zero expectation.
As an example, consider Rock Paper Scissors. The mixed NE for this game is to randomize evenly: play one of the pure strategies of always playing one of rock,paper or scissors with 1/3 probability (to be clear, an example of a &amp;lt;em&amp;gt;pure strategy&amp;lt;/em&amp;gt; in this game is to always play rock).
If you always play: &amp;#128240; then I can switch to a strategy of always playing ✂. It you play a strategy like [&amp;#128074;,70% ;&amp;#128240;, 10%; ✂, 20%; ] then I can play the pure strategy of [&amp;#128240;, 100%] and perform optimally against you. This is because 70% of the time I win, 10% of the time I tie and only 20% of the time am I losing. You can expect to lose -0.5 in expectation (this might not seem like much but on average, if we play 100 rounds and bet 5&amp;#128181;/round each, then I&amp;#39;d have taken &amp;#128178;250 from you). You can check the code &amp;lt;a href=&amp;quot;https://gist.github.com/sir-deenicus/e4e19f9a01fb224fecd3ebdcd02c3f71&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt;, other adjustments will do worse than just playing pure paper.
Imagine you always played &amp;#128240;. Then 1/3 times you&amp;#39;ll tie with &amp;#128240;, 1/3 times you&amp;#39;ll win against &amp;#128074;, and 1/3 times you&amp;#39;ll lose vs. ✂.
There might even be runs where it will seem like you&amp;#39;re winning. If your goal is to maximize profit against an opponent, then NE might not be the correct strategy for games like RPS.
One thing you might try would be to learn patterns of your opponent and then playing a strategy exploiting that. The problem is this in turn always leaves you vulnerable. The opponent might know what you are doing and mislead you for a bit and then BAM, switch things up and collect some wins before your algorithm notices something is off. A NE has the advantage that even if your opponent knew what you were up to, they still could not (e.g.) take money off of you (in the long run).
Not all 0-sum 2 player games have the interesting quirk where it&amp;#39;s impossible to lose against an optimal player. Some games punish you for taking stupid actions. Of course, stupid is relative, and some games such as poker have action spaces so complex that it&amp;#39;s very, very difficult to know when you&amp;#39;re being stupid. For this example we&amp;#39;ll look at a simple extension of Rock, Paper, Scissors. We&amp;#39;ll add another move: telephone. How to handle rewards? If we make telephone a duplicate of paper, then we end up with a game that&amp;#39;s exactly the same as RPS save that there are now two equilibria: [&amp;#128074;,1/3 ;&amp;#128240;, 1/3; ✂, 1/3;  &amp;#128224;,0%] and [&amp;#128074;,1/3 ;&amp;#128240;, 1/6; ✂, 1/3;  &amp;#128224;,1/6]. The other aspects, where playing any mixed non-equilibrium strategy can be defeated by a pure strategy or where it is impossible to lose against an optimal player remain.
Imagine you played pure paper again. Then a NE player will either play the same mixed strategy as for RPS, with the same results or play the second NE strategy. Then: 1/3 of the time you lose against rock, 1/3 of the time you win against scissors and tie for the other two scenarios.
What if we made the telephone over-powered? That is, it ties against everything except paper, which it defeats. What&amp;#39;s the best way to play against someone who only plays telephone and what&amp;#39;s the equilibrium for this game?
Did you get it? Don&amp;#39;t play paper. As long as you don&amp;#39;t play paper then you have a zero loss expectation against a pure telephone player, which is also the pure equilibrium strategy for this game. Playing paper is the stupid action, with your loss proportional to the probability with which you play paper. In this game, it is possible to lose against an equilibrium player by playing paper.
Poker is a game like over-powered telephone, rock,paper, scissors. The optimal player only wins because there are stupid actions you can take which act essentially as donations. In our modification to RPS, it&amp;#39;s clear that playing paper is dumb. Let&amp;#39;s modify the game to make it a bit more complex. A slight mod to RPST such as &amp;#128224; loses to ✂ but defeats &amp;#128074; and &amp;#128240; yields a slightly more interesting result. Once again, playing paper is a stupid action but playing just &amp;#128224; means someone else can play just ✂. The NE is: [&amp;#128074;,1/3 ;&amp;#128240;, 0%; ✂, 1/3;  &amp;#128224;,1/3]. With some thought, you should notice that as long as we never play paper, we can&amp;#39;t lose against an optimal player. Even playing just &amp;#128074; is fine since 1/3 of the time we defeat ✂,1/3 of the time we lose to &amp;#128224; and tie with &amp;#128074; 1/3 of the time. This game has a &amp;quot;stupid&amp;quot; action but offering to play against someone out of the blue, they might not realize that paper is a trap.
The final modification will be to introduce this game: Telephone ties with telephone, Telephone loses to scissors but 50% of the time it defeats rock and 4/6 of the time it defeats paper. The game is no longer straightforward but it&amp;#39;s a lot closer to the original RPS. The NE is  [&amp;#128074;,1/3 ;&amp;#128240;, 1/3; ✂, 1/3;  &amp;#128224;,0%], other mixed strategies have a pure counter. A strategy like  [&amp;#128074;,40% ;&amp;#128240;, 40%; ✂, 10%;  &amp;#128224;,10%] can be countered with [&amp;#128240;,100%]. The expected win rate is ~0.26. This can add up:
This time, the only way to lose against an optimal player is to include telephone in your mix. Although it seems powerful (can defeat paper ~67% of the time and 50% vs rock and only loses to scissors), in the long run it loses (playing pure &amp;#128224; has a lose rate of about -0.22). If properly presented, a player can easily be tricked into thinking that &amp;#128224; is over powered.
You can look at the code relevant to this section &amp;lt;a href=&amp;quot;https://gist.github.com/sir-deenicus/623e57a88c3321edde2a00c0d1cd7053&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt;.
Well, I think that&amp;#39;s about enough of rock, paper scissors. Although zero sum games are well studied, this is because they are one of the few scenarios where computing a Nash Equilibrium is tractable. There are probably not many situations that are 2 player and zero sum, and the concept of zero sum is really only sensible in the two player setting. For example, people might say that markets are zero sum in the short run but that doesn&amp;#39;t make sense to me. Firstly, this requires that the traded currency amount is equal to the utility or the value placed by the respective traders. This need not be the case. Secondly, the game can really only be two player if we pretend that other players actions do not affect the available actions and information. This seems unlikely too. Beyond Games and Game Theory exercises, it&amp;#39;s hard to think up scenarios that are genuinely two player and zero sum. Sometimes I think people claim zero sum as an excuse for their selfish behavior without any real understanding of what the notion entails.
Even two player general sum games can be intractable—as I mentioned earlier general sum games can have NE that are too difficult to find because finding a NE is in the computational complexity class of PPAD.
Computational Complexity seems like a subject you&amp;#39;d get if you combined zoology, logic and a guide on how to survive the apocalypse (let&amp;#39;s prepare for the worst). There&amp;#39;s a lot of taxonomical terminology with strange names that will quickly turn any onlooker dizzy. When reading about Nash Equilibria in the context of poker bots and evolution, I nodded to myself and said &amp;quot;ah yes of course it would be PPAD...&amp;#128527;&amp;quot;. I&amp;#39;m still not confident in my understanding—there are lots of fine details—so will welcome any corrections.
Everyone who has heard of computational complexity has probably heard of polynomial time vs non-deterministic polynomial time (NP). By the way, the unwieldy naming of NP derives from the non-deterministic turing machine (NDTM) which is essentially a psychic computer and should not be confused with a proper probabilistic computer. NDTMs take choices in a way that is not determined by their state and solve NP problems in polynomial time by magic. Polynomial time (P) is in principle efficient but anything that scales faster than a factor of N^2 is truly stretching the meaning of efficient. Many useful algorithms are O(N^3) and unusable in practical settings. O(N log N) is about the limit of what is practically efficient (matrix x vector is O(N^2) but manages due to how well it parallelizes).
Problems that are NP-hard are not tractable to solve and their solutions are not &amp;lt;em&amp;gt;necessarily&amp;lt;/em&amp;gt; efficiently verifiable. Problems in NP are efficiently verifiable and NP-complete problems are problems where a solution to one of them yields a solution to all other problems in NP. Of course P is in NP. NP-complete problems are NP-hard problems that are efficiently verifiable. P and NP actually deal with decision problems. My favorite description of the question of whether P=NP is whether search is equal to recognition/calculation. Since we can also view learning as a form of search, the question of whether P=NP can be seen as: Is learning ever necessary? From this frame, P=NP, even with a high polynomial (which would be to say, no, planning and learning are not needed in theory) seems somehow unnatural.
A decision problem is, if I asked &amp;quot;what&amp;#39;s the shortest tour through these cities&amp;quot; and you replied &amp;quot;what?&amp;quot;. And then I rephrased  it as: &amp;quot;sorry, what I really not really meant to ask, is there a tour through these cities that takes less than N number of steps?&amp;quot; to which you said &amp;quot;yes&amp;quot; and fell silent. As you can see, decision problems are very curt and business like.
Well, a corresponding class is FP and FNP which deal with &amp;lt;em&amp;gt;function problems&amp;lt;/em&amp;gt;. This class is more literate and actually does things, like, returning an example which satisfies my touring question. FNP has the same property as NP of efficient verification. FNP might not always return an answer so there is a subclass, TFNP, dealing with total functions which always return an answer. PPAD is a subset of that class with the further requirement that the proof gadget of totality is by a directed graph. PPAD problems are conjectured as harder than FP problems and this conjecture is borne out by a lack of any polynomial time algorithms for their solution (although there are exponential lower bounds).
This is the essence of the class PPAD: search problems whose existence of solution is guaranteed by virtue of an exponentially large directed graph, implicitly represented through an algo- rithm, plus one given unbalanced point of the graph. Many problems are known to be PPAD- complete; perhaps the most fundamental such problem is related to Brouwer’s theorem
...
And it seems counterintuitive that there is always a way to telescope the search through all points of an exponentially large directed graph, to zero in the other unbalanced node, or the sink, that is sought.
&amp;lt;em&amp;gt;Page 3 | PNAS-2014-Papadimitriou-15881-7&amp;lt;/em&amp;gt;
Many celebrated concepts of economics rely in some way on Brouwer&amp;#39;s fixed point theorem. Actually finding this fixed point is PPAD-complete, in other words, intractable. In addition to Nash Equilibria there is also the Arrow-Debreu theorem, which tells us of the existence of a price equilibrium in free markets (under realistic conditions), where allocation of resources are pareto efficient. Pareto efficiency is a concept of &amp;quot;fairness&amp;quot; which is optimal in the sense that there is no reallocation of resources that doesn&amp;#39;t leave someone worse off and unbalanced. &amp;quot;Fairness&amp;quot; because a pareto efficient allocation need not be what we would intuit as fair. In a scenario where you have 99/100 items and I have 1/100, any reallocation will leave one of us worse off.
Common attacks against markets include violations of perfect information, transaction costs, externalities, barriers, assumptions of convexity in preferences and so on.
However, even before looking at those utopian requirements, we find that the very concepts motivating markets require intractable computations to reach their notion of fairness. And these (Nash, Pareto, Arrow) are not even guaranteed to be fair in an intuitive sense. It&amp;#39;s informative to contrast these computational issues of markets with those of Planned Economies—which are at least tractable in principle (but not in practice, &amp;quot;merely&amp;quot; requiring O(N^3.5) scaling with problem size). This is not to say that markets lead to worse outcomes than planned economies, merely to point out it is technically more impossible to achieve fair outcomes with them, compared to planned economies.
At this point, the concept of what an economy entails is worth revisiting given what we now know from computational learning theory. We can do better than planned or market economies.
Although the Nash Equilibrium is much celebrated, the correlated equilibrium concept is more natural in several senses. It&amp;#39;s not just tractable but also efficiently computable to high accuracy, the dynamics of many learning algorithms converge on it (or its coarse version) and even agents acting independently can conceivably converge to its equilibrium concept. But what is it? I actually find the definition to be a bit awkward.
The correlated equilibrium is a joint (as opposed to Nash&amp;#39;s independent) probability distribution over action profiles where, the expected utility from playing according to a drawn profile, conditioned on seeing its suggested action at least matches that of any other actions&amp;#39; (it is a best response). A coarse correlated equilibrium is similar except there is no suggested action to condition on—they can lead to lame, weakly dominated suggestions however. In math, a correlated equilibrium is:
&amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\(E_{a\sim D}[u_i(a)] \ge E_{a \sim D}[u_i(x_i,a_{-i})|a_i]\)&amp;lt;/span&amp;gt;
for every action &amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\(x_i\)&amp;lt;/span&amp;gt;. For the coarse version, there is no &amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\(a_i\)&amp;lt;/span&amp;gt; to condition on. Algorithms which minimize external regret converge on coarse equilibria whilst those minimizing swap or internal regret converge on correlated equilibriums.
More clearly, imagine there is some neutral device. This device draws from a joint probability distribution and tells each player to play according to its suggestion: a correlated equilibrium is when there is no incentive to deviate from its suggestion. These outcomes can be better than the Nash concept since CEs act on the joint space of actions while the Nash concept is very solipsistic and self-centered, looking at signals provides no information.
The example everyone gives is a traffic light signal. When it shows GO, you know the other player sees a STOP so there is no incentive to deviate. Here are some other examples I think should count, roughly in order of how well they fit: the non (now dead) Dennard Scaling portion of Moore&amp;#39;s Law, the I give up signal in animals (such as when a dog lays on its back—it&amp;#39;s better to think of the phenotype and not any specific instantiation as the player), functioning courts and laws, rituals/tradition (however, as self identity and values shift and change, such systems quickly induce equilibriums that are inequitable).
Regret minimizing algorithms are a class of efficient on-line (meaning learn on the go) learning algorithms which given some reward function and access to some set of experts (which can also be moves such as rock in RPS), which quickly learn what actions to take in order to minimize long term regret. External regret minimizers do this with respect to a fixed strategy while internal regreters do this with respect to any swapped set of actions. The internal regret scenario lets you say when I did this, I should have done that instead. External regret says, I did almost as well as that really good advisor.
Prediction markets let people bet on binary propositions. Imagine there was a prediction market with a layer  tracking how well each bettor (expert) did. External regret could do almost as well as the best bettor and perhaps even better with randomization over all bettors, weighted according to how well they did in the past.
I believe, but am not certain, that a scenario where you could match experts to contexts: for example, a technology expert, a sports expert and so on would likely allow you to minimize something like internal regret. From this you could even extract a weighted average over hypotheses.
For more detail on regret algorithms, you can take a look at the demo code. I implement both Multiplicative Weights Update and Regret Matching.
Consider the following game: there are N people, you each choose a number between 0 and 5 and show it with your hand &amp;#128400; on a ready signal. The winner is the person displaying the lowest unique number. The winner gets a score equal to N-1 while everyone else gets a score of -1.  This makes it effectively a 2 player 0-sum game. What&amp;#39;s the Nash Equilibrium strategy? I don&amp;#39;t know but when I run &amp;lt;a href=&amp;quot;https://gist.github.com/sir-deenicus/2ac6a2a064f4a82e09d81e60156d8915&amp;quot;&amp;gt;this&amp;lt;/a&amp;gt; I get the following sequence:
Move
Move Probability
0
0.499
1
0.25
2
0.126
3
0.063
4
0.031
5
0.03
Which is a pattern of ~&amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\(0.5^{x+1}\)&amp;lt;/span&amp;gt; but with some aliasing at the end. Simulation gives a mean utility of -0.006 &amp;#177; 1.31 (regret algorithms only get within epsilon of the exact equilibrium). The distribution over means is:  -0.0006 &amp;#177; 0.01. This corresponds to a win ~28.8% of the time, a loss ~56.9% of the time and a tie ~14.2% of the time. This looks close to what&amp;#39;s realistically achievable. It&amp;#39;s also interesting that in this game it is both possible to not be able to lose (if playing against two optimal players) or lose while providing donations to a single optimal player (with two off equilibrium players).
What happens when we try for a coarse correlated equilibrium? In &amp;lt;a href=&amp;quot;https://gist.github.com/sir-deenicus/2ac6a2a064f4a82e09d81e60156d8915&amp;quot;&amp;gt;this&amp;lt;/a&amp;gt;, I also apply regret matching on 3 players trying to maximize utility while looking at the other players. Sometimes, an interesting strategy emerges which kind of looks like collusion in that two players end up with positive expectation (e.g. 61% win, 39% lose, 0.83 expected utility; 61% lose, 39% win, 0.17 EU) and one is guaranteed losses such that it doesn&amp;#39;t matter what they do:
Move
Move Probability
0
0.395
1
0.289
2
0.132
3
0.079
4
0.053
5
0.053
GUESS STRATEGY2
Move
Move Probability
0
1
1
0
2
0
3
0
4
0
5
0
GUESS STRATEGY3
Move
Move Probability
0
0
1
0.091
2
0.091
3
0.455
4
0.182
5
0.182
Usually, a strategy emerges where one dominates: (e.g. 77% win, 23% loss,1.3 expected utility), another has moderate-large losses (23% win, 77% loss,-0.3 expected utility) and the last has guaranteed losses. &amp;lt;em&amp;gt;This is surprisingly reflective of reality &amp;#128541;&amp;#128532;&amp;lt;/em&amp;gt;
Another strategy would be if there was some device which sampled uniformly from a joint probability distribution after eliminating any ties. This would have expectation equal to the game for all, that of zero, matching the Nash equilibrium. No one would have any incentive to deviate knowing that everyone was playing according to the suggestions of the device. Note that knowing my number also tells me something about what the others saw: if I get a 2, I know that no one else received that number. Something else worth noting is that part of this game&amp;#39;s problem is it&amp;#39;s zero sum. If the game was not zero sum, the outcomes could sometimes be even better than Nash.
Code &amp;lt;a href=&amp;quot;https://gist.github.com/sir-deenicus/2ac6a2a064f4a82e09d81e60156d8915&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt;:
This example is a simple game where two players are at an intersection. If they both GO &amp;#128665; they both suffer a utility of -100. If one drives and the other STOPs &amp;#128721; then the goer gets a utility of 1 and the other 0. If they both &amp;#128721; then they both get a utility of 0. This is no longer a zero sum game. The regret matching algorithm learns the following strategy:
Move
Move Probability
A &amp;#128721;
0.99
A &amp;#128665;
0.01
This is pretty lame since there is a 0.01% chance of a catastrophic incident. The expected utility is ~zero for both players. If we try regret matching on two players that try to coordinate we get:
DRIVE STRATEGY
Move
Move Probability
A &amp;#128721;
0
A &amp;#128665;
1
DRIVE STRATEGY2
Move
Move Probability
A &amp;#128721;
1
A &amp;#128665;
0
In this scenario one player always goes and the other always stops.  Thus one player gets a utility of 1 per round while the other always gets 0 but there is a 0% chance of a crash. While not ideal for one player, the situation is overall better than the Nash equilibrium strategy which has an expected utility of 0 for both players. Notice that without some way to independently and reliably coordinate, it is not possible for the pair to do better than the above.
If instead we optimize over the joint space of actions we get:
DRIVE STRATEGY
Move
Move Probability
(A &amp;#128721;, A &amp;#128721;)
0
(A &amp;#128721;, A &amp;#128665;)
0.5
(A &amp;#128665;, A &amp;#128721;)
0.5
(A &amp;#128665;, A &amp;#128665;)
0
This is a correlated equilibrium and is fair for all parties, with both experiencing a positive utility. Yet there is a sense in which this result is unsatisfying. It is rare that we can compute or even know what joint probability distribution is being sample from, to speak of how to properly compute a reward on it. But as I mentioned above, agents working independently can sometimes learn to do this.
In &amp;lt;a href=&amp;quot;https://ideone.com/qUXG27&amp;quot;&amp;gt;this (scroll down to bottom to see results)&amp;lt;/a&amp;gt; modification to the driving game, I use Multiplicative weights update with two layers of weights. The first layer of weights computes the cost of adhering to the signal or listening to an inner expert. The inner expert learns regret from playing either &amp;#128721; or &amp;#128665;. Thus the agent has the ability to either heed the signal or do as it pleases. To make things interesting I vary the reliability of the signal. A signal with &amp;lt;strong&amp;gt;90% reliability&amp;lt;/strong&amp;gt; does as it pleases 10% of the time. In this case the agents learn to ignore the signal and settle on a strategy equivalent to when there is no signal (one always goes and the other always stops):
&amp;lt;strong&amp;gt;Agent1&amp;lt;/strong&amp;gt;: [&amp;quot;&amp;#128678;: 0.0%&amp;quot;; &amp;quot;&amp;#128527;: 100.0% [(A &amp;#128721;, 0.0); (A &amp;#128665;, 1.0)]&amp;quot;],
&amp;lt;strong&amp;gt;Agent2&amp;lt;/strong&amp;gt;: [&amp;quot;&amp;#128678;: 0.0%&amp;quot;; &amp;quot;&amp;#128527;: 100.0% [(A &amp;#128721;, 1.0); (A &amp;#128665;, 0.0)]&amp;quot;]
Sometimes however, one agent will choose to heed the signal while the other chooses to always stop:
[&amp;quot;&amp;#128678;: 0.0%&amp;quot;; &amp;quot;&amp;#128527;: 100.0% [(A &amp;#128721;, 1.0); (A &amp;#128665;, 0.0)]&amp;quot;],
[&amp;quot;&amp;#128678;: 99.44%&amp;quot;; &amp;quot;&amp;#128527;: 0.56% [(A &amp;#128721;, 0.4); (A &amp;#128665;, 0.6)]&amp;quot;]
It takes about 99.9% reliability for the agents to consistently choose a strategy that goes along with the signal, matching the correlated equilibrium.
[&amp;quot;&amp;#128678;: 99.68%&amp;quot;; &amp;quot;&amp;#128527;: 0.32% [(A &amp;#128721;, 1.0); (A &amp;#128665;, 0.0)]&amp;quot;],
[&amp;quot;&amp;#128678;: 99.72%&amp;quot;; &amp;quot;&amp;#128527;: 0.28% [(A &amp;#128721;, 1.0); (A &amp;#128665;, 0.0)]&amp;quot;]
This result still feels unsatisfactory since one might argue that there is too much hand-holding. Therefore, in &amp;lt;a href=&amp;quot;https://ideone.com/Tn3LeS&amp;quot;&amp;gt;this version&amp;lt;/a&amp;gt;, I decided to make a learner that must learn how to read the signals &amp;lt;em&amp;gt;and&amp;lt;/em&amp;gt; how to coordinate. The hand holding is minimal and typical of features in Machine learning algorithms. The system is inefficient in that it tracks all possible combinations—this is clearly untenable for more players,signals, and granularity. However, it doesn&amp;#39;t take much work to modify the system to explore adding  and removing rules in an attempt to learn a Set Cover over rules. But generally, one of the hardest parts of machine learning is getting efficient representations of large or complex state spaces.
In this model, any rule that matches the current situation is filtered to, after which weights are normalized for this reduced space. An action is taken and then a reward is used to calculate regret on only the active rules. This system is richer than all the previous and so allows us to explore an interesting few scenarios. The game is also modified as such: a player acts first and then the second player decides what to do based on what the first player does.
If we have the first player choose an action without looking for a signal and then have the second player decide based on the first the following major rule pair is arrived at:
[(1.0, &amp;quot;if signal=Do &amp;#128721; &amp;amp;amp;&amp;amp;amp; other=Do &amp;#128721; then Do &amp;#128665;&amp;quot;)],
[(1.0, &amp;quot;if signal=Do &amp;#128721; &amp;amp;amp;&amp;amp;amp; other=Do &amp;#128721; then Do &amp;#128665;&amp;quot;)]
Essentially, only drive if the other driver stops. Because of how the system filters matching rules, missing values end up such that only one value is looked at but this ends up including rules which don&amp;#39;t make sense when the full conjunction is looked at. One fix would be to include an additional state representing &amp;lt;em&amp;gt;none&amp;lt;/em&amp;gt;, but that&amp;#39;s not necessary for demonstration purposes. Nonetheless, the agents work around that issue and learn a useful set of rules. Sampling some interactions we see that most of the time the first car will do whatever (usually drive) and the second one will drive or stop depending on the other. We see that the situation learned is better than the experts above without access to a signal. The first player dominates but the second player still sees a positive reward.
[((Do &amp;#128721;, Do &amp;#128721;), 0.1); ((Do &amp;#128721;, Do &amp;#128665;), 0.36); ((Do &amp;#128665;, Do &amp;#128721;), 0.54);((Do &amp;#128665;, Do &amp;#128665;), 0.0)]
If we instead provide a signal for what the other will do as the light they see (the system can&amp;#39;t reason after all), the following rule pair is matched:
[(1.0, &amp;quot;if signal=Do &amp;#128665; &amp;amp;amp;&amp;amp;amp; other=Do &amp;#128721; then Do &amp;#128665;&amp;quot;)],
[(1.0, &amp;quot;if signal=Do &amp;#128665; &amp;amp;amp;&amp;amp;amp; other=Do &amp;#128721; then Do &amp;#128665;&amp;quot;)]
And we see that their behavior is converging on a correlated equilibrium:
[((Do &amp;#128721;, Do &amp;#128665;), 0.49); ((Do &amp;#128665;, Do &amp;#128721;), 0.5); ((Do &amp;#128665;, Do &amp;#128665;), 0.0)|]
On the other hand, if we take away the ability for the agents to attend to what the others will do they will tend to just stop and learn a more complicated set of rules leading to statistics like:
[((Do &amp;#128721;, Do &amp;#128721;), 0.97); ((Do &amp;#128721;, Do &amp;#128665;), 0.03); ((Do &amp;#128665;, Do &amp;#128721;), 0.0)]
In essence, so long as you can jointly learn to attend to signals, and so long as there is some shared memory to draw upon (history, text) then many learning agents can efficiently converge on a correlated equilibrium, bottom up. Without any top-down enforcement of joint distributions. In the absence of anything to condition and coordinate actions by, coarse equilibria (which I expect many short term—within the life of an individual—scenarios share much in common with) can be subpar.
&amp;lt;img src=&amp;quot;Images/balifarms.jpg&amp;quot; alt=&amp;quot;bali farms&amp;quot;&amp;gt;
As an example, I think the historical knowledge of Bali Rice Farmers [1], with insects and weather as the coordination signal lead to a bottom up correlated equilibrium of planting patterns.
So far (excepting the last rule based AI scenario), the focus has only been on Normal Form (no turn taking) games but the story for imperfect information extensive (multiple turns) games is not that much more complicated. And going to repeated or iterated games is no trouble at all. It&amp;#39;s only a handful more lines to be able to handle counterfactual scenarios. This is in contrast to the study by game theory where the complexity suddenly ratchets. In fact, the methods are so complicated that only simpler toy problems can be studied. However, with learning algorithms that have been proven to converge on useful equilibria concepts, we can design scenarios and look at what happens to study multiplayer, repeated, stochastic, imperfect information extensive games. As I&amp;#39;ve not yet had need to venture into those areas, my knowledge there is limited and only of a sketch like nature.
Compared to poker the state space is very large and it&amp;#39;s difficult to imagine how the notion of information sets will apply here. Rounds are ill define and there are really no such things as turns. Within each game there are many interactions that aren&amp;#39;t zero sum. Learning a precomputed nash equilibrium strategy simply isn&amp;#39;t tenable. Especially if we want an AI that can do just as well in multiplayer scenarios with no additional fuss. Starcraft is complex enough that a low power, low data solution stands a chance, I think.


In this essay I talked about Nash Equilibria and how it is impossible to &amp;lt;em&amp;gt;lose&amp;lt;/em&amp;gt; against an optimal player in some zero sum 2 player games. I then talked about why some 0-sum 2 player games allow for losses. I mentioned the likely intractability of Nash Equilibria and many market economy concepts. I also discussed computational complexity and in particular, the PPAD complexity of Nash Equilibria and what that roughly means. I also mentioned correlated equilibria and how they are common, efficiently computable and often lead to more preferable outcomes than Nash Equilibria.
I noted that coarse correlated equilibria are likely most common given the lack of signals to coordinate on in many real world situations. These can lead to positions for many players that are at least weakly dominated and therefore highly not preferable.
Correlated Equilibria can be learned so long as multiple agents can learn to attend to the conjunction of certain patterns, together with some memory or shared history, to reach a point where there is no incentive to deviate. I motivate min-regret learning as a way to achieve correlated equilibria using a modification to prediction markets as an example.
I mention that barring a simple hack for Starcraft, it poses a much more difficult problem than Poker or Go. It&amp;#39;s interesting to see aspects of Moravec Paradox reflected here too. It takes more effort for a computer to be better than a human in Starcraft than at Chess or &amp;lt;em&amp;gt;Go&amp;lt;/em&amp;gt;. Despite Starcraft being considered the less intellectual game.
The full article contains links to code that you can run in your browser to explore simple non-deterministic, zero and non-zero sum games. Namely modifications of Rock-Paper-Scissors, a game where two drivers can either both stop or drive or some variation thereof. Finally, a game where 1 of 3 people has to guess a number that&amp;#39;s less than or unique compared to what everyone else guesses. I also demonstrate a rule learning AI using regret to learn in matched subspaces, how to use signals and then coordinate under minimal handholding to show how learners might achieve a correlated equilibrium.
[1] &amp;lt;a href=&amp;quot;http://www.pnas.org/content/114/25/6504.full.pdf&amp;quot;&amp;gt;http://www.pnas.org/content/114/25/6504.full.pdf&amp;lt;/a&amp;gt;
[2] &amp;lt;a href=&amp;quot;http://www.cis.upenn.edu/~aaroth/courses/slides/agt17/lect08.pdf&amp;quot;&amp;gt;http://www.cis.upenn.edu/~aaroth/courses/slides/agt17/lect08.pdf&amp;lt;/a&amp;gt;
[3] &amp;lt;a href=&amp;quot;http://modelai.gettysburg.edu/2013/cfr/cfr.pdf&amp;quot;&amp;gt;http://modelai.gettysburg.edu/2013/cfr/cfr.pdf&amp;lt;/a&amp;gt;</description>
    </item>
    <item>
      <title>RNNs are probably not practically Turing Complete.</title>
      <link>RNNs_are_probably_not_practically_Turing_Complete..htm</link>
      <guid>RNNs_are_probably_not_practically_Turing_Complete..htm</guid>
      <pubDate>Fri, 18 Aug 2017 09:53:45 GMT</pubDate>
      <description>&amp;lt;img src=&amp;quot;Images/Ca110-structures2.png&amp;quot; alt=&amp;quot;Rule 110&amp;quot;&amp;gt;
Are RNNs Turing Complete? Is a question that has bothered me for some time. I still do not have an air tight resolution but my general belief is no, not really.
This issue came to the fore today for me, when the author of Keras (Fran&amp;#231;ois Chollet‏), a popular wrapper atop the Tensorflow deep learning library, &amp;lt;a href=&amp;quot;https://twitter.com/fchollet/status/887709906559741952&amp;quot;&amp;gt;stated&amp;lt;/a&amp;gt; that matrix multiplication is insufficient to capture general intelligence. He also made two confusing statements: Long Term Short Term Memory Networks are not Turing complete but ConvLSTMs are. Recurrent Neural networks have been proven to be Turing complete and LSTMs generalize these, therefore LSTMs should also be Turing complete (the LSTM should learn not to use the &amp;quot;forget&amp;quot; gate if it is getting in the way).
Convolutional nets are a simplification of Multilayer Neural networks—that is, they invoke several structural biases such as translational invariance and how to coarse grain to reduce the number of parameters to be learned—therefore, it is difficult to see what they could add to LSTMs that they lost in comparison to RNNs. This is answered in [2], where the authors point out a close correspondence between &amp;lt;em&amp;gt;Neural GPUs&amp;lt;/em&amp;gt; and Convolutional LSTMs. Neural GPUs extend Gated Recurrent Units (a simplification of LSTMs) to operate on state that is a 2D grid of vectors, which are then convolved over across time steps. This allows for more parallel computation and efficient implementations.
&amp;lt;img src=&amp;quot;Images/neural-gpu.png&amp;quot; alt=&amp;quot;neural gpu&amp;quot;&amp;gt;
They draw a correspondence with 2D discrete cellular automata, as a class with similar computational power. While it seems likely that Turing Complete algorithms can be less elaborately represented in this architecture, the learnability of more complex many step algorithms remains strongly in doubt. They mention difficulty multiplying decimal numbers as well as scaling to larger instance sizes of say, binary addition problems.
Those trivialities aside, I actually agree with his statement from the tweet:
&amp;lt;img src=&amp;quot;Images/DFHI5hfVYAA2tTS.jpg&amp;quot; alt=&amp;quot;Snippet showing a quote from Chollet&amp;#39;s book:&amp;quot;&amp;gt;
Before discussing his important and very likely true point, it&amp;#39;s worth looking at the proof of the Turing Completeness of RNNs and why I don&amp;#39;t think it&amp;#39;s of practical consequence.
I found the proof difficult to unpack—it&amp;#39;s dense, there&amp;#39;s lots of notation and even outmoded terminology. It seems like it would take me days to fully understand it. But I could extract a sketch:
It is known that two Push Down Automata (a Finite state machine extended with a stack) to simulate both sides of a Turing Machine, are Turing Equivalent. The proof then sets up a dynamical system which can simulate two PDAs. The task is to simulate that system with a neural net. At this point, it&amp;#39;s worth pointing out as the paper does, that Minsky proves the existence of a Universal Turing Machine with just 7 control states and a 4 letter tape. Thus, their construction only needs a relatively small number of states to be universal.
To complete the proof, they show the composition of a series of functions (defined over the rationals) computes the dynamical system of interest. Their recurrent neural network is shown to match this. However, the construction is very particular. For example one layer computes:
&amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\(\sigma(4q_i - 2\zeta[q_i] - 1 + \sum\limits_{j=0}^s \sum\limits_{i=1}^{16} c_{ij}\sigma(v_i \cdot\mu) - 1)\)&amp;lt;/span&amp;gt;
where ζ is a thresholding function over the subset of rationals expressible as a particular kind of Cantor set. The method also seems to need some post processing in the form of re-ordering and such like. It&amp;#39;s an elegant argument (which I don&amp;#39;t even fully understand), but what is clear is that this recurrent net bears little resemblance to systems as used in practice. I daresay it&amp;#39;s closer to cellular automata than LSTMs. For example, suppose I showed you a rule 110 automata and said: &amp;quot;you know, in principle it is possible to encode an AI in this flashing pattern of black and white dots that would want to harvest humans for carbons&amp;quot;.  You&amp;#39;d probably say &amp;quot;neat!&amp;quot; and promptly forget. It&amp;#39;s not the kind of observation that comes with any practical consequence.
Another (and the easiest) objection to make is that the proof requires &amp;lt;em&amp;gt;rational&amp;lt;/em&amp;gt; weights while most neural networks utilize 32 bit (and soon 16 bit) floating point numbers as weights. But a similarly easy objection is that the neural nets can learn a good enough approximation. I doubt it—the rationals require a particular and precise form, else the memory would not be dependable. The nets would need to learn corrections atop the structure and stack operations. This seems unlikely.
The other objection is there&amp;#39;s no known method to learn on these nor any guarantee that your training will ever converge on such an architecture. Simulated annealing over arbitrary network architectures might get there first—and it only converges giving an infinite amount of time. A response in turn might be that the proof is not necessarily the only form a Turing Complete RNN can take. The proof only shows that such an encoding is possible and it might therefore be feasible that RNNs could arrive at easier representations. This is plausible and it leads me to my third objection, pointed out and explained &amp;lt;a href=&amp;quot;https://www.youtube.com/watch?v=FIr_SaKT52U&amp;quot;&amp;gt;here&amp;lt;/a&amp;gt; by Edward Grefenstette. The memory vector has finite state and newer inputs saturate such that RNNs only learn short ranged correlations and are not much more powerful than a Finite State machine.
Most sequence to sequence architectures come in encoder/decoder pairs. Encoders are folds and decoders are unfolds. A fold is a function that structurally recurses over and deconstructs an input.
In the last example, the s stands for the string built so far and would correspond to the &amp;quot;hidden state&amp;quot; or memory of an RNN.
An RNN is:
(&amp;lt;code&amp;gt;A List of some kind&amp;lt;/code&amp;gt;) ▷ &amp;lt;strong&amp;gt;fold&amp;lt;/strong&amp;gt; (&amp;lt;em&amp;gt;(h,x)&amp;lt;/em&amp;gt; ⇒ &amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\(W_{hx} x + W_{hh} h + b_h\)&amp;lt;/span&amp;gt; ▷ &amp;lt;strong&amp;gt;Vector.map&amp;lt;/strong&amp;gt; tanh) [0,0,...,0]
&amp;lt;em&amp;gt;Wxh&amp;lt;/em&amp;gt; and &amp;lt;em&amp;gt;Whh&amp;lt;/em&amp;gt; are matrices which together represent whatever function so far best minimizes error with respect to some loss function. If the function above was calculated with respect to say, dual numbers—automatic differentiation—then one gets derivatives for free, which one can then use to adjust the weights; hence, learning. There is a great deal more plumbing to actually make things work but the above captures the essence.
The decoder is more of an unfold, which constructs possibly an infinite stream given some initial state:
&amp;lt;strong&amp;gt;unfold&amp;lt;/strong&amp;gt; (&amp;lt;em&amp;gt;state&amp;lt;/em&amp;gt; -&amp;gt; &amp;lt;strong&amp;gt;Maybe&amp;lt;/strong&amp;gt;(Output,state&amp;#39;)) initial_state
Where the maybe terminates given an appropriate condition. Encode/Decode, such as in language translation (encode english, decode japanese) then becomes a &amp;lt;em&amp;gt;fold&amp;lt;/em&amp;gt; ∘ &amp;lt;em&amp;gt;unfold&amp;lt;/em&amp;gt; style computation.
Since the internals are just affine matrix transformations and the general structure is one of hylomorphisms, I think RNNs are at most learning total functions. The (encoder) RNN is searching for a function like (+), that combines the current state with the new input. This is a good thing—learning arbitrary programs would make training undecidable (humans run some parts in parallel—it&amp;#39;s feasible for a human to spend their entire runtime searching for structural regularities—with those processes never terminating, until a segfault).
If we bring limited memory back into the picture, we see that the number of bits representable by state is limited by the size of the vector. Since the state is degraded for distant inputs, the search will focus on functions that depend on a relatively short history. As a rough analogy: in the above where we were &amp;lt;em&amp;gt;fold&amp;lt;/em&amp;gt;ing over the characters of a word, imagine instead if the string was of limited size and we wanted to build patterns based on the previous patterns. We would be limited in what we could express.
The LSTM makes the best out of a crappy situation by learning what to evict and what to keep, learning longer (but still limited) range correlations. LSTMs might be close to Pushdown automata with a faulty stack. So RNNs I think, while probably in principle (co)inductive, learn only a subset of that class of functions. The subset which are smooth and well enough behaved and that also satisfy the low weight prior of stochastic gradient descent.
RNNs don&amp;#39;t just have to learn good representations, they also have to learn operations and algorithmic transformations on the data. This means trading feature engineering time for more Joules and longer, more complicated training regiments.
The combination of few assumptions, powerful model class and large complicated space all but guarantees the requirement of a large amount of data and energy spend. Several steps have to be learned to make progress and this quickly gets very complicated by the time high level concepts are the target. This, in my opinion is wasteful—even humans come with certain inductive biases. Finding the right combination of not overly specific hand engineered features seems like one of the many things which will need to be discovered to arrive at more powerful learners.
All together, the above highlight my strong skepticism of the Turing completeness of RNNs. The worries that they&amp;#39;ll replace everything, including humans, by 2025 are verging on nonsensical.
[1] &amp;lt;a href=&amp;quot;https://pdfs.semanticscholar.org/1759/4df98c222217a11510dd454ba52a5a737378.pdf&amp;quot;&amp;gt;https://pdfs.semanticscholar.org/1759/4df98c222217a11510dd454ba52a5a737378.pdf&amp;lt;/a&amp;gt;
[2] &amp;lt;a href=&amp;quot;https://arxiv.org/pdf/1511.08228.pdf&amp;quot;&amp;gt;https://arxiv.org/pdf/1511.08228.pdf&amp;lt;/a&amp;gt;</description>
    </item>
    <item>
      <title>On N Year Floods</title>
      <link>On_N_Year_Floods.htm</link>
      <guid>On_N_Year_Floods.htm</guid>
      <pubDate>Mon, 04 Sep 2017 00:00:00 GMT</pubDate>
      <description>I think several things about this &amp;lt;a href=&amp;quot;https://www.youtube.com/watch?v=EACkiMRT0pc&amp;quot;&amp;gt;video&amp;lt;/a&amp;gt;. The first is, a lot of people commit the gambler&amp;#39;s fallacy when talking about 100 year or 500 year floods. On twitter, someone thought a 500 year flood was due (100% and guaranteed by the 500th year). But that&amp;#39;s not true. A better way is to look in terms of a binomial distribution. This takes each year as a trial of a Bernoulli. For a 500 year span, we compute P = Bin(1/500, 500) in n+1, 1 - P(X ≤ n) for n in 0..3 as:
Chance of 1+ occurrences is ~63%
Chance of 2+ occurrences is ~26%
Chance of 3+ occurrences is ~8%
Chance of 4+ occurrences is ~2%
5+ is negligible. But even for a 500 year flood, a short time interval (better think a few number of trials), say 10 years (trials), the probability of 1+ successes is non-negligible.
Chance of 1+ occurrences is ~1.98%
Chance of 2+ occurrences is ~0.02%
If we look at a &amp;quot;100 year flood&amp;quot; for a span of 10 trials (years), it&amp;#39;s:
Chance of 1+ occurrences is ~9.56%
Chance of 2+ occurrences is ~0.43%
Chance of 3+ occurrences is ~0.01%
But I find not specifying any notion of uncertainty really dissatisfying. A better way to do this would be to specify a prior such as &amp;lt;em&amp;gt;Beta(1,499)&amp;lt;/em&amp;gt; and then for each year, update with the number of occurrences of 500 year floods for the region. Over time, the distribution will shift, narrow or broaden reflecting changes in our uncertainty about what the parameter for our Bernoulli distribution should be.
&amp;lt;img src=&amp;quot;Images/flood.png&amp;quot; alt=&amp;quot;distribution shift&amp;quot;&amp;gt;
Better yet, would be to model rainfall volumes directly, with say a log normal distribution (the video had a short snippet on normals but I assume this is due to their familiarity, rain fall is probably better modeled as log normal [1]). Then I might ask &amp;quot;what&amp;#39;s the probability of seeing such an amount?&amp;quot; Again, I&amp;#39;d specify a prior over possible values for the parameters of the distribution to capture my uncertainty. With another model for the frequency of rain fall, I could then talk about N year floods except I would also be able to &amp;lt;em&amp;gt;cleanly&amp;lt;/em&amp;gt; talk about N months, N day floods etc. And most importantly, I&amp;#39;d be able to parameterize my uncertainty. The upper end of the distribution could be used to plan for worst case scenarios instead of misleading people with useless point estimates.
I expect that over time, the number of formerly rare events will increase. The link between CO2 and the greenhouse effect has been known since at least the early 1900s. With melting poles, increased moisture and higher ocean temperatures, we can expect better fed hurricanes and tropical storms. Floods, heat waves, droughts and forest fires will be more common too. The general pattern of weather in places will shift as a result of change in the &amp;quot;parameters&amp;quot; deciding climate. This in turn will make weather prediction more difficult.
Tornadoes, hailstorms and even snowstorms—can those be linked to warming? The default answer would be to state or even over-state our uncertainty and bemoan our lack of data. I think that&amp;#39;s wrong, optimizing for average case correctness is not always ideal. When lives and livelihoods are at stake, it&amp;#39;s better to &amp;lt;em&amp;gt;minimize regret and plan for the worst&amp;lt;/em&amp;gt;. Therefore, if ever I&amp;#39;m asked is this because of climate change? My answer will be yes &amp;lt;em&amp;gt;and&amp;lt;/em&amp;gt; summarizing the previous recent paragraphs. Then I&amp;#39;d talk about what needs to be done, so people don&amp;#39;t forcefully forget about it in hopeless despair.
[1] &amp;lt;a href=&amp;quot;http://journals.ametsoc.org/doi/pdf/10.1175/1520-0450%281976%29015&amp;amp;lt;0205%3ACMARS&amp;amp;gt;2.0.CO%3B2&amp;quot;&amp;gt;http://journals.ametsoc.org/doi/pdf/10.1175/1520-0450%281976%29015&amp;lt;0205%3acmars&amp;gt;2.0.CO%3B2&amp;lt;/0205%3acmars&amp;gt;&amp;lt;/a&amp;gt;</description>
    </item>
    <item>
      <title>A High Level Introduction to Differential Privacy and Information</title>
      <link>A_High_Level_Introduction_to_Differential_Privacy_and_Information.html</link>
      <guid>A_High_Level_Introduction_to_Differential_Privacy_and_Information.html</guid>
      <pubDate>Tue, 26 Sep 2017 22:49:39 GMT</pubDate>
      <description>I&amp;#39;ve known of the term &amp;lt;em&amp;gt;differential privacy&amp;lt;/em&amp;gt; for a long while, but only vaguely. Recently, I decided to look a bit more into the topic and also thought it a good place to start/try out interactive explanations. As it turns out, differential privacy is essentially about probabilities and information, which means an excuse to experiment with interactive explanations of relevant areas from probability theory (and an excuse to play with a discrete probability monad).
Of course, there is a great deal more to the subject of differential privacy than I can cover (or looked into) but I think I am satisfied with this as providing a decent high level overview.
One early precursor to DP is the method of randomized response. Proposed by S. L. Warner in 1965 [1], it&amp;#39;s a method of confidentially surveying a population.
Suppose you were surveying the population about something controversial and wanted to do so in a manner allowing plausible deniability. You could use the following procedure:
Flip a coin, if it&amp;#39;s heads, the responder must answer truthfully and if it&amp;#39;s tails they must answer &amp;lt;em&amp;gt;yes&amp;lt;/em&amp;gt;. Note that this leaks some information about the responder: if they answer &amp;lt;em&amp;gt;no&amp;lt;/em&amp;gt; then you know that they definitely have not performed said action. If they answer, &amp;lt;em&amp;gt;yes&amp;lt;/em&amp;gt;, however, you have no way (at that instance in time) of distinguishing between whether it was truthful or the result of a coin flip. But across the entire population, since you control the procedure, you can work backwards to get the true distribution.
Suppose for example, you were surveying individuals about whether they love or hate bacon&amp;#129363;. They flip a coin and if it&amp;#39;s heads they answer truthfully. If it&amp;#39;s tails, they must say they &amp;lt;em&amp;gt;Hate Bacon&amp;lt;/em&amp;gt;. Using this procedure, the surveyed number of those that love bacon is always ~half the true number in the population. This is because, for bacon loving responses, all the results are true but only get reached half the time. And for &amp;lt;em&amp;gt;bacon hating&amp;lt;/em&amp;gt; answers (the protected class), half the time, the answers were truthful while the other half were &amp;lt;em&amp;gt;I love bacon answers&amp;lt;/em&amp;gt; converted to &amp;lt;em&amp;gt;I hate the consumption of bacon&amp;lt;/em&amp;gt; answers.
In the example below, you can &amp;lt;strong&amp;gt;adjust the slider&amp;lt;/strong&amp;gt; to see how the surveyed numbers change.
True Proportion of Population that &amp;lt;em&amp;gt;Hates Bacon&amp;lt;/em&amp;gt;:
In the below, assume &amp;lt;em&amp;gt;p&amp;lt;/em&amp;gt; is the true proportion that hates bacon. Then:
&amp;lt;em&amp;gt;p&amp;lt;/em&amp;gt; = &amp;lt;span name=&amp;quot;rrp&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;
&amp;lt;strong&amp;gt;Like Bacon&amp;lt;/strong&amp;gt;: 0.5 * (1 - &amp;lt;span name=&amp;quot;rrp&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;) = &amp;lt;span id=&amp;quot;rra1&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;
&amp;lt;strong&amp;gt;Hate Bacon&amp;lt;/strong&amp;gt;: 0.5 + 0.5 * &amp;lt;span name=&amp;quot;rrp&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;  = &amp;lt;span id=&amp;quot;rra2&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;
&amp;lt;strong&amp;gt;True Against&amp;lt;/strong&amp;gt;: 2 * ((q=&amp;lt;span id=&amp;quot;rrq&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;) - 0.5) = &amp;lt;span id=&amp;quot;rra3&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;
Which you can subtract from 1 to get the proportion that enjoys bacon. If none of this makes sense, play with the slider and it should start to.
Something to note is that if some (ahem, barbaric) human says they love bacon, you definitely know they are speaking the truth (the &amp;lt;em&amp;gt;End Bacon Now&amp;lt;/em&amp;gt; controversial but clearly more appropriate true belief is protected). Suppose we wanted to adjust this to be more anonymous?
Differential Privacy was initially expanded upon and given a solid mathematical footing by the prolific computer scientist/cryptographer Cynthia Dwork. It is a large field so we&amp;#39;ll only be taking a broad overview of it.
In the example for this section, we&amp;#39;ll be surveying people about their favorite sandwich. To keep things simple we&amp;#39;ll assume the true preferences of sandwiches are:
Best Sandwich
Share of Favorites
Hotdog &amp;#127789;
10%
Sandwich &amp;#129366;
30%
Vegan Hamburger&amp;#127828;
60%
How to tally votes without risking shame or ridicule for your belief that hotdogs are the best sandwich? A simple modification of randomized response allows for this. This time we don&amp;#39;t demand a specific answer--if the coin lands heads you speak truthfully but if it lands on tails, you sample uniformly (choose randomly) from among the choices. We can also allow the coin to be loaded or weighted. For example, we can use a coin that comes up heads 1% of the time. As long as we are only interested in population level things, despite the high levels of randomization, we can fully recover the original proportions.
With some algebra, I was able to work out that computing the following, for each possible answer recovers the true underlying percentages:
  &amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\[p_{true} = \frac{p_{survey} - \frac{1}{|C|}(1 - p_{heads})}{p_{heads}}\]&amp;lt;/span&amp;gt;
Where |&amp;lt;em&amp;gt;C&amp;lt;/em&amp;gt;| stands for total number of choices in the set &amp;lt;em&amp;gt;C&amp;lt;/em&amp;gt; = {choice&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt;,..,choice&amp;lt;sub&amp;gt;n&amp;lt;/sub&amp;gt;}. This time, the slider will control how biased our coin is.
&amp;lt;strong&amp;gt;Coin Bias: &amp;lt;/strong&amp;gt;
  &amp;lt;input type=&amp;quot;range&amp;quot; name=&amp;quot;points2&amp;quot; value=&amp;quot;40&amp;quot; min=&amp;quot;0&amp;quot; max=&amp;quot;100&amp;quot; data-show-value=&amp;quot;true&amp;quot; onchange=&amp;quot;foodSliderChanged(this)&amp;quot;&amp;gt; &amp;lt;span name=&amp;quot;fbias&amp;quot;&amp;gt;40%&amp;lt;/span&amp;gt;
  
&amp;lt;strong&amp;gt;Coin Bias: &amp;lt;/strong&amp;gt;
  &amp;lt;input type=&amp;quot;range&amp;quot; name=&amp;quot;points2&amp;quot; value=&amp;quot;40&amp;quot; min=&amp;quot;0&amp;quot; max=&amp;quot;100&amp;quot; data-show-value=&amp;quot;true&amp;quot; onchange=&amp;quot;foodSliderChanged(this)&amp;quot;&amp;gt; &amp;lt;span name=&amp;quot;fbias&amp;quot;&amp;gt;40%&amp;lt;/span&amp;gt;
  
Differential Privacy is not an impenetrable seal of protection; it is possible to introduce leaks. Two ways that I could think of are attacks involving remembering queries and by asking multiple correlated questions.
If the queries do not retain any data on what each individual response was, privacy remains protected. If instead the responses were recorded, the collector can revisit the data to make new inferences. For example, suppose we were surveying whether people were for or against some action and that against is the protected class. After the population estimates of the proportions have been worked out, one can condition to just those who said against and work out the probability that those who said against truly are against.
In our randomized response scenario, if the proportion of the population that is against is 41%, the probability that those who answered against truly are against is ~59%. With the second differential privacy method, if it were 36% against at the population level, then those responding against are truly against with a 63% chance. This is a large change in probability! However, if a biased coin was instead used, say one that turns up tails 95% of the time, the worst case scenario would only involve going from 49% to 51%. The population level true values are still as precise but the individuals are much more protected.
The amount of information leaked depends on the underlying population probability and increases from zero and then decreases. Here&amp;#39;s a graph for the randomized response scenario:
As you can see, if the purpose is to secure the privacy of individual responses, then retaining the data of responses is subideal, especially when 30%-60% of the populace is against. If the results are to be retained, we can at least demand a high bias or a low probability of requiring a truthful response (most differential privacy work is biased towards the concerns of the data collector so they might not agree with my suggestion).
Another manner where which the implementer can cheat is by retaining responses and querying with either the same or a very similar set of questions. If the survey giver keeps asking the same questions, they can get ever more confident as to the true value of the responses. But that is not the only way to act in bad faith. If the survey process constructs different questions whose responses are correlated, they can become fairly certain about true answers in just two queries (or the first if enough different questions are asked).
In our final scenario, we will visit a world of dogs, mice and cats ruled by fat cats. The Fat Cats are performing what is ostensibly a demographic survey. To respect the right to anonymity of the denizens, they tell everyone they&amp;#39;re implementing differential privacy. Those extra few probing questions? To provide better services, they say. In actuality, they want to figure out who to increase insurance premiums for (you see, dogs are much too playful and mice keep getting injured by cats).
We will take the perspective of a single animal being queried. In addition to &amp;lt;i&amp;gt;&amp;quot;which species are you&amp;quot;&amp;lt;/i&amp;gt;, we will also ask: &amp;lt;i&amp;gt;what is your favorite food (fish, meat or cheese) and what is your favorite toy (bone, yarn,egg carton or cardboard box)?&amp;lt;/i&amp;gt; There is a predictor, a bayesian, that doesn&amp;#39;t get to see our selected species. We simulate it asking questions each time the button is pressed (you can also think of it as different phrasings each time). The Fat Cats are sly and their coin is slightly rigged--52% of the time it requires truth and 48% of the time allows randomization. Directly below (for comparison) we will also simulate our change in predicted species from asking the &amp;lt;em&amp;gt;same single&amp;lt;/em&amp;gt; question of &amp;lt;em&amp;gt;are you a dog or cat or mouse?&amp;lt;/em&amp;gt; a number of times equal to button presses.
Click the &amp;lt;em&amp;gt;query&amp;lt;/em&amp;gt; button to see how our bayesian changes its confidence in its predictions.
&amp;lt;strong&amp;gt;Select your species&amp;lt;/strong&amp;gt;:
  &amp;lt;select id=&amp;quot;species&amp;quot; onchange=&amp;quot;resetGuesses()&amp;quot;&amp;gt;
    &amp;lt;option value=&amp;quot;cat&amp;quot;&amp;gt;cat&amp;#128008;
    &amp;lt;option value=&amp;quot;dog&amp;quot;&amp;gt;dog&amp;#128054;
    &amp;lt;option value=&amp;quot;mouse&amp;quot;&amp;gt;mouse&amp;#128045;
  &amp;lt;/select&amp;gt;
&amp;lt;strong&amp;gt;Relationship between questions:&amp;lt;/strong&amp;gt;
&amp;lt;input type=&amp;quot;radio&amp;quot; id=&amp;quot;corropt1&amp;quot; name=&amp;quot;corropt&amp;quot; value=&amp;quot;correlated&amp;quot; checked=&amp;quot;&amp;quot;&amp;gt;
  &amp;lt;label for=&amp;quot;corropt1&amp;quot;&amp;gt;Correlated&amp;lt;/label&amp;gt; | &amp;lt;input type=&amp;quot;radio&amp;quot; id=&amp;quot;corropt2&amp;quot; name=&amp;quot;corropt&amp;quot; value=&amp;quot;independent&amp;quot;&amp;gt;
  &amp;lt;label for=&amp;quot;corropt2&amp;quot;&amp;gt;Independent&amp;lt;/label&amp;gt;
Times asked: &amp;lt;span id=&amp;quot;nqueries&amp;quot;&amp;gt;1&amp;lt;/span&amp;gt;
    &amp;lt;input type=&amp;quot;button&amp;quot; id=&amp;quot;askq&amp;quot; value=&amp;quot;Query&amp;quot; onclick=&amp;quot;updateGuesses()&amp;quot;&amp;gt; | &amp;lt;input type=&amp;quot;button&amp;quot; id=&amp;quot;qreset&amp;quot; value=&amp;quot;Reset&amp;quot; onclick=&amp;quot;resetGuesses()&amp;quot;&amp;gt;
If you try reset a few times you might notice that mice are hardest to guess (since each of their non-food tastes share a bit with one of the other species). You might also notice that a surprising number of times, the correlator guesses correctly in one try (but can, especially for mice, fixate on the wrong species).
In this article we&amp;#39;ve covered only a small part of Differential Privacy, there remain many more sophisticated methods to inject noise and protect the user. Nonetheless, we were able to explore its core aspect. While Differential Privacy is a way to maintain privacy when data must be collected, it&amp;#39;s no panacea. If there&amp;#39;s no way to audit the process, an element of trust will always be needed. A hostile actor might technically be offering privacy but by retaining answers, using weights biased against the user, multiple identical queries (an issue in the digital world where devices can act on your behalf without your knowing the details) or designing queries so as to leverage correlations much more information than naively assumed can be leaked. All that said, properly implemented Differential Privacy strikes a balance between the needs of the user and the polling entity.
The following section is a bit more technical and assumes programming knowledge.
The likelihoods for the bayesians were chosen lazily. In particular, the correlator&amp;#39;s likelihood is not even properly sensible: it simply uses the joint probability of seeing those particular items together and so is very jumpy. Works well enough for this demonstration&amp;#39;s purposes though.
For the multiple asker:
I&amp;#39;d originally wanted to cover mutual information in the main text but realized I could not do it at the level of detail I preferred and so moved it here. Mutual information is an important concept, it&amp;#39;s definition is:
&amp;lt;span class=&amp;quot;math&amp;quot;&amp;gt;\[\sum_{(x,y)\in X \times Y} p(x,y) \log\left(\frac{p(x,y)}{p(x)p(y)}\right)\]&amp;lt;/span&amp;gt;
When &amp;lt;em&amp;gt;X&amp;lt;/em&amp;gt; and &amp;lt;em&amp;gt;Y&amp;lt;/em&amp;gt; are independent we get &amp;lt;strong&amp;gt;log(1) = 0&amp;lt;/strong&amp;gt;. But a more motivated definition is: &amp;lt;em&amp;gt;I(X;Y) = H(X) - H(X|Y)&amp;lt;/em&amp;gt; where &amp;lt;em&amp;gt;H(X)&amp;lt;/em&amp;gt; stands for the entropy or our uncertainty about the random variable &amp;lt;em&amp;gt;X&amp;lt;/em&amp;gt;. Mutual information then is, how uncertain we remain about &amp;lt;em&amp;gt;X&amp;lt;/em&amp;gt; given that we know &amp;lt;em&amp;gt;Y&amp;lt;/em&amp;gt;. If &amp;lt;em&amp;gt;X&amp;lt;/em&amp;gt; and &amp;lt;em&amp;gt;Y&amp;lt;/em&amp;gt; are independent of each other then knowing &amp;lt;em&amp;gt;Y&amp;lt;/em&amp;gt; changes nothing about our uncertainty around &amp;lt;em&amp;gt;X&amp;lt;/em&amp;gt;. But when they are correlated, knowing one tells us something and reduces our uncertainty about the other. In our Differential Privacy example, positive mutual information between the subjects of our questions allows us to narrow down and reduce our uncertainty about attributes that in principle, should have been private.
Entropy is roughly, our uncertainty about possible outcomes. We want this concept to be low when the bulk of probability is concentrated on a few outcomes and high when it&amp;#39;s diffuse. For a binary proposition, this is a function that&amp;#39;s low for low probability and high probability events (i.e. 1% means we are very certain this thing will not happen). Additionally, we want this uncertainty to change smoothly with changes in probability and to not depend on the order in which the probabilities are presented. Finally and most importantly, is the notion of coarse graining or throwing away detail (going from &amp;lt;em&amp;gt;a shiny green toy car&amp;lt;/em&amp;gt; to &amp;lt;em&amp;gt;a toy car&amp;lt;/em&amp;gt;).
If we have that the entropy at the coarse grained level is equal to that of our full level of detail minus the branching distinctions we don&amp;#39;t care about, there is essentially only one form entropy can take. That is, the entropy of our coarse graining should be less than or equal to that of the fine grained level. It is less exhausting to communicate at a high level than to finely go over pedantic distinctions (programming can be tedious).
If we have a set &amp;lt;em&amp;gt;{a,b,c}&amp;lt;/em&amp;gt;, sets &amp;lt;em&amp;gt;A = {a,b}, B = {c}&amp;lt;/em&amp;gt; and we want a function &amp;lt;em&amp;gt;H:Distribution -&amp;gt; Real&amp;lt;/em&amp;gt; such that H({a,b,c}) = H({A,B}) + (p&amp;lt;sub&amp;gt;A&amp;lt;/sub&amp;gt;H({A}={a,b}) + p&amp;lt;sub&amp;gt;B&amp;lt;/sub&amp;gt;H({B} = {c})), the function which solves this is (for discrete systems): -Σ&amp;lt;sub&amp;gt;x&amp;lt;/sub&amp;gt;(p(x) * log p(x)). In code:
Why this is has been derived in many places but to boost intuition I will expand upon our simple example. Let&amp;#39;s define:
This filters our space to matching conditions and ensures the probabilities sum to one. Let&amp;#39;s encode our example.
Notice that &amp;lt;em&amp;gt;A&amp;lt;/em&amp;gt; has an 80% chance of occurring and &amp;lt;em&amp;gt;B&amp;lt;/em&amp;gt;, a 20% chance. The entropy of our full system is ~1.48 bits and our coarse system is 0.72 bits. In code our above requirement is:
&amp;lt;img src=&amp;quot;Images/system.png&amp;quot; alt=&amp;quot;system&amp;quot;&amp;gt;
The coarse graining requirements says the fine grained entropy should equal our coarse graining and the entropy of each of the contained subsystems weighted by their probability. This makes sense but a lack of appreciation for this can lead people astray when attempting to define emergence.
We can now move on to conditional entropy. Conditioning effectively means filtering to some condition as we showed above. Thus, the conditional entropy is, given we&amp;#39;ve filtered to some subspace that matches our condition, what is the entropy of that subspace?
&amp;lt;em&amp;gt;projectWith&amp;lt;/em&amp;gt; is a function we pass in to select on a tuple. For example, if we have &amp;lt;em&amp;gt;(a,b,c)&amp;lt;/em&amp;gt; then &amp;lt;em&amp;gt;projectWith = third&amp;lt;/em&amp;gt; will give us &amp;lt;em&amp;gt;c&amp;lt;/em&amp;gt; Our conditional entropy is then conditioning on each possible value that the projected variable can take on, calculating the entropy of that space and then multiplying it by the probability of the current value. It&amp;#39;s the average entropy from conditioning our random variable of focus across possible values of another random variable. We can now define:
An explicit example always helps. We define the below joint distribution based on the animal example above and visualize its joint distribution:
Relative entropy or KL Divergence (&amp;lt;em&amp;gt;D&amp;lt;sub&amp;gt;KL&amp;lt;/sub&amp;gt;(P||Q)&amp;lt;/em&amp;gt;) is a measure of how well one distribution codes another distribution. Or the loss (in bits) incurred from using the wrong distribution to code for another distribution. If &amp;lt;em&amp;gt;D&amp;lt;sub&amp;gt;KL&amp;lt;/sub&amp;gt;(Posterior||Prior)&amp;lt;/em&amp;gt; represents the change in our distribution after updating with new information, then it can be viewed as surprise. Indeed, the notion probably well matches the colloquial use of the term meaningful information when constrained to state changes the agent experiences in practice.
&amp;lt;em&amp;gt;D&amp;lt;sub&amp;gt;KL&amp;lt;/sub&amp;gt;(P||Q)&amp;lt;/em&amp;gt; is related to mutual information. I(X;Y) = &amp;lt;em&amp;gt;D&amp;lt;sub&amp;gt;KL&amp;lt;/sub&amp;gt;(P(X,Y)||P(X)P(Y))&amp;lt;/em&amp;gt;. Their forms in our discrete space are fairly similar:
Although there are derivations for why entropy should be as it is, they require an uncommon level of mathematical sophistication. The following might be a bit more helpful for a start as an intuition builder.
For a start, we will look at addressing items in terms of indexing some of their properties. Suppose you had 4 things and wanted to label them. One way to do this would be to number them: e.g. Items #0-#3. If we were to use the base 2 numbering system instead of base 10, we would have the added advantage that our labelling system could also be looked at in terms of addressing each item with respect to whether it possessed or not some property. For example:
Imagine our labels as the base 2 numbers: #00, #01, #10 and #11.
&amp;lt;code&amp;gt;Is it green? Then Item #1&amp;lt;/code&amp;gt; OR &amp;lt;code&amp;gt;Is it Blue? Then Item #2&amp;lt;/code&amp;gt; OR &amp;lt;code&amp;gt;Is It Green AND Blue? Then Item #3&amp;lt;/code&amp;gt;. Else It must be Item #0.
In terms of base 2, with 3 things, we would need at most 2^2 = 4 labels. With 10 things we would need at most 2^4 = 16 labels. In general, N will be ≤ 2 ^(&amp;lt;em&amp;gt;ceil&amp;lt;/em&amp;gt; log&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; N), where ceiling rounds up always (5.2 -&amp;gt; 6 or 5.8 -&amp;gt; 6). Essentially, all we&amp;#39;re doing is computing the maximum power 2 needs to be raised to in order to be greater than or equal to N. If we have b=&amp;lt;em&amp;gt;ceil&amp;lt;/em&amp;gt; log&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; N we can simply say &amp;lt;em&amp;gt;N&amp;lt;/em&amp;gt; items require no more than &amp;lt;em&amp;gt;b&amp;lt;/em&amp;gt; &amp;lt;em&amp;gt;properties&amp;lt;/em&amp;gt; to address or distinguish between them. In our example, that was checking whether &amp;lt;i&amp;gt;green or blue&amp;lt;/i&amp;gt; to distinguish between 4 items.
We can also look at it in terms of asking &amp;lt;i&amp;gt;yes/no&amp;lt;/i&amp;gt; questions (this is gone over in clear detail in [3]). This creates a (balanced?) binary tree. If we have &amp;lt;i&amp;gt;N&amp;lt;/i&amp;gt; items, we can address or look them up in the tree using no more than log&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; N steps. Imagine playing a guessing game; even if you had to choose between a billion numbers, it would take no more than 30 guesses if you kept slicing the possibilities in half. For our questions, the rare and more surprising items will be deeper in the tree.
Intuitively, things that occur rarely should be more surprising and so we should devote more space or attention to them. This can be viewed as difficulty distinguishing that item and requiring many questions to be confident about it. If we have each split at 0.5 probability then each node at depth d will have 1/2&amp;lt;sup&amp;gt;depth&amp;lt;/sup&amp;gt; or 2&amp;lt;sup&amp;gt;-depth&amp;lt;/sup&amp;gt; reachability probability (if there is more than one path to the node, we take their sum).
Suppose we have an object that can be in {A,B} and &amp;#39;&amp;lt;em&amp;gt;B&amp;lt;/em&amp;gt;&amp;#39; has probability 12.5%, then we should devote -log&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt;(0.125) or 3 bits of uncertainty space (technical term is surprisal) to it. Meanwhile &amp;#39;A&amp;#39;, with p = 87.5%, gets about 0.19 bits of surprisal...not as much. Entropy is our (weighted) average surprisal over possible states (0.54 bits for this example). For a high entropy (uniform) situation, I imagine a deep tree with many nodes at the bottom, each having an equal probability of being reached.
&amp;lt;img src=&amp;quot;Images/questions.png&amp;quot; alt=&amp;quot;alt text&amp;quot;&amp;gt;
You can most easily play with the probabilistic programming code by copy and pasting code sections in &amp;lt;a href=&amp;quot;https://gist.github.com/sir-deenicus/d8183d73ed12c2aa7d57f621c8a99ad1&amp;quot;&amp;gt;https://gist.github.com/sir-deenicus/d8183d73ed12c2aa7d57f621c8a99ad1&amp;lt;/a&amp;gt; into &amp;lt;a href=&amp;quot;http://fable.io/repl/&amp;quot;&amp;gt;http://fable.io/repl/&amp;lt;/a&amp;gt;
[1] &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Randomized_response&amp;quot;&amp;gt;https://en.wikipedia.org/wiki/Randomized_response&amp;lt;/a&amp;gt;
[2] &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/Differential_privacy&amp;quot;&amp;gt;https://en.wikipedia.org/wiki/Differential_privacy&amp;lt;/a&amp;gt;
[3] &amp;lt;a href=&amp;quot;http://tuvalu.santafe.edu/~simon/it.pdf&amp;quot;&amp;gt;http://tuvalu.santafe.edu/~simon/it.pdf&amp;lt;/a&amp;gt;</description>
    </item>
    <item>
      <title>In which I use watching a movie to explain Bayesian Probability. </title>
      <link>In_which_I_use_watching_a_movie_to_explain_Bayesian_Probability._.htm</link>
      <guid>In_which_I_use_watching_a_movie_to_explain_Bayesian_Probability._.htm</guid>
      <pubDate>Fri, 13 Oct 2017 09:59:54 GMT</pubDate>
      <description>(&amp;lt;em&amp;gt;10/13/2017: ~4.5 years ago, I wrote this article to explain bayesianism without invoking any math. Bayes Theorem is not particular to bayesianism although it is true that the rule underpins the technique when you zoom out far enough. But the essence of what bayesian computation consists of has, in my opinion, never been properly explicated at a high level. Looking back, I think I did well but for a few misunderstandings I have corrected in parenthesized italics&amp;lt;/em&amp;gt;).
&amp;lt;strong&amp;gt;Friday, February 22, 2013&amp;lt;/strong&amp;gt;
Subjective (&amp;lt;em&amp;gt;as of now, I consider the notion of subjective to be superfluous&amp;lt;/em&amp;gt;) Bayesian Probability is a very powerful but partially impractical philosophy about the most sensible way to handle information. It was basically invented by this guy called &amp;lt;a href=&amp;quot;http://en.wikipedia.org/wiki/Pierre-Simon_Laplace&amp;quot;&amp;gt;Laplace&amp;lt;/a&amp;gt;. Quite often, when people explain Bayesian probability, the focus of their efforts gets tripped up because of the name. So they end up spending way too much time talking about Bayes Theorem or Rule. Which is actually not at all special to Bayesian Probability.
The proper way to view Bayesian probability is as the idea that everything can be assigned a probability (&amp;lt;em&amp;gt;this is not the best frame, it&amp;#39;s much better to think of it as repeated conditioning on a space/collection of hypotheses. That is, slicing away and eliminating falsified hypotheses as well as up or down weighting supported hypotheses or not, as approrpriate&amp;lt;/em&amp;gt;). In this way we are all natural Bayesians since a typical response would go like, &amp;#39;eh? you mean you can&amp;#39;t assign probabilities to everything?&amp;#39; You can&amp;#39;t but people act this way anyways; talking about likely, unlikely, betting and percent chance. You can think of it as subsuming logic. Where each proposition has a value between 0 and 1, with false as 0 and 1 as true. Then I can ask you if you think you&amp;#39;re at risk of being eaten by bigfoot and you could give me an answer between yes and no. Bayesian Probability is also naturally extensible to quantum mechanics and makes a lot of (but not all) things less counterintuitive. But how to understand it?
Well consider a movie. We can view a particular aspect of watching a movie as a game of prediction. The fun of the game is to try and get as close a guess to the movie&amp;#39;s final outcome as possible, while the purpose of the director is to try and surprise as much as possible. This surprise is a measure of how not boring the movie is. So you come into the movie with your past experiences, the title, poster and maybe trailers. You&amp;#39;ve got a bunch of ideas of what will happen. As each scene unfolds you get more data and you lower the score of some guesses, while giving other guesses more weight. Each guess is a hypothesis and its weight is how likely you think it is, i.e. its probability. You can then think of the whole bunch of guesses with weights as your &amp;lt;em&amp;gt;prior distribution&amp;lt;/em&amp;gt; (&amp;lt;em&amp;gt;the purpose of the prior is to keep you grounded, rooting future predictions to be consistent with past knowledge&amp;lt;/em&amp;gt;). As you get more data or scenes you need to do an update of your beliefs.
That update is where the famous Bayes rule comes in. You can think of it as rebalancing your guesses with a hard to beat sanity check. Where, you have how likely each guess is (prior), check if the scenes so far match the guess (likelihood) (&amp;lt;em&amp;gt;I would rephrase that as how likely each guess rates the scene by assuming itself true, the guesses that didn&amp;#39;t do well get pushed down&amp;lt;/em&amp;gt;) and then balance it with how much sense the movie makes now (probability of data &amp;lt;em&amp;gt;(more specifically, think of it as after slicing some things away, squishing others and inflating the rest, you need to rebalance the existing hypotheses with respect to the new space so they remain as probabilities)&amp;lt;/em&amp;gt;). What you get out is a posterior distribution. As it turns out, we are not consciously good at being Bayesians &amp;lt;em&amp;gt;(this isn&amp;#39;t quite accurate. It&amp;#39;s true we rely on brittle heuristics to make up for the small scratch space but being bayesian is intractable anyways)&amp;lt;/em&amp;gt;. We don&amp;#39;t do the update step properly. But our unconscious mind can approximate it very well &amp;lt;em&amp;gt;(the truth is more that the natural world is highly structured and hierarchical, therefore forgiving of many hacks)&amp;lt;/em&amp;gt;. This is good because you are not consciously guessing what will happen, it just happens in the background. So people as doing bayesian movie watching is a pretty good description. There&amp;#39;s one more concept to talk about. Nonparametricity. It sounds complicated but you can think of it (roughly) like this. When you start the movie you don&amp;#39;t have all possible scenarios already figured out, as you get more data/scenes you grow the size and complexity of your hypothesis space/bag.
Now, at the end of the movie one hypothesis ends up as true. If you did well then the difference between your final posterior distribution and your prior before that—where you ended up concentrating most of the weight—should be minimal. This is a typical movie where you are not very surprised. However, good directors and writers have a handful of choices to trip you up with.
Directors and writers can overwhelm you with information or use non intuitive methods such as playing the film backwards or jumping around scenes or write a script with lots of possible scenarios. So you are not able to incorporate data properly and your distribution goes out of whack.
They can downplay certain cues so you don&amp;#39;t give certain guesses the proper weight till near the end. If they do it right, you would hold the guess in your bag of guesses but not count it for much and if they do it wrong they will just outright give an unjustified conclusion that was not at all in your prior. This is often upsetting and out of nowhere. A Movie can also be so nonsensical that you reject it because of the probability of the data. Maybe there is another semantic or logical meta-layer (&amp;lt;em&amp;gt;this is hierarchical bayes which is necessary in the real world—note the probabilistic programming baysian community also and IMO confusingly calls nested parameter dependencies hierarchical&amp;lt;/em&amp;gt;) that looks at the consistency of the movie, compares it to experience and says: wtf m8?
Another option is to come up with an original story that is totally out of sync with your priors that your updates don&amp;#39;t end up converging/on target in time for the movie end. These are those super rare &amp;quot;make you think movies&amp;quot;. A typical good movie is one which surprises you but ends up with an explanation that you did not overly discount and with a collection of scenes which are consistent with their internal logic.
Now, there is a controversy between Bayesians and non-Bayesians. Which basically reduces to dissatisfaction that a great deal of the time, a Director can come up with a final story so completely outside your prior that you never converge despite it being completely consistent and with you updating all the data properly. You can only ever have imperfect coverage over all possibilities (&amp;lt;em&amp;gt;There is a more common but less defensible series of arguments I omitted. A lot of people like to argue about objective vs subjective priors. This makes no sense since objective priors merely hide subjectivity. There is also a misapprehension leading to a belief that non-informative priors are superior. In reality, use of informative priors is vital to get learning going and to guard against conspiracy theories&amp;lt;/em&amp;gt;).
Another problem is that it is very hard to compute. So it must be estimated. In reality, a respectable Bayesian will use non-Bayesian methods as sanity checks or guides. Beware of the Rabid Rationalist Zealots.
An aside: our brains optimized on a particular tract that did well in its proper environment but now shine as &amp;quot;biases&amp;quot;. Speaking of which. This example also gives somewhat of an idea of a basic model of evolution. Replace guesses with particular versions of genes (alleles), likelihood as number of organisms with that gene (&amp;lt;em&amp;gt;I don&amp;#39;t like this phrasing. Since likelihood reweights things, I&amp;#39;d amend it as a process that yields the new proportion of each gene in organisms that survived to reproduction&amp;lt;/em&amp;gt;) and surprise as number of deaths. Indeed Bayesian updating is just a special case of evolutionary learning (which fuels my belief that methods like Holland&amp;#39;s deserve more study &amp;lt;em&amp;gt;(To clarify: it&amp;#39;s general in the sense of being less strict on what constitutes an update, i.e. you can do more involved things than simoply conditioning on observations. You can mutate and recombine hypotheses and not be strict about things adding up to or being probabilities—not as strict on normalizing in order to understand what is going on, local.&amp;lt;/em&amp;gt;).
(&amp;lt;em&amp;gt;Another criticism is that the prior has to match the data generation process but this is true for any learning method—every learner equips structural priors which must match the data regularities or do no better than random. Sanity checks, testing and noting the lack of separability between the likelihood and prior acknowledge the limitations of bayes in reality. Bayesianism, or more usefully, probabilistic programming is ideal as it&amp;#39;s a simple way to reason about uncertainty, regularize, package hypothesis and condition on new information. It&amp;#39;s preferable in exactly the same way that a statically typed functional programming is preferable to C (there are less ways to shoot yourself in the foot and more ways to guard against simple but impactful mistakes, despite not being a panacea. Probabilistic programming also has the advantage of extensibility, generalizability and modularization between inference procedure and problem declearation. Furthermore, a bayesian system that can generate hypotheses online, that is not limited to some fixed set of hypotheses and has some &amp;quot;fast learning&amp;quot; trap door, and that learns in a hierarchical manner is probably the most powerful learner this universe can host.&amp;lt;/em&amp;gt;)
And, taking this further you can look at evolution as a type of intelligence&amp;lt;sup&amp;gt;&amp;lt;a href=&amp;quot;#f1&amp;quot;&amp;gt;1&amp;lt;/a&amp;gt;&amp;lt;/sup&amp;gt;. The universe learning about itself doing inference and making predictions about its own rules. Life as a computation trying to figure out what combination of elements lead to the most stable and (&amp;lt;em&amp;gt;entropy exporting&amp;lt;/em&amp;gt;) persistent structures. Why, If I was being poetic I would say Life is the universe thinking about itself (&amp;lt;em&amp;gt;recursively self modeling—Good Regulator invokes self-similarity!&amp;lt;/em&amp;gt;).
And there you have it. I&amp;#39;ve taken some liberties but I think this is a comprehensible but still representative explanation.
&amp;lt;em&amp;gt;Deen Abiola&amp;lt;/em&amp;gt;</description>
    </item>
  </channel>
</rss>